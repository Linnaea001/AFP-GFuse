{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8161,
     "status": "ok",
     "timestamp": 1759456559185,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "-521mSLjDu2T",
    "outputId": "7427adb3-5d74-4c61-814c-7a2ff11cb88a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8BDbX0lIZSs"
   },
   "source": [
    "# åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9455,
     "status": "ok",
     "timestamp": 1759456575827,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "eQTzzqA6IaiX",
    "outputId": "d4ec024f-acf1-40a7-ebfa-bf1051bc0ea5"
   },
   "outputs": [],
   "source": [
    "pip install Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 65838,
     "status": "ok",
     "timestamp": 1759456641667,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "plH_EKiYEvX7",
    "outputId": "005f042a-9444-4573-9411-6d333c14ee46"
   },
   "outputs": [],
   "source": [
    "pip install torch torch-geometric optuna tqdm scikit-learn esm umap-learn shap xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 66614,
     "status": "ok",
     "timestamp": 1759456708288,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "GsFMLAa9waEZ",
    "outputId": "d7551d7e-1c0a-4ca6-d0ee-202b030d69b5"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy pandas\n",
    "!pip install numpy==1.26.4 pandas==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2006,
     "status": "ok",
     "timestamp": 1759456710305,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "8E6hDtucnwBm",
    "outputId": "33742803-5469-4518-e61b-2972e08294b1"
   },
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1759456711932,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "I-6FNFN-T9Bt",
    "outputId": "e0daea41-baf8-495e-a005-f8f8909216fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dUEQFzFIb3o"
   },
   "source": [
    "# æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnqCMjKgIhng"
   },
   "source": [
    "## é˜³æ€§æ ·æœ¬cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13838,
     "status": "ok",
     "timestamp": 1753430799855,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "JJAvM38xIkKp",
    "outputId": "b9f4bde0-5106-4072-b177-ed45ce0ca42e"
   },
   "outputs": [],
   "source": [
    "!apt-get install cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1753430800008,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "geFeazrvKkPZ",
    "outputId": "951ec2e2-2a32-474b-b893-580795ad0d4f"
   },
   "outputs": [],
   "source": [
    "!grep -c \"^>\" AFP.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1753430800087,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "6HMPMZIdJv6t",
    "outputId": "b8917c51-af19-480d-b3a8-4564d0b47b33"
   },
   "outputs": [],
   "source": [
    "!cd-hit -i AFP.fasta -o AFP_clustered.fasta -c 0.8 -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "error",
     "timestamp": 1753430800229,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "nJiSgc8FL6JT",
    "outputId": "7e4df39f-1f80-48cb-c642-4555118e1d24"
   },
   "outputs": [],
   "source": [
    "def convert_to_fasta(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        sequences = file.readlines()\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            sequence = sequence.strip()  \n",
    "            file.write(f\">Sequence_{i + 1}\\n{sequence}\\n\")\n",
    "\n",
    "convert_to_fasta('AFP_2.txt', 'AFP_2.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4XKTJ4scakY"
   },
   "outputs": [],
   "source": [
    "with open(\"AFP.fasta\", \"r\") as file:\n",
    "    sequences = file.read().split('>')\n",
    "    sequences = [seq for seq in sequences if seq.strip()]\n",
    "    unique_sequences = set(sequences)\n",
    "\n",
    "print(f\"Total sequences: {len(sequences)}\")\n",
    "print(f\"Unique sequences: {len(unique_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5Z1Ywo-mU9q"
   },
   "outputs": [],
   "source": [
    "def filter_sequences(input_file, output_file, max_length=100):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        write_sequence = False\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                if write_sequence:\n",
    "                    outfile.write(sequence_header + sequence_data)\n",
    "                sequence_header = line\n",
    "                sequence_data = ''\n",
    "                write_sequence = False  # Reset for next sequence\n",
    "            else:\n",
    "                sequence_data += line\n",
    "                if len(sequence_data.replace('\\n', '')) <= max_length:\n",
    "                    write_sequence = True\n",
    "                else:\n",
    "                    write_sequence = False\n",
    "\n",
    "        # Check last sequence\n",
    "        if write_sequence:\n",
    "            outfile.write(sequence_header + sequence_data)\n",
    "\n",
    "input_filename = 'AFP_clustered.fasta'\n",
    "output_filename = 'AFP_CD-hit.fasta'\n",
    "filter_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5qqxCNwm8BW"
   },
   "outputs": [],
   "source": [
    "def renumber_sequences(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        counter = 1\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                outfile.write(f'>Sequence_{counter}\\n')\n",
    "                counter += 1\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_filename = 'AFP_CD-hit.fasta'  \n",
    "output_filename = 'AFP_renumbered.fasta'  \n",
    "renumber_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBHmxnQzo6b6"
   },
   "source": [
    "## é˜´æ€§æ ·æœ¬cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu3deDiwqtNb"
   },
   "outputs": [],
   "source": [
    "def convert_to_fasta(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        sequences = file.readlines()\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            sequence = sequence.strip()  \n",
    "            file.write(f\">Sequence_{i + 1}\\n{sequence}\\n\")\n",
    "\n",
    "convert_to_fasta('Non_AFP.txt', 'Non_AFP.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QlQDfafAo9Vf"
   },
   "outputs": [],
   "source": [
    "!cd-hit -i Non_AFP.fasta -o Non_AFP_clustered.fasta -c 0.8 -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDYq6rtxo_zs"
   },
   "outputs": [],
   "source": [
    "def filter_sequences(input_file, output_file, max_length=100):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        write_sequence = False\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                if write_sequence:\n",
    "                    outfile.write(sequence_header + sequence_data)\n",
    "                sequence_header = line\n",
    "                sequence_data = ''\n",
    "                write_sequence = False  # Reset for next sequence\n",
    "            else:\n",
    "                sequence_data += line\n",
    "                if len(sequence_data.replace('\\n', '')) <= max_length:\n",
    "                    write_sequence = True\n",
    "                else:\n",
    "                    write_sequence = False\n",
    "\n",
    "        # Check last sequence\n",
    "        if write_sequence:\n",
    "            outfile.write(sequence_header + sequence_data)\n",
    "\n",
    "input_filename = 'Non_AFP_clustered.fasta'\n",
    "output_filename = 'Non_AFP_CD-hit.fasta'\n",
    "filter_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPXwHb7no_2K"
   },
   "outputs": [],
   "source": [
    "def renumber_sequences(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        counter = 1\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                outfile.write(f'>Sequence_{counter}\\n')\n",
    "                counter += 1\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_filename = 'Non_AFP_CD-hit.fasta' \n",
    "output_filename = 'Non_AFP_renumbered.fasta' \n",
    "renumber_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Za8fI8fov8b"
   },
   "source": [
    "## åˆ’åˆ†æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5806,
     "status": "ok",
     "timestamp": 1753501578999,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "KI0I5tCstwMz",
    "outputId": "a4ac182d-fd29-4ce0-b3a5-266b77ac098f"
   },
   "outputs": [],
   "source": [
    "pip install Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRHQxHp6oyrJ"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_sequences(file_path, label):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        labels.append(label)  \n",
    "    return sequences, labels\n",
    "\n",
    "def balance_and_split(sequences, labels):\n",
    "    df = pd.DataFrame({\n",
    "        'sequence': sequences,\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "    print(f\"åŸå§‹æ•°æ®æ€»é‡: {df.shape[0]}\")\n",
    "    print(f\"å„ç±»æ ·æœ¬æ•°é‡ï¼š\\n{df['label'].value_counts()}\")\n",
    "\n",
    "    # è¿›è¡Œæ¬ é‡‡æ ·\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = rus.fit_resample(df[['sequence']], df['label'])\n",
    "\n",
    "    print(f\"æ¬ é‡‡æ ·åæ•°æ®æ€»é‡: {X_res.shape[0]}\")\n",
    "    print(f\"æ¬ é‡‡æ ·åå„ç±»æ ·æœ¬æ•°é‡ï¼š\\n{pd.Series(y_res).value_counts()}\")\n",
    "\n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"è®­ç»ƒé›†æ•°é‡: {X_train.shape[0]}\")\n",
    "    print(f\"æµ‹è¯•é›†æ•°é‡: {X_test.shape[0]}\")\n",
    "\n",
    "    train_df = pd.DataFrame(X_train, columns=['sequence'])\n",
    "    train_df['label'] = y_train\n",
    "    test_df = pd.DataFrame(X_test, columns=['sequence'])\n",
    "    test_df['label'] = y_test\n",
    "\n",
    "    train_df.to_csv('train_dataset.csv', index=False)\n",
    "    test_df.to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "pos_sequences, pos_labels = load_sequences('AFP_renumbered.fasta', 1)\n",
    "neg_sequences, neg_labels = load_sequences('Non_AFP_renumbered.fasta', 0)\n",
    "\n",
    "print(f\"é˜³æ€§æ ·æœ¬æ•°é‡: {len(pos_sequences)}\")\n",
    "print(f\"é˜´æ€§æ ·æœ¬æ•°é‡: {len(neg_sequences)}\")\n",
    "\n",
    "all_sequences = pos_sequences + neg_sequences\n",
    "all_labels = pos_labels + neg_labels\n",
    "\n",
    "train_df, test_df = balance_and_split(all_sequences, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz6lWj8Lx6a_"
   },
   "source": [
    "## å»é™¤é20ä¸ªæ ‡å‡†æ°¨åŸºé…¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1757042768681,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "oOlms3L9x-Ct",
    "outputId": "9fa8a027-9033-4f5c-f0ab-ee700f4a62a5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def replace_non_standard_amino_acids(seq):\n",
    "\n",
    "    replacements = {'B': 'D', 'Z': 'E', 'X': 'A', 'J': 'L', 'U': 'C', 'O': 'K'}\n",
    "    for old, new in replacements.items():\n",
    "        seq = seq.replace(old, new)\n",
    "    return seq\n",
    "\n",
    "def load_and_preprocess(file_path, output_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['sequence'] = df['sequence'].apply(replace_non_standard_amino_acids)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "train_df = load_and_preprocess('train_dataset.csv', 'processed_train_dataset.csv')\n",
    "test_df = load_and_preprocess('test_dataset.csv', 'processed_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUJ__KTOivXB"
   },
   "outputs": [],
   "source": [
    "def renumber_sequences(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        counter = 1\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                outfile.write(f'>Sequence_{counter}\\n')\n",
    "                counter += 1\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_filename = 'processed_test_dataset.csv'\n",
    "output_filename = 'ColabFold_test_dataset.csv'\n",
    "renumber_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LADKYcKABvi8"
   },
   "source": [
    "## åˆ†åˆ«åˆ’åˆ†æˆè®­ç»ƒé›†é˜³æ€§ï¼Œè®­ç»ƒé›†é˜´æ€§ï¼Œæµ‹è¯•é›†é˜³æ€§ï¼Œæµ‹è¯•é›†é˜´æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taWjz7KIB0a2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def to_fasta(df, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for index, row in df.iterrows():\n",
    "            f.write(f\">{index}\\n{row['sequence']}\\n\")\n",
    "\n",
    "def process_and_save(data_path, output_prefix):\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    pos = df[df['label'] == 1]\n",
    "    neg = df[df['label'] == 0]\n",
    "\n",
    "    to_fasta(pos, f'{output_prefix}_pos.fasta')\n",
    "    to_fasta(neg, f'{output_prefix}_neg.fasta')\n",
    "\n",
    "train_path = 'processed_train_dataset.csv'\n",
    "test_path = 'processed_test_dataset.csv'\n",
    "\n",
    "process_and_save(train_path, 'Colab_train')\n",
    "process_and_save(test_path, 'Colab_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RnxO1AX2u46"
   },
   "source": [
    "# ç»“æ„ä¿¡æ¯\n",
    "*   åŠ è½½pdbæ–‡ä»¶\n",
    "*   ä½¿ç”¨pdbæ–‡ä»¶æå–ç‰¹å¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQqE1ofqsviH"
   },
   "source": [
    "## åŠ è½½pdbæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1753502154056,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "-8QFQR61s4ED",
    "outputId": "c7f28949-bb85-4eb3-c6ed-8acfc36ba55d"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "zip_folder_path = '/content/drive/MyDrive/AFP_work/result/test_neg'\n",
    "extract_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'  # è§£å‹åçš„è·¯å¾„\n",
    "\n",
    "os.makedirs(extract_folder_path, exist_ok=True)\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "for filename in os.listdir(zip_folder_path):\n",
    "    if filename.endswith('.zip'):\n",
    "        zip_path = os.path.join(zip_folder_path, filename)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for pdb_file in zip_ref.namelist():\n",
    "                if pdb_file.endswith('.pdb') and 'relaxed_rank_001' in pdb_file:\n",
    "                    zip_ref.extract(pdb_file, extract_folder_path)\n",
    "                    print(f'è§£å‹ç¼©å®Œæˆï¼š{pdb_file} ä» {filename} åˆ° {extract_folder_path}')\n",
    "                    file_count += 1\n",
    "\n",
    "print(f'æ€»å…±è§£å‹äº† {file_count} ä¸ªæ–‡ä»¶')\n",
    "\n",
    "pdb_files = [f for f in os.listdir(extract_folder_path) if f.endswith('.pdb')]\n",
    "print(f'è§£å‹åçš„æ–‡ä»¶å¤¹ä¸­å…±æœ‰ {len(pdb_files)} ä¸ª PDB æ–‡ä»¶')\n",
    "\n",
    "# åˆ é™¤åŒ…å« \"unrelaxed\" çš„ PDB æ–‡ä»¶\n",
    "for pdb_file in pdb_files:\n",
    "    if 'unrelaxed' in pdb_file:\n",
    "        file_path = os.path.join(extract_folder_path, pdb_file)\n",
    "        os.remove(file_path)\n",
    "        print(f'å·²åˆ é™¤æ–‡ä»¶ï¼š{file_path}')\n",
    "\n",
    "remaining_pdb_files = [f for f in os.listdir(extract_folder_path) if f.endswith('.pdb')]\n",
    "print(f'åˆ é™¤ unrelaxed æ–‡ä»¶åï¼Œæ–‡ä»¶å¤¹ä¸­å…±æœ‰ {len(remaining_pdb_files)} ä¸ª PDB æ–‡ä»¶')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1754101167178,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "UWRjQiwMTlms",
    "outputId": "3465b49c-86cb-4934-bf02-afdb66fb122a"
   },
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "\n",
    "parser = PDBParser(QUIET=True)\n",
    "ppb = PPBuilder()\n",
    "\n",
    "def extract_sequence_from_pdb(pdb_path):\n",
    "    structure = parser.get_structure('', pdb_path)\n",
    "    sequence = \"\"\n",
    "    for pp in ppb.build_peptides(structure):\n",
    "        sequence += str(pp.get_sequence())\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1754101179783,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "kf2BXidF2290",
    "outputId": "69f64539-8b4d-48ef-fe47-65086479005b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# å››ä¸ªæ–‡ä»¶å¤¹è·¯å¾„\n",
    "train_pos_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "test_pos_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "train_neg_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_neg_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "\n",
    "train_pdb_count = len([f for f in os.listdir(train_pos_path) if f.endswith('.pdb')])\n",
    "test_pdb_count = len([f for f in os.listdir(test_pos_path) if f.endswith('.pdb')])\n",
    "train_neg_pdb_count = len([f for f in os.listdir(train_neg_path) if f.endswith('.pdb')])\n",
    "test_neg_pdb_count = len([f for f in os.listdir(test_neg_path) if f.endswith('.pdb')])\n",
    "\n",
    "print(f'è®­ç»ƒé›†ä¸­çš„ PDB æ–‡ä»¶æ•°é‡ï¼ˆé˜³æ€§ï¼‰ï¼š{train_pdb_count}')\n",
    "print(f'æµ‹è¯•é›†ä¸­çš„ PDB æ–‡ä»¶æ•°é‡ï¼ˆé˜³æ€§ï¼‰ï¼š{test_pdb_count}')\n",
    "print(f'è®­ç»ƒé›†ä¸­çš„ PDB æ–‡ä»¶æ•°é‡ï¼ˆé˜´æ€§ï¼‰ï¼š{train_neg_pdb_count}')\n",
    "print(f'æµ‹è¯•é›†ä¸­çš„ PDB æ–‡ä»¶æ•°é‡ï¼ˆé˜´æ€§ï¼‰ï¼š{test_neg_pdb_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0LT6AZYEBL3"
   },
   "source": [
    "## ä½¿ç”¨pdbæ–‡ä»¶æå–ç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10505,
     "status": "error",
     "timestamp": 1754102535596,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "xaF1V0LzEElS",
    "outputId": "5e52bcad-f684-4966-d61b-cab31de5d693"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "output_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features'\n",
    "\n",
    "os.makedirs(output_train_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_train_neg_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_neg_folder_path, exist_ok=True)\n",
    "\n",
    "# æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ‰€æœ‰ PDB æ–‡ä»¶\n",
    "train_pos_pdb_files = [f for f in os.listdir(train_pos_folder_path) if f.endswith('.pdb')]\n",
    "train_neg_pdb_files = [f for f in os.listdir(train_neg_folder_path) if f.endswith('.pdb')]\n",
    "test_pos_pdb_files = [f for f in os.listdir(test_pos_folder_path) if f.endswith('.pdb')]\n",
    "test_neg_pdb_files = [f for f in os.listdir(test_neg_folder_path) if f.endswith('.pdb')]\n",
    "\n",
    "# æå–é€‰ä¸­çš„ PDB æ–‡ä»¶çš„ç»“æ„ä¿¡æ¯\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ä¼˜åŒ– PDB æ–‡ä»¶å¤„ç†å‡½æ•°\n",
    "def process_pdb_file(pdb_path):\n",
    "    structure = parser.get_structure('', pdb_path)\n",
    "    residues = [residue for residue in structure.get_residues() if 'CA' in residue]\n",
    "    num_residues = len(residues)\n",
    "\n",
    "    # æå–ä½ç½®ç‰¹å¾ã€æ–¹å‘ç‰¹å¾å’Œæ—‹è½¬ç‰¹å¾\n",
    "    positions = np.array([residue['CA'].get_coord() for residue in residues], dtype=np.float64)\n",
    "    edges = []\n",
    "    directions = []\n",
    "    rotations = []\n",
    "\n",
    "    # è®¡ç®—æ¥è§¦å›¾å’Œé™„åŠ ç‰¹å¾\n",
    "    for i in range(num_residues):\n",
    "        for j in range(i + 1, num_residues):\n",
    "            distance = np.linalg.norm(positions[i] - positions[j])\n",
    "            if distance < 10.0:  # é˜ˆå€¼ä¸º10Ã…æ¥å®šä¹‰æ¥è§¦\n",
    "                edges.append([i, j])\n",
    "                direction = positions[j] - positions[i]\n",
    "                norm = np.linalg.norm(direction)\n",
    "                if norm != 0:\n",
    "                    directions.append(direction / norm)\n",
    "                    rotations.append(float(np.arctan2(direction[1], direction[0])))\n",
    "\n",
    "    return positions, edges, directions, rotations\n",
    "\n",
    "# å¤„ç†è®­ç»ƒé›†ä¸­çš„ PDB æ–‡ä»¶å¹¶æ·»åŠ æ ‡ç­¾\n",
    "print(f'è®­ç»ƒé›†ä¸­çš„ PDB æ–‡ä»¶æ•°é‡ (é˜³æ€§): {len(train_pos_pdb_files)}')\n",
    "print(f'è®­ç»ƒé›†ä¸­çš„ PDB æ–‡ä»¶æ•°é‡ (é˜´æ€§): {len(train_neg_pdb_files)}')\n",
    "\n",
    "for pdb_file in train_pos_pdb_files:\n",
    "    pdb_path = os.path.join(train_pos_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 1  # é˜³æ€§æ ·æœ¬æ ‡ç­¾\n",
    "    }\n",
    "    # ä¿å­˜ç‰¹å¾åˆ°æ–‡ä»¶\n",
    "    output_file_path = os.path.join(output_train_pos_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json') if features['label'] == 1 else os.path.join(output_train_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "for pdb_file in train_neg_pdb_files:\n",
    "    pdb_path = os.path.join(train_neg_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 0  # é˜´æ€§æ ·æœ¬æ ‡ç­¾\n",
    "    }\n",
    "    # ä¿å­˜ç‰¹å¾åˆ°æ–‡ä»¶\n",
    "    output_file_path = os.path.join(output_train_neg_folder_path if features['label'] == 1 else output_test_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "# å¤„ç†æµ‹è¯•é›†ä¸­çš„ PDB æ–‡ä»¶å¹¶æ·»åŠ æ ‡ç­¾\n",
    "print(f'æµ‹è¯•é›†ä¸­çš„ PDB æ–‡ä»¶æ•°é‡ (é˜³æ€§): {len(test_pos_pdb_files)}')\n",
    "print(f'æµ‹è¯•é›†ä¸­çš„ PDB æ–‡ä»¶æ•°é‡ (é˜´æ€§): {len(test_neg_pdb_files)}')\n",
    "\n",
    "for pdb_file in test_pos_pdb_files:\n",
    "    pdb_path = os.path.join(test_pos_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 1  # é˜³æ€§æ ·æœ¬æ ‡ç­¾\n",
    "    }\n",
    "    # ä¿å­˜ç‰¹å¾åˆ°æ–‡ä»¶\n",
    "    output_file_path = os.path.join(output_test_pos_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json') if features['label'] == 1 else os.path.join(output_test_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "for pdb_file in test_neg_pdb_files:\n",
    "    pdb_path = os.path.join(test_neg_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 0  # é˜´æ€§æ ·æœ¬æ ‡ç­¾\n",
    "    }\n",
    "    # ä¿å­˜ç‰¹å¾åˆ°æ–‡ä»¶\n",
    "    output_file_path = os.path.join(output_test_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "# è¾“å‡ºä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡\n",
    "# json_files = [f for f in os.listdir(output_folder_path) if f.endswith('.json')]\n",
    "# print(f'ä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡: {len(json_files)}')\n",
    "\n",
    "# æ£€æŸ¥æ¯ä¸ªä¿å­˜çš„ JSON æ–‡ä»¶ä¸­çš„ç»´åº¦ä¿¡æ¯\n",
    "# for json_file in json_files:\n",
    "#     json_path = os.path.join(output_folder_path, json_file)\n",
    "#     with open(json_path, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "#         print(f'æ–‡ä»¶: {json_file}')\n",
    "#         print(f'èŠ‚ç‚¹ç‰¹å¾æ•°é‡: {len(data[\"node_features\"])}, ä½ç½®ç‰¹å¾ç»´åº¦: {len(data[\"node_features\"][0])}')\n",
    "#         print(f'è¾¹ç‰¹å¾æ•°é‡: {len(data[\"edge_features\"][\"edges\"])}, æ–¹å‘ç‰¹å¾æ•°é‡: {len(data[\"edge_features\"][\"directions\"])}, æ—‹è½¬ç‰¹å¾æ•°é‡: {len(data[\"edge_features\"][\"rotations\"])}')\n",
    "\n",
    "\n",
    "output_train_neg_folder_path\n",
    "\n",
    "# è¾“å‡ºä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡\n",
    "json_files = [f for f in os.listdir(output_train_neg_folder_path) if f.endswith('.json')]\n",
    "print(f'output_train_neg_folder_path ä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡: {len(json_files)}')\n",
    "\n",
    "# æ£€æŸ¥æ¯ä¸ªä¿å­˜çš„ JSON æ–‡ä»¶ä¸­çš„ç»´åº¦ä¿¡æ¯\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(output_train_neg_folder_path, json_file)\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        print(f'æ–‡ä»¶: {json_file}')\n",
    "        print(f'èŠ‚ç‚¹ç‰¹å¾æ•°é‡: {len(data[\"node_features\"])}, ä½ç½®ç‰¹å¾ç»´åº¦: {len(data[\"node_features\"][0])}')\n",
    "        print(f'è¾¹ç‰¹å¾æ•°é‡: {len(data[\"edge_features\"][\"edges\"])}, æ–¹å‘ç‰¹å¾æ•°é‡: {len(data[\"edge_features\"][\"directions\"])}, æ—‹è½¬ç‰¹å¾æ•°é‡: {len(data[\"edge_features\"][\"rotations\"])}')\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"æ€»å¤„ç†æ—¶é—´: {end_time - start_time:.2f} ç§’\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2bQyS6_wILJ"
   },
   "source": [
    "# ESM-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9004,
     "status": "ok",
     "timestamp": 1754191779238,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "qmCgs2eb6Mjq",
    "outputId": "be6b644f-bcca-4899-8578-9c1e2d07d56e"
   },
   "outputs": [],
   "source": [
    "pip install esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WEZikzFcP-h",
    "outputId": "850ecb7f-dde9-435a-f9f4-7c06afe9c5e3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import SeqIO\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "# å®šä¹‰ FASTA æ–‡ä»¶è·¯å¾„\n",
    "#train_pos_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_train_pos.fasta'\n",
    "#train_neg_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_train_neg.fasta'\n",
    "test_pos_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_test_pos.fasta'\n",
    "#test_neg_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_test_neg.fasta'\n",
    "\n",
    "output_feature_path = '/content/drive/MyDrive/esmc_600_test_pos'\n",
    "\n",
    "os.makedirs(output_feature_path, exist_ok=True)\n",
    "\n",
    "def read_fasta_sequences(fasta_file):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        seq_id = record.id\n",
    "        sequence = str(record.seq).replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        sequences.append((seq_id, sequence))\n",
    "    return sequences\n",
    "\n",
    "def save_features(features, output_dir):\n",
    "    for seq_id, feature_dict in features.items():\n",
    "        logits = feature_dict['logits']\n",
    "        embeddings = feature_dict['embeddings']\n",
    "\n",
    "        # å®šä¹‰æ–‡ä»¶è·¯å¾„\n",
    "        logits_path = os.path.join(output_dir, f\"{seq_id}_logits.npy\")\n",
    "        embeddings_path = os.path.join(output_dir, f\"{seq_id}_embeddings.npy\")\n",
    "\n",
    "        # ä¿å­˜ logits å’Œ embeddings\n",
    "        np.save(logits_path, logits)\n",
    "        np.save(embeddings_path, embeddings)\n",
    "\n",
    "def extract_features_individual(client, sequences):\n",
    "    features = {}\n",
    "    for seq_id, seq in tqdm(sequences, desc=\"æå–ç‰¹å¾\"):\n",
    "        try:\n",
    "            # åˆ›å»º ESMProtein å®ä¾‹\n",
    "            protein = ESMProtein(sequence=seq)\n",
    "\n",
    "            # ç¼–ç è›‹ç™½è´¨åºåˆ—\n",
    "            protein_tensor = client.encode(protein)\n",
    "\n",
    "            # è·å– logits å’Œ embeddings\n",
    "            logits_output = client.logits(\n",
    "                protein_tensor,\n",
    "                LogitsConfig(sequence=True, return_embeddings=True)\n",
    "            )\n",
    "\n",
    "            logits = logits_output.logits  # éœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹\n",
    "            embeddings = logits_output.embeddings  # éœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹\n",
    "\n",
    "            # æ£€æŸ¥ logits å’Œ embeddings çš„ç±»å‹\n",
    "            if isinstance(logits, torch.Tensor):\n",
    "                logits = logits.cpu().numpy()\n",
    "            elif isinstance(logits, np.ndarray):\n",
    "                pass  # å·²ç»æ˜¯ NumPy æ•°ç»„\n",
    "            else:\n",
    "                # å¦‚æœä¸æ˜¯ tensor æˆ– ndarrayï¼Œå°è¯•å…¶ä»–è½¬æ¢\n",
    "                logits = np.array(logits)\n",
    "\n",
    "            if isinstance(embeddings, torch.Tensor):\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "            elif isinstance(embeddings, np.ndarray):\n",
    "                pass  # å·²ç»æ˜¯ NumPy æ•°ç»„\n",
    "            else:\n",
    "                # å¦‚æœä¸æ˜¯ tensor æˆ– ndarrayï¼Œå°è¯•å…¶ä»–è½¬æ¢\n",
    "                embeddings = np.array(embeddings)\n",
    "\n",
    "            # ä¿å­˜ç‰¹å¾\n",
    "            features[seq_id] = {\n",
    "                'logits': logits,\n",
    "                'embeddings': embeddings\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"å¤„ç†åºåˆ— {seq_id} æ—¶å‡ºé”™: {e}\")\n",
    "            continue\n",
    "    return features\n",
    "\n",
    "# åˆå§‹åŒ– ESMC å®¢æˆ·ç«¯\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "client = ESMC.from_pretrained(\"esmc_600m\").to(device)\n",
    "client.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "\n",
    "fasta_files = [\n",
    "    #train_pos_seq_file,\n",
    "    #train_neg_seq_file,\n",
    "    test_pos_seq_file,\n",
    "    #test_neg_seq_file\n",
    "]\n",
    "\n",
    "for fasta_file in fasta_files:\n",
    "    print(f\"æ­£åœ¨å¤„ç†æ–‡ä»¶: {fasta_file}\")\n",
    "\n",
    "    # è¯»å–åºåˆ—\n",
    "    sequences = read_fasta_sequences(fasta_file)\n",
    "    print(f\"åºåˆ—æ•°é‡: {len(sequences)}\")\n",
    "\n",
    "    # æå–ç‰¹å¾ï¼ˆé€æ¡å¤„ç†ï¼‰\n",
    "    features = extract_features_individual(client, sequences)\n",
    "\n",
    "    # ä¿å­˜ç‰¹å¾\n",
    "    save_features(features, output_feature_path)\n",
    "\n",
    "    print(f\"ç‰¹å¾å·²ä¿å­˜: {fasta_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW7S96HwO3Vg"
   },
   "source": [
    "# å¤„ç†åºåˆ—ç»“æ„ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrR3SQWpO5qt"
   },
   "source": [
    "## å¤„ç†åºåˆ—ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16799,
     "status": "ok",
     "timestamp": 1754199247054,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "i_J1oT45O1XL",
    "outputId": "f49d5193-029a-4e64-9aed-6ffe5dfa2eab"
   },
   "outputs": [],
   "source": [
    "# 2. å¯¼å…¥å¿…è¦çš„åº“\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 3. å®šä¹‰ç‰¹å¾è¾“å‡ºæ–‡ä»¶å¤¹åˆ—è¡¨\n",
    "feature_dirs = [\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',  # è¯·ç¡®è®¤æ­¤è·¯å¾„æ˜¯å¦æ­£ç¡®\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "    # å¦‚æœæœ‰å…¶ä»–æ–‡ä»¶å¤¹ï¼Œå¦‚ esmc_600_test_posï¼Œè¯·åœ¨æ­¤æ·»åŠ \n",
    "]\n",
    "\n",
    "# 4. å®šä¹‰å‡½æ•°ä»¥æŸ¥çœ‹æ±‡æ€»æ–‡ä»¶çš„ç»´åº¦\n",
    "def inspect_combined_files(feature_dirs):\n",
    "    \"\"\"\n",
    "    éå†æ¯ä¸ªç‰¹å¾æ–‡ä»¶å¤¹ï¼ŒåŠ è½½å¹¶æ‰“å° combined_logits.npy å’Œ combined_embeddings.npy çš„ç»´åº¦ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        feature_dirs (List[str]): ç‰¹å¾æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    for feature_dir in feature_dirs:\n",
    "        print(f\"æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹: {feature_dir}\")\n",
    "\n",
    "        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨\n",
    "        if not os.path.isdir(feature_dir):\n",
    "            print(f\"æ–‡ä»¶å¤¹ '{feature_dir}' ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚\\n\")\n",
    "            continue\n",
    "\n",
    "        # å®šä¹‰ combined_logits.npy å’Œ combined_embeddings.npy çš„è·¯å¾„\n",
    "        logits_path = os.path.join(feature_dir, 'combined_logits.npy')\n",
    "        embeddings_path = os.path.join(feature_dir, 'combined_embeddings.npy')\n",
    "\n",
    "        # æ£€æŸ¥ combined_logits.npy æ˜¯å¦å­˜åœ¨\n",
    "        if os.path.isfile(logits_path):\n",
    "            try:\n",
    "                combined_logits = np.load(logits_path, allow_pickle=True)\n",
    "                print(f\" - {os.path.basename(logits_path)} çš„å½¢çŠ¶: {combined_logits.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\" - åŠ è½½ {os.path.basename(logits_path)} æ—¶å‡ºé”™: {e}\")\n",
    "        else:\n",
    "            print(f\" - {os.path.basename(logits_path)} ä¸å­˜åœ¨ã€‚\")\n",
    "\n",
    "        # æ£€æŸ¥ combined_embeddings.npy æ˜¯å¦å­˜åœ¨\n",
    "        if os.path.isfile(embeddings_path):\n",
    "            try:\n",
    "                combined_embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "                print(f\" - {os.path.basename(embeddings_path)} çš„å½¢çŠ¶: {combined_embeddings.shape}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\" - åŠ è½½ {os.path.basename(embeddings_path)} æ—¶å‡ºé”™: {e}\\n\")\n",
    "        else:\n",
    "            print(f\" - {os.path.basename(embeddings_path)} ä¸å­˜åœ¨ã€‚\\n\")\n",
    "\n",
    "        # mxlinæ·»åŠ \n",
    "        sample = combined_embeddings[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "        print(\"æ•°æ®çº§åˆ«ï¼š\",sample.shape)  # è¾“å‡º (1152,)ï¼Œåˆ™æ˜¯åºåˆ—çº§åˆ«\n",
    "\n",
    "# 5. è¿è¡Œå‡½æ•°ä»¥æŸ¥çœ‹ç»´åº¦\n",
    "inspect_combined_files(feature_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1754199247189,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "uSFwZslPPyip",
    "outputId": "e3407317-896e-43bf-f9f8-be1b19590f6c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "feature_dirs = [\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "]\n",
    "\n",
    "# ç»Ÿè®¡æ¯ä¸ªç‰¹å¾æ–‡ä»¶å¤¹ä¸­ logits å’Œ embeddings æ–‡ä»¶çš„æ•°é‡ï¼Œå¹¶éªŒè¯æ˜¯å¦åŒ¹é…ã€‚\n",
    "def count_logit_embedding_files(feature_dirs):\n",
    "    for feature_dir in feature_dirs:\n",
    "\n",
    "        if not os.path.isdir(feature_dir):\n",
    "            continue\n",
    "\n",
    "        logits_files = sorted(glob.glob(os.path.join(feature_dir, '*_logits.npy')))\n",
    "        embeddings_files = sorted(glob.glob(os.path.join(feature_dir, '*_embeddings.npy')))\n",
    "\n",
    "        num_logits = len(logits_files)\n",
    "        num_embeddings = len(embeddings_files)\n",
    "\n",
    "        if num_logits == num_embeddings:\n",
    "            print(f\"æ•°é‡ä¸€è‡´ã€‚\\n\")\n",
    "        else:\n",
    "            print(f\"æ•°é‡ä¸ä¸€è‡´ã€‚\")\n",
    "            print(f\"- logits æ–‡ä»¶æ•°é‡: {num_logits}\")\n",
    "            print(f\"- embeddings æ–‡ä»¶æ•°é‡: {num_embeddings}\")\n",
    "\n",
    "            # æ‰¾å‡ºç¼ºå¤±æˆ–å¤šä½™çš„æ–‡ä»¶\n",
    "            logits_indices = set([os.path.basename(f).split('_')[0] for f in logits_files])\n",
    "            embeddings_indices = set([os.path.basename(f).split('_')[0] for f in embeddings_files])\n",
    "\n",
    "            missing_in_embeddings = logits_indices - embeddings_indices\n",
    "            missing_in_logits = embeddings_indices - logits_indices\n",
    "\n",
    "            if missing_in_embeddings:\n",
    "                print(f\"åœ¨ embeddings æ–‡ä»¶å¤¹ä¸­ç¼ºå¤±: {sorted(missing_in_embeddings)}\")\n",
    "            if missing_in_logits:\n",
    "                print(f\"åœ¨ logits æ–‡ä»¶å¤¹ä¸­ç¼ºå¤±: {sorted(missing_in_logits)}\")\n",
    "            print()\n",
    "\n",
    "count_logit_embedding_files(feature_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1270,
     "status": "ok",
     "timestamp": 1754199254991,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "3GLMRuQjP5vC",
    "outputId": "694847e4-d1e1-4865-ac25-22d488c4466a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨æ£€æŸ¥æ–‡ä»¶å¤¹: /content/drive/MyDrive/AFP_work/esmc_600_train_pos\n",
      "  ğŸ“Š combined_logits.npy çš„å½¢çŠ¶: (1200, 1)\n",
      "  ğŸ“„ combined_logits.npy çš„å‰5è¡Œæ•°æ®ï¼š\n",
      "[[ForwardTrackData(sequence=tensor([[[-21.2500, -21.1250, -21.2500,  ..., -21.2500, -21.2500, -21.2500],\n",
      "           [-26.6250, -26.6250, -26.6250,  ..., -26.6250, -26.6250, -26.6250],\n",
      "           [-28.3750, -28.2500, -28.3750,  ..., -28.3750, -28.3750, -28.2500],\n",
      "           ...,\n",
      "           [-24.7500, -24.6250, -24.7500,  ..., -24.7500, -24.7500, -24.7500],\n",
      "           [-21.1250, -21.0000, -21.1250,  ..., -21.1250, -21.1250, -21.1250],\n",
      "           [-20.0000, -19.8750, -20.0000,  ..., -19.8750, -20.0000, -20.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.7500, -22.6250, -22.7500,  ..., -22.7500, -22.7500, -22.7500],\n",
      "           [-26.3750, -26.2500, -26.3750,  ..., -26.2500, -26.3750, -26.3750],\n",
      "           [-24.2500, -24.2500, -24.3750,  ..., -24.2500, -24.2500, -24.2500],\n",
      "           ...,\n",
      "           [-24.8750, -24.7500, -24.8750,  ..., -24.8750, -24.8750, -24.8750],\n",
      "           [-22.0000, -21.8750, -22.0000,  ..., -22.0000, -22.0000, -22.0000],\n",
      "           [-22.1250, -21.8750, -22.0000,  ..., -22.0000, -22.1250, -22.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-23.0000, -22.8750, -23.0000,  ..., -23.0000, -23.0000, -23.0000],\n",
      "           [-30.2500, -30.1250, -30.2500,  ..., -30.2500, -30.3750, -30.2500],\n",
      "           [-29.2500, -29.1250, -29.2500,  ..., -29.2500, -29.3750, -29.2500],\n",
      "           ...,\n",
      "           [-29.6250, -29.5000, -29.6250,  ..., -29.6250, -29.6250, -29.6250],\n",
      "           [-27.0000, -26.7500, -26.8750,  ..., -26.8750, -27.0000, -26.8750],\n",
      "           [-24.5000, -24.5000, -24.5000,  ..., -24.5000, -24.6250, -24.5000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.8750, -22.7500, -22.8750,  ..., -22.8750, -23.0000, -23.0000],\n",
      "           [-25.8750, -25.7500, -25.8750,  ..., -25.7500, -25.8750, -25.8750],\n",
      "           [-26.3750, -26.2500, -26.3750,  ..., -26.2500, -26.3750, -26.3750],\n",
      "           ...,\n",
      "           [-27.2500, -27.1250, -27.2500,  ..., -27.1250, -27.2500, -27.2500],\n",
      "           [-24.1250, -24.0000, -24.1250,  ..., -24.1250, -24.1250, -24.1250],\n",
      "           [-23.6250, -23.5000, -23.6250,  ..., -23.5000, -23.6250, -23.6250]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-20.8750, -20.7500, -20.8750,  ..., -20.8750, -20.8750, -20.8750],\n",
      "           [-23.7500, -23.5000, -23.5000,  ..., -23.6250, -23.7500, -23.7500],\n",
      "           [-24.8750, -24.7500, -24.7500,  ..., -24.8750, -24.8750, -24.7500],\n",
      "           ...,\n",
      "           [-26.1250, -26.1250, -26.2500,  ..., -26.1250, -26.2500, -26.2500],\n",
      "           [-17.2500, -17.1250, -17.2500,  ..., -17.2500, -17.2500, -17.1250],\n",
      "           [-18.5000, -18.3750, -18.5000,  ..., -18.5000, -18.6250, -18.5000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]]\n",
      "\n",
      "  ğŸ“Š /content/drive/MyDrive/AFP_work/esmc_600_train_pos combined_embeddings.npy çš„å½¢çŠ¶: (1200, 1152)\n",
      "  ğŸ“„ /content/drive/MyDrive/AFP_work/esmc_600_train_pos combined_embeddings.npy çš„å‰5è¡Œæ•°æ®ï¼š\n",
      "[[-0.00485136  0.01375214 -0.01221031 ... -0.00554397  0.00420806\n",
      "   0.00168132]\n",
      " [-0.00074414  0.02556396 -0.01695777 ... -0.00902656  0.00836704\n",
      "   0.01187631]\n",
      " [-0.01295047  0.02892382 -0.02908008 ... -0.0041075  -0.00061104\n",
      "   0.00659786]\n",
      " [-0.00783114  0.026164   -0.02013981 ... -0.0024441  -0.0033889\n",
      "   0.00785554]\n",
      " [ 0.00682404 -0.01013107 -0.01611771 ... -0.01646785 -0.01872984\n",
      "   0.00195817]]\n",
      "\n",
      "æ­£åœ¨æ£€æŸ¥æ–‡ä»¶å¤¹: /content/drive/MyDrive/AFP_work/esmc_600_train_neg\n",
      "  ğŸ“Š combined_logits.npy çš„å½¢çŠ¶: (1200, 1)\n",
      "  ğŸ“„ combined_logits.npy çš„å‰5è¡Œæ•°æ®ï¼š\n",
      "[[ForwardTrackData(sequence=tensor([[[-23.5000, -23.3750, -23.5000,  ..., -23.5000, -23.5000, -23.6250],\n",
      "           [-23.3750, -23.2500, -23.3750,  ..., -23.3750, -23.3750, -23.3750],\n",
      "           [-26.6250, -26.5000, -26.6250,  ..., -26.6250, -26.6250, -26.6250],\n",
      "           ...,\n",
      "           [-21.3750, -21.2500, -21.3750,  ..., -21.3750, -21.3750, -21.3750],\n",
      "           [-17.2500, -17.1250, -17.2500,  ..., -17.2500, -17.2500, -17.2500],\n",
      "           [-20.1250, -20.0000, -20.0000,  ..., -20.1250, -20.0000, -20.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.5000, -22.3750, -22.6250,  ..., -22.5000, -22.5000, -22.6250],\n",
      "           [-23.2500, -23.2500, -23.2500,  ..., -23.2500, -23.2500, -23.2500],\n",
      "           [-24.6250, -24.6250, -24.6250,  ..., -24.6250, -24.6250, -24.6250],\n",
      "           ...,\n",
      "           [-21.7500, -21.6250, -21.7500,  ..., -21.7500, -21.7500, -21.7500],\n",
      "           [-20.1250, -20.1250, -20.1250,  ..., -20.1250, -20.2500, -20.1250],\n",
      "           [-22.3750, -22.2500, -22.3750,  ..., -22.2500, -22.3750, -22.3750]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-23.5000, -23.3750, -23.5000,  ..., -23.5000, -23.5000, -23.6250],\n",
      "           [-27.2500, -27.0000, -27.1250,  ..., -27.1250, -27.2500, -27.1250],\n",
      "           [-27.5000, -27.3750, -27.5000,  ..., -27.3750, -27.5000, -27.5000],\n",
      "           ...,\n",
      "           [-28.2500, -28.1250, -28.2500,  ..., -28.1250, -28.2500, -28.2500],\n",
      "           [-23.2500, -23.1250, -23.2500,  ..., -23.1250, -23.2500, -23.2500],\n",
      "           [-23.6250, -23.5000, -23.6250,  ..., -23.5000, -23.6250, -23.6250]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-21.6250, -21.5000, -21.6250,  ..., -21.6250, -21.6250, -21.6250],\n",
      "           [-25.0000, -24.8750, -25.0000,  ..., -25.0000, -25.0000, -25.0000],\n",
      "           [-26.7500, -26.6250, -26.7500,  ..., -26.7500, -26.7500, -26.6250],\n",
      "           ...,\n",
      "           [-25.1250, -25.0000, -25.1250,  ..., -25.1250, -25.2500, -25.1250],\n",
      "           [-20.7500, -20.6250, -20.7500,  ..., -20.6250, -20.7500, -20.7500],\n",
      "           [-21.7500, -21.7500, -21.7500,  ..., -21.6250, -21.8750, -21.8750]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-23.0000, -22.8750, -23.0000,  ..., -22.8750, -23.0000, -23.0000],\n",
      "           [-27.6250, -27.5000, -27.6250,  ..., -27.5000, -27.6250, -27.6250],\n",
      "           [-27.3750, -27.1250, -27.3750,  ..., -27.2500, -27.3750, -27.2500],\n",
      "           ...,\n",
      "           [-27.6250, -27.5000, -27.6250,  ..., -27.5000, -27.6250, -27.5000],\n",
      "           [-24.1250, -24.0000, -24.1250,  ..., -24.0000, -24.1250, -24.1250],\n",
      "           [-22.6250, -22.3750, -22.5000,  ..., -22.5000, -22.6250, -22.5000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]]\n",
      "\n",
      "  ğŸ“Š /content/drive/MyDrive/AFP_work/esmc_600_train_neg combined_embeddings.npy çš„å½¢çŠ¶: (1200, 1152)\n",
      "  ğŸ“„ /content/drive/MyDrive/AFP_work/esmc_600_train_neg combined_embeddings.npy çš„å‰5è¡Œæ•°æ®ï¼š\n",
      "[[-0.01458055  0.02122218 -0.02280448 ... -0.02389562 -0.01433752\n",
      "   0.01356946]\n",
      " [-0.00363107  0.0026253   0.00232742 ...  0.00368061  0.00893849\n",
      "   0.00313839]\n",
      " [ 0.00705204  0.03198364 -0.02578153 ...  0.0242763   0.0118436\n",
      "  -0.00296395]\n",
      " [-0.00482746  0.0018164   0.00409979 ... -0.00863889  0.0098847\n",
      "   0.00861327]\n",
      " [ 0.01053481  0.04374865 -0.02046082 ...  0.01394143 -0.00197555\n",
      "   0.00511242]]\n",
      "\n",
      "æ­£åœ¨æ£€æŸ¥æ–‡ä»¶å¤¹: /content/drive/MyDrive/AFP_work/esmc_600_test_pos\n",
      "  ğŸ“Š combined_logits.npy çš„å½¢çŠ¶: (308, 1)\n",
      "  ğŸ“„ combined_logits.npy çš„å‰5è¡Œæ•°æ®ï¼š\n",
      "[[ForwardTrackData(sequence=tensor([[[-22.1250, -22.0000, -22.0000,  ..., -22.1250, -22.1250, -22.1250],\n",
      "           [-25.8750, -25.7500, -25.7500,  ..., -25.8750, -25.8750, -25.8750],\n",
      "           [-24.2500, -24.0000, -24.1250,  ..., -24.1250, -24.1250, -24.1250],\n",
      "           ...,\n",
      "           [-24.2500, -24.2500, -24.2500,  ..., -24.1250, -24.2500, -24.2500],\n",
      "           [-18.8750, -18.8750, -18.8750,  ..., -18.7500, -18.8750, -18.8750],\n",
      "           [-20.7500, -20.6250, -20.7500,  ..., -20.7500, -20.7500, -20.7500]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-21.5000, -21.3750, -21.5000,  ..., -21.5000, -21.6250, -21.5000],\n",
      "           [-27.3750, -27.2500, -27.3750,  ..., -27.2500, -27.3750, -27.3750],\n",
      "           [-22.7500, -22.6250, -22.7500,  ..., -22.7500, -22.7500, -22.7500],\n",
      "           ...,\n",
      "           [-26.2500, -26.1250, -26.1250,  ..., -26.1250, -26.2500, -26.1250],\n",
      "           [-21.7500, -21.6250, -21.6250,  ..., -21.6250, -21.6250, -21.6250],\n",
      "           [-19.5000, -19.3750, -19.5000,  ..., -19.5000, -19.5000, -19.5000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.5000, -22.2500, -22.3750,  ..., -22.3750, -22.5000, -22.3750],\n",
      "           [-24.1250, -23.8750, -23.7500,  ..., -23.8750, -24.0000, -24.0000],\n",
      "           [-25.6250, -25.6250, -25.6250,  ..., -25.6250, -25.6250, -25.6250],\n",
      "           ...,\n",
      "           [-26.6250, -26.5000, -26.6250,  ..., -26.6250, -26.7500, -26.5000],\n",
      "           [-17.0000, -16.8750, -17.0000,  ..., -16.8750, -17.0000, -16.8750],\n",
      "           [-19.2500, -19.1250, -19.2500,  ..., -19.1250, -19.2500, -19.2500]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.0000, -21.8750, -22.0000,  ..., -21.8750, -22.0000, -22.0000],\n",
      "           [-25.2500, -25.1250, -25.2500,  ..., -25.1250, -25.2500, -25.2500],\n",
      "           [-26.0000, -25.8750, -26.0000,  ..., -26.0000, -26.1250, -26.0000],\n",
      "           ...,\n",
      "           [-25.5000, -25.3750, -25.5000,  ..., -25.5000, -25.5000, -25.5000],\n",
      "           [-18.5000, -18.5000, -18.5000,  ..., -18.5000, -18.5000, -18.5000],\n",
      "           [-21.2500, -21.1250, -21.2500,  ..., -21.1250, -21.2500, -21.2500]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.0000, -21.7500, -22.0000,  15.8750,  17.0000,  19.3750,  21.0000,\n",
      "             17.3750,  17.7500,  17.1250,  15.2500,  14.7500,  16.0000,  17.0000,\n",
      "             18.8750,  17.3750,  11.8125,  14.3125,  17.0000,  13.8125,  15.6250,\n",
      "             13.5000,  17.1250,  16.3750,  17.0000,   0.4453,   0.5898, -12.0625,\n",
      "             -5.8438, -21.8750, -22.0000, -22.0000, -21.8750, -21.7500, -22.1250,\n",
      "            -21.8750, -22.0000, -22.0000, -21.8750, -21.8750, -21.8750, -21.7500,\n",
      "            -21.7500, -21.8750, -22.0000, -21.7500, -22.0000, -22.0000, -21.7500,\n",
      "            -22.0000, -21.8750, -21.8750, -21.6250, -21.8750, -21.7500, -22.0000,\n",
      "            -21.8750, -21.7500, -21.8750, -21.7500, -21.8750, -22.0000, -22.0000,\n",
      "            -22.0000],\n",
      "           [-22.7500, -22.6250, -22.6250,   3.0000,  16.1250,  13.6875,  11.4375,\n",
      "             14.7500,  12.2500,   7.3125,   8.5625,  10.2500,  16.7500,   6.8125,\n",
      "             11.5625,   8.8750,   8.1875,   8.9375,  22.7500,  13.3750,  14.1875,\n",
      "             10.5000,  13.0000,  11.0625,  11.6250, -18.7500, -20.0000, -21.1250,\n",
      "            -10.0625, -22.6250, -22.7500, -22.7500, -22.6250, -22.6250, -22.6250,\n",
      "            -22.5000, -22.6250, -22.5000, -22.5000, -22.6250, -22.6250, -22.6250,\n",
      "            -22.6250, -22.5000, -22.6250, -22.6250, -22.7500, -22.8750, -22.6250,\n",
      "            -22.6250, -22.5000, -22.7500, -22.6250, -22.7500, -22.6250, -22.6250,\n",
      "            -22.6250, -22.7500, -22.5000, -22.6250, -22.6250, -22.5000, -22.6250,\n",
      "            -22.7500],\n",
      "           [-21.0000, -21.0000, -21.1250,   9.6250,  27.8750,  20.0000,  16.7500,\n",
      "             22.7500,  16.5000,  11.8125,  15.7500,  16.1250,  25.5000,   9.6875,\n",
      "             17.5000,  15.0000,  14.5625,  12.8750,  24.3750,  17.0000,  21.3750,\n",
      "             15.5000,  19.3750,  13.1875,  18.8750, -24.0000, -26.6250, -22.0000,\n",
      "             -5.0000, -21.0000, -21.1250, -21.1250, -21.0000, -20.8750, -21.0000,\n",
      "            -21.0000, -21.1250, -21.0000, -21.0000, -21.0000, -21.0000, -21.0000,\n",
      "            -20.8750, -20.8750, -21.1250, -21.1250, -21.0000, -21.1250, -21.0000,\n",
      "            -21.0000, -21.0000, -21.0000, -21.0000, -21.1250, -21.2500, -21.1250,\n",
      "            -21.1250, -21.0000, -21.0000, -21.0000, -21.0000, -21.1250, -21.0000,\n",
      "            -21.1250],\n",
      "           [-19.7500, -19.6250, -19.7500,   9.6250,  15.0625,  18.0000,  21.2500,\n",
      "             12.0000,  20.8750,  13.5000,  15.8750,  16.7500,  11.1250,  14.5000,\n",
      "             24.7500,  16.0000,  17.0000,  17.6250,  12.5625,  11.9375,  12.1250,\n",
      "             16.8750,  10.7500,  10.9375,  16.6250, -20.5000, -14.8125, -19.6250,\n",
      "             -4.9375, -19.8750, -19.6250, -19.7500, -19.7500, -19.6250, -19.6250,\n",
      "            -19.6250, -19.6250, -19.7500, -19.6250, -19.7500, -19.7500, -19.6250,\n",
      "            -19.6250, -19.7500, -19.7500, -19.6250, -19.7500, -19.7500, -19.7500,\n",
      "            -19.7500, -19.7500, -19.7500, -19.5000, -19.7500, -19.7500, -19.7500,\n",
      "            -19.6250, -19.6250, -19.7500, -19.7500, -19.7500, -19.8750, -19.6250,\n",
      "            -19.6250],\n",
      "           [-18.2500, -18.1250, -18.2500,   4.8438,  20.8750,  23.1250,  17.8750,\n",
      "             21.7500,  18.8750,  14.3125,  14.2500,  20.1250,  21.5000,  12.9375,\n",
      "             14.2500,  15.3750,  16.7500,  14.9375,  18.5000,  16.0000,  20.8750,\n",
      "             17.5000,  11.9375,  10.1875,  16.7500, -27.7500, -23.5000, -25.7500,\n",
      "              0.1143, -18.2500, -18.2500, -18.2500, -18.2500, -18.0000, -18.2500,\n",
      "            -18.2500, -18.2500, -18.1250, -18.1250, -18.1250, -18.1250, -18.1250,\n",
      "            -18.1250, -18.1250, -18.1250, -18.2500, -18.2500, -18.2500, -18.1250,\n",
      "            -18.0000, -18.1250, -18.1250, -18.1250, -18.2500, -18.1250, -18.2500,\n",
      "            -18.1250, -18.1250, -18.2500, -18.2500, -18.3750, -18.2500, -18.2500,\n",
      "            -18.2500],\n",
      "           [-19.5000, -19.3750, -19.5000,   8.1250,  25.3750,  25.8750,  15.9375,\n",
      "             27.1250,  16.3750,  12.5000,  16.0000,  18.6250,  27.5000,   9.9375,\n",
      "             15.5625,  15.2500,  13.5000,  10.0625,  20.2500,  15.6250,  22.0000,\n",
      "             12.6875,  16.1250,  14.2500,  18.1250, -45.5000, -32.0000, -39.2500,\n",
      "             -0.7656, -19.5000, -19.5000, -19.3750, -19.3750, -19.2500, -19.3750,\n",
      "            -19.2500, -19.3750, -19.2500, -19.2500, -19.2500, -19.2500, -19.2500,\n",
      "            -19.3750, -19.3750, -19.2500, -19.3750, -19.3750, -19.3750, -19.2500,\n",
      "            -19.2500, -19.3750, -19.2500, -19.1250, -19.5000, -19.3750, -19.5000,\n",
      "            -19.3750, -19.3750, -19.2500, -19.3750, -19.5000, -19.2500, -19.3750,\n",
      "            -19.3750],\n",
      "           [-17.8750, -17.8750, -18.0000,   7.6250,  17.5000,  25.5000,  23.6250,\n",
      "             18.8750,  24.3750,  16.1250,  15.6875,  23.6250,  16.7500,  13.0625,\n",
      "             20.2500,  16.8750,  16.6250,  16.1250,  17.3750,  15.2500,  18.2500,\n",
      "             17.5000,  12.8750,  15.3750,  17.5000, -22.0000, -10.8750, -20.6250,\n",
      "              1.9062, -18.0000, -17.7500, -17.8750, -17.8750, -17.6250, -17.7500,\n",
      "            -17.8750, -17.8750, -17.8750, -17.8750, -17.8750, -17.7500, -17.7500,\n",
      "            -17.7500, -17.8750, -17.8750, -17.8750, -18.0000, -18.0000, -17.8750,\n",
      "            -17.8750, -17.8750, -17.8750, -17.8750, -18.0000, -17.7500, -17.8750,\n",
      "            -17.8750, -17.7500, -17.8750, -17.8750, -18.0000, -17.8750, -17.8750,\n",
      "            -17.8750],\n",
      "           [-23.7500, -23.6250, -23.7500,  13.8125,  15.1875,  16.8750,  21.3750,\n",
      "             13.4375,  22.8750,  22.5000,  24.0000,  20.5000,  14.6875,  21.2500,\n",
      "             17.5000,  29.6250,  25.0000,  25.6250,  12.8750,  15.7500,  16.8750,\n",
      "             22.6250,  14.3750,  14.3125,  21.3750, -10.1250, -15.1875, -12.4375,\n",
      "             -2.2656, -23.7500, -23.6250, -23.7500, -23.6250, -23.5000, -23.5000,\n",
      "            -23.6250, -23.6250, -23.5000, -23.6250, -23.8750, -23.7500, -23.6250,\n",
      "            -23.6250, -23.6250, -23.6250, -23.7500, -23.6250, -23.6250, -23.6250,\n",
      "            -23.7500, -23.7500, -23.6250, -23.5000, -23.7500, -23.6250, -23.5000,\n",
      "            -23.6250, -23.5000, -23.6250, -23.6250, -23.7500, -23.7500, -23.7500,\n",
      "            -23.6250],\n",
      "           [-20.3750, -20.2500, -20.5000,   6.5312,  27.7500,  23.3750,  16.7500,\n",
      "             25.0000,  19.1250,  15.4375,  17.5000,  22.1250,  23.7500,  10.1250,\n",
      "             15.1875,  18.8750,  17.7500,  14.2500,  23.2500,  20.8750,  23.8750,\n",
      "             19.8750,  17.2500,  15.8750,  21.3750, -22.6250, -12.3125, -23.2500,\n",
      "             -0.1982, -20.3750, -20.5000, -20.3750, -20.5000, -20.2500, -20.2500,\n",
      "            -20.3750, -20.3750, -20.3750, -20.2500, -20.2500, -20.2500, -20.2500,\n",
      "            -20.3750, -20.3750, -20.3750, -20.3750, -20.3750, -20.3750, -20.3750,\n",
      "            -20.1250, -20.1250, -20.2500, -20.2500, -20.3750, -20.3750, -20.3750,\n",
      "            -20.3750, -20.3750, -20.3750, -20.3750, -20.5000, -20.3750, -20.3750,\n",
      "            -20.2500],\n",
      "           [-20.0000, -19.8750, -20.0000,   7.5625,  28.3750,  22.3750,  16.0000,\n",
      "             24.5000,  16.3750,  10.1875,  17.3750,  16.7500,  25.0000,   9.4375,\n",
      "             16.6250,  15.4375,  13.0000,   9.8750,  23.5000,  14.6250,  21.7500,\n",
      "             15.0625,  14.3750,  16.5000,  21.7500, -20.0000, -12.7500, -21.6250,\n",
      "             -3.1250, -20.0000, -20.0000, -20.0000, -20.0000, -19.7500, -20.0000,\n",
      "            -19.8750, -19.8750, -19.8750, -19.8750, -19.8750, -19.8750, -19.8750,\n",
      "            -19.7500, -19.8750, -20.0000, -19.8750, -20.0000, -20.0000, -19.8750,\n",
      "            -19.7500, -19.7500, -19.8750, -19.7500, -20.0000, -20.0000, -20.0000,\n",
      "            -20.0000, -20.0000, -19.8750, -20.0000, -20.0000, -20.0000, -20.0000,\n",
      "            -20.0000],\n",
      "           [-19.1250, -19.1250, -19.2500,  11.5625,  12.8750,  18.2500,  24.7500,\n",
      "             15.2500,  22.3750,  17.5000,  16.2500,  19.6250,  12.8125,  17.3750,\n",
      "             17.8750,  17.1250,  16.8750,  17.7500,  13.0625,  11.8750,  13.1250,\n",
      "             16.7500,  10.4375,  14.9375,  15.9375, -13.1250,  -3.9531, -10.6875,\n",
      "             -0.9688, -19.2500, -19.1250, -19.2500, -19.1250, -19.0000, -19.1250,\n",
      "            -19.0000, -19.1250, -19.1250, -19.1250, -19.0000, -19.0000, -19.0000,\n",
      "            -19.1250, -19.1250, -19.1250, -19.0000, -19.1250, -19.1250, -19.0000,\n",
      "            -19.1250, -19.1250, -19.1250, -19.0000, -19.1250, -19.1250, -19.0000,\n",
      "            -19.0000, -19.1250, -19.1250, -19.1250, -19.1250, -19.1250, -19.2500,\n",
      "            -19.1250],\n",
      "           [-22.5000, -22.3750, -22.5000,  10.5000,  14.3750,  18.5000,  25.6250,\n",
      "             15.7500,  22.7500,  16.8750,  18.1250,  18.6250,  13.9375,  19.5000,\n",
      "             18.1250,  19.5000,  17.8750,  22.3750,  13.6875,  13.0625,  14.5000,\n",
      "             17.2500,  13.1875,  15.0000,  14.6250,  -9.9375, -11.1875,  -8.6250,\n",
      "             -2.9688, -22.5000, -22.3750, -22.3750, -22.3750, -22.2500, -22.2500,\n",
      "            -22.3750, -22.5000, -22.3750, -22.3750, -22.2500, -22.3750, -22.3750,\n",
      "            -22.3750, -22.3750, -22.5000, -22.3750, -22.3750, -22.5000, -22.3750,\n",
      "            -22.3750, -22.3750, -22.3750, -22.2500, -22.5000, -22.3750, -22.3750,\n",
      "            -22.3750, -22.3750, -22.3750, -22.3750, -22.5000, -22.3750, -22.5000,\n",
      "            -22.5000],\n",
      "           [-21.1250, -21.0000, -21.1250,   6.1250,  28.0000,  21.1250,  16.0000,\n",
      "             25.5000,  18.2500,  15.2500,  18.5000,  17.8750,  26.2500,  12.9375,\n",
      "             17.1250,  19.3750,  17.7500,  16.1250,  23.5000,  18.6250,  23.6250,\n",
      "             18.7500,  17.7500,  15.6875,  20.8750, -21.2500,  -8.4375, -20.1250,\n",
      "             -3.4062, -21.1250, -21.2500, -21.0000, -21.1250, -21.0000, -21.1250,\n",
      "            -21.1250, -21.0000, -20.8750, -20.8750, -21.0000, -21.0000, -20.8750,\n",
      "            -20.8750, -21.0000, -21.1250, -20.8750, -21.1250, -21.1250, -21.1250,\n",
      "            -20.8750, -20.8750, -21.0000, -21.0000, -21.1250, -21.1250, -21.1250,\n",
      "            -21.0000, -21.1250, -21.0000, -21.0000, -21.1250, -21.0000, -21.0000,\n",
      "            -21.0000],\n",
      "           [-15.0625, -15.0625, -15.0625,  14.0000,  25.0000,  18.0000,  14.8750,\n",
      "             22.6250,  16.7500,  12.0000,  16.2500,  19.2500,  23.0000,  11.2500,\n",
      "             15.6875,  17.0000,  15.1250,  14.1875,  22.7500,  14.7500,  21.8750,\n",
      "             15.7500,  14.8750,  14.6250,  18.2500, -19.7500,  -7.1250, -19.7500,\n",
      "              0.2910, -15.0625, -15.2500, -15.1250, -15.1250, -14.8750, -15.1875,\n",
      "            -15.1250, -14.9375, -15.0000, -15.0000, -14.9375, -15.0000, -15.0000,\n",
      "            -14.9375, -15.0000, -15.0625, -15.0000, -15.0000, -15.1875, -15.0625,\n",
      "            -14.9375, -15.0625, -15.0000, -15.0000, -15.1250, -15.1250, -15.1250,\n",
      "            -14.9375, -15.0625, -15.1250, -15.0000, -15.0000, -15.0000, -15.0625,\n",
      "            -15.0000],\n",
      "           [-13.3125, -13.1875, -13.5000,  14.6875,  11.6250,  18.6250,  26.3750,\n",
      "             15.5625,  20.7500,  22.0000,  17.1250,  18.7500,  10.6875,  22.6250,\n",
      "             23.2500,  19.3750,  17.3750,  21.6250,  13.8750,  12.8125,  12.2500,\n",
      "             18.0000,  12.5625,  17.8750,  20.2500, -15.6250, -10.5000, -22.6250,\n",
      "              3.0469, -13.5000, -13.3750, -13.4375, -13.6250, -13.2500, -13.1875,\n",
      "            -13.1875, -13.3125, -13.1875, -13.3750, -13.2500, -13.1875, -13.2500,\n",
      "            -13.2500, -13.2500, -13.3125, -13.3750, -13.3750, -13.4375, -13.2500,\n",
      "            -13.3750, -13.3750, -13.2500, -13.1250, -13.3750, -13.2500, -13.1875,\n",
      "            -13.2500, -13.1875, -13.4375, -13.1875, -13.1875, -13.3125, -13.3750,\n",
      "            -13.1875]]], device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]]\n",
      "\n",
      "  ğŸ“Š /content/drive/MyDrive/AFP_work/esmc_600_test_pos combined_embeddings.npy çš„å½¢çŠ¶: (308, 1152)\n",
      "  ğŸ“„ /content/drive/MyDrive/AFP_work/esmc_600_test_pos combined_embeddings.npy çš„å‰5è¡Œæ•°æ®ï¼š\n",
      "[[ 0.00955147 -0.01634502 -0.0180667  ... -0.02730146 -0.01468285\n",
      "   0.00568047]\n",
      " [ 0.02001305 -0.01209932 -0.0231419  ...  0.00063677 -0.0013002\n",
      "  -0.01522294]\n",
      " [ 0.00369677 -0.00684411 -0.01874603 ... -0.01421224 -0.02489749\n",
      "   0.0071571 ]\n",
      " [-0.00890252  0.013904   -0.01096845 ... -0.00885079  0.0251286\n",
      "  -0.00218562]\n",
      " [ 0.01952402 -0.0061013   0.00099249 ... -0.01355654 -0.00697304\n",
      "  -0.01040544]]\n",
      "\n",
      "æ­£åœ¨æ£€æŸ¥æ–‡ä»¶å¤¹: /content/drive/MyDrive/AFP_work/esmc_600_test_neg\n",
      "  ğŸ“Š combined_logits.npy çš„å½¢çŠ¶: (308, 1)\n",
      "  ğŸ“„ combined_logits.npy çš„å‰5è¡Œæ•°æ®ï¼š\n",
      "[[ForwardTrackData(sequence=tensor([[[-22.7500, -22.6250, -22.7500,  ..., -22.7500, -22.7500, -22.8750],\n",
      "           [-24.5000, -24.5000, -24.5000,  ..., -24.3750, -24.6250, -24.5000],\n",
      "           [-29.0000, -28.8750, -28.8750,  ..., -28.8750, -29.0000, -28.8750],\n",
      "           ...,\n",
      "           [-20.5000, -20.3750, -20.3750,  ..., -20.5000, -20.5000, -20.5000],\n",
      "           [-19.7500, -19.6250, -19.7500,  ..., -19.7500, -19.7500, -19.7500],\n",
      "           [-19.6250, -19.5000, -19.5000,  ..., -19.5000, -19.6250, -19.6250]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.5000, -22.3750, -22.5000,  ..., -22.5000, -22.5000, -22.5000],\n",
      "           [-24.3750, -24.3750, -24.3750,  ..., -24.3750, -24.3750, -24.3750],\n",
      "           [-25.2500, -25.1250, -25.1250,  ..., -25.2500, -25.2500, -25.2500],\n",
      "           ...,\n",
      "           [-23.1250, -23.1250, -23.2500,  ..., -23.1250, -23.2500, -23.2500],\n",
      "           [-18.2500, -18.1250, -18.2500,  ..., -18.2500, -18.2500, -18.2500],\n",
      "           [-19.3750, -19.2500, -19.3750,  ..., -19.3750, -19.3750, -19.3750]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.3750, -22.2500, -22.3750,  ..., -22.3750, -22.3750, -22.3750],\n",
      "           [-22.8750, -22.7500, -22.8750,  ..., -22.7500, -22.8750, -22.8750],\n",
      "           [-24.5000, -24.5000, -24.5000,  ..., -24.5000, -24.5000, -24.5000],\n",
      "           ...,\n",
      "           [-26.0000, -25.8750, -25.8750,  ..., -25.8750, -25.8750, -26.0000],\n",
      "           [-20.0000, -20.0000, -20.0000,  ..., -20.0000, -20.1250, -20.0000],\n",
      "           [-20.7500, -20.7500, -20.7500,  ..., -20.7500, -20.8750, -20.7500]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-23.0000, -22.8750, -23.0000,  ..., -23.0000, -23.0000, -23.1250],\n",
      "           [-21.7500, -21.6250, -21.6250,  ..., -21.6250, -21.7500, -21.7500],\n",
      "           [-21.7500, -21.7500, -21.8750,  ..., -21.7500, -21.7500, -21.8750],\n",
      "           ...,\n",
      "           [-26.0000, -25.8750, -26.0000,  ..., -25.8750, -26.0000, -26.0000],\n",
      "           [-19.5000, -19.5000, -19.6250,  ..., -19.5000, -19.6250, -19.6250],\n",
      "           [-22.0000, -22.0000, -22.0000,  ..., -22.0000, -22.0000, -22.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.8750, -22.6250, -22.8750,  ..., -22.7500, -22.7500, -22.8750],\n",
      "           [-23.6250, -23.5000, -23.6250,  ..., -23.6250, -23.6250, -23.6250],\n",
      "           [-22.3750, -22.2500, -22.2500,  ..., -22.2500, -22.3750, -22.3750],\n",
      "           ...,\n",
      "           [-21.5000, -21.2500, -21.3750,  ..., -21.3750, -21.5000, -21.3750],\n",
      "           [-20.0000, -20.0000, -20.1250,  ..., -20.0000, -20.1250, -20.1250],\n",
      "           [-23.0000, -22.8750, -23.0000,  ..., -22.8750, -23.0000, -23.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]]\n",
      "\n",
      "  ğŸ“Š /content/drive/MyDrive/AFP_work/esmc_600_test_neg combined_embeddings.npy çš„å½¢çŠ¶: (308, 1152)\n",
      "  ğŸ“„ /content/drive/MyDrive/AFP_work/esmc_600_test_neg combined_embeddings.npy çš„å‰5è¡Œæ•°æ®ï¼š\n",
      "[[-0.01633092  0.01812963 -0.00544356 ...  0.00477491  0.00082364\n",
      "   0.00811116]\n",
      " [ 0.00572116  0.02359726 -0.01338577 ...  0.00237886 -0.00929524\n",
      "  -0.00353848]\n",
      " [-0.00797354  0.01936996 -0.01320011 ... -0.00628619 -0.00710818\n",
      "  -0.00221466]\n",
      " [-0.02267375  0.0115626  -0.02234262 ... -0.02272365 -0.0095801\n",
      "   0.00937222]\n",
      " [ 0.00310926 -0.01185363 -0.01291295 ...  0.00137915  0.02103562\n",
      "   0.00094701]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# éå†æ¯ä¸ªç‰¹å¾æ–‡ä»¶å¤¹ï¼ŒåŠ è½½å¹¶æ‰“å° combined_logits.npy å’Œ combined_embeddings.npy çš„ç»´åº¦\n",
    "def inspect_combined_files(feature_dirs):\n",
    "    \"\"\"\n",
    "    éå†æ¯ä¸ªç‰¹å¾æ–‡ä»¶å¤¹ï¼ŒåŠ è½½å¹¶æ‰“å° combined_logits.npy å’Œ combined_embeddings.npy çš„ç»´åº¦ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        feature_dirs (List[str]): ç‰¹å¾æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    for feature_dir in feature_dirs:\n",
    "        print(f\"æ­£åœ¨æ£€æŸ¥æ–‡ä»¶å¤¹: {feature_dir}\")\n",
    "\n",
    "        # å®šä¹‰ combined_logits.npy å’Œ combined_embeddings.npy çš„è·¯å¾„\n",
    "        combined_logits_path = os.path.join(feature_dir, 'combined_logits.npy')\n",
    "        combined_embeddings_path = os.path.join(feature_dir, 'combined_embeddings.npy')\n",
    "\n",
    "        # æ£€æŸ¥å¹¶åŠ è½½ combined_logits.npy\n",
    "        if os.path.isfile(combined_logits_path):\n",
    "            try:\n",
    "                combined_logits = np.load(combined_logits_path, allow_pickle=True)\n",
    "                print(f\"  {os.path.basename(combined_logits_path)} çš„å½¢çŠ¶: {combined_logits.shape}\")\n",
    "                print(f\"  {os.path.basename(combined_logits_path)} çš„å‰5è¡Œæ•°æ®ï¼š\\n{combined_logits[:5]}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"  åŠ è½½ {os.path.basename(combined_logits_path)} æ—¶å‡ºé”™: {e}\")\n",
    "        else:\n",
    "            print(f\" {os.path.basename(combined_logits_path)} ä¸å­˜åœ¨ã€‚\")\n",
    "\n",
    "        # æ£€æŸ¥å¹¶åŠ è½½ combined_embeddings.npy\n",
    "        if os.path.isfile(combined_embeddings_path):\n",
    "            try:\n",
    "                combined_embeddings = np.load(combined_embeddings_path, allow_pickle=True)\n",
    "                print(f\" {feature_dir} {os.path.basename(combined_embeddings_path)} çš„å½¢çŠ¶: {combined_embeddings.shape}\")\n",
    "                print(f\" {feature_dir} {os.path.basename(combined_embeddings_path)} çš„å‰5è¡Œæ•°æ®ï¼š\\n{combined_embeddings[:5]}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"åŠ è½½ {os.path.basename(combined_embeddings_path)} æ—¶å‡ºé”™: {e}\\n\")\n",
    "        else:\n",
    "            print(f\"{os.path.basename(combined_embeddings_path)} ä¸å­˜åœ¨ã€‚\\n\")\n",
    "            \n",
    "inspect_combined_files(feature_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pnBCTCqhdFH"
   },
   "source": [
    "## å¤„ç†ç»“æ„ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1754200816480,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "HVvzEYr8hfQt",
    "outputId": "963779f9-1fe3-415b-805a-4dfa9b9c6033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹: /content/drive/MyDrive/AFP_work/pdb/train_pos\n",
      "âœ… æ–‡ä»¶å¤¹ 'train_pos' ä¸­å…±æœ‰ 1140 ä¸ª .pdb æ–‡ä»¶ã€‚\n",
      "\n",
      "æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹: /content/drive/MyDrive/AFP_work/pdb/train_neg\n",
      "âœ… æ–‡ä»¶å¤¹ 'train_neg' ä¸­å…±æœ‰ 1200 ä¸ª .pdb æ–‡ä»¶ã€‚\n",
      "\n",
      "æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹: /content/drive/MyDrive/AFP_work/pdb/test_pos\n",
      "âœ… æ–‡ä»¶å¤¹ 'test_pos' ä¸­å…±æœ‰ 367 ä¸ª .pdb æ–‡ä»¶ã€‚\n",
      "\n",
      "æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹: /content/drive/MyDrive/AFP_work/pdb/test_neg\n",
      "âœ… æ–‡ä»¶å¤¹ 'test_neg' ä¸­å…±æœ‰ 308 ä¸ª .pdb æ–‡ä»¶ã€‚\n",
      "\n",
      "ğŸ“Š å„æ–‡ä»¶å¤¹ä¸­ .pdb æ–‡ä»¶çš„æ•°é‡ç»Ÿè®¡ï¼š\n",
      "      folder                                           path  pdb_file_count\n",
      "0  train_pos  /content/drive/MyDrive/AFP_work/pdb/train_pos            1140\n",
      "1  train_neg  /content/drive/MyDrive/AFP_work/pdb/train_neg            1200\n",
      "2   test_pos   /content/drive/MyDrive/AFP_work/pdb/test_pos             367\n",
      "3   test_neg   /content/drive/MyDrive/AFP_work/pdb/test_neg             308\n",
      "\n",
      "âœ… ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ° '/content/drive/MyDrive/AFP_work/pdb/pdb_file_counts.csv'ã€‚\n",
      "\n",
      "ğŸ“„ ç»Ÿè®¡ç»“æœ CSV æ–‡ä»¶å†…å®¹ï¼š\n",
      "      folder                                           path  pdb_file_count\n",
      "0  train_pos  /content/drive/MyDrive/AFP_work/pdb/train_pos            1140\n",
      "1  train_neg  /content/drive/MyDrive/AFP_work/pdb/train_neg            1200\n",
      "2   test_pos   /content/drive/MyDrive/AFP_work/pdb/test_pos             367\n",
      "3   test_neg   /content/drive/MyDrive/AFP_work/pdb/test_neg             308\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# å®šä¹‰ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "\n",
    "# å°†æ–‡ä»¶å¤¹è·¯å¾„å­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸ä¸­ï¼Œä¾¿äºéå†\n",
    "folder_paths = {\n",
    "    'train_pos': train_pos_folder_path,\n",
    "    'train_neg': train_neg_folder_path,\n",
    "    'test_pos': test_pos_folder_path,\n",
    "    'test_neg': test_neg_folder_path\n",
    "}\n",
    "\n",
    "# åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨ç»Ÿè®¡ç»“æœ\n",
    "stats = []\n",
    "\n",
    "# éå†æ¯ä¸ªæ–‡ä»¶å¤¹ï¼Œç»Ÿè®¡ä¸ªæ•°\n",
    "for folder_name, folder_path in folder_paths.items():\n",
    "    print(f\"æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹: {folder_path}\")\n",
    "\n",
    "    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"âŒ æ–‡ä»¶å¤¹ '{folder_path}' ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\\n\")\n",
    "        stats.append({\n",
    "            'folder': folder_name,\n",
    "            'path': folder_path,\n",
    "            'pdb_file_count': 'æ–‡ä»¶å¤¹ä¸å­˜åœ¨'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # ä½¿ç”¨ glob æŸ¥æ‰¾æ‰€æœ‰ .pdb æ–‡ä»¶ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰\n",
    "    pdb_files = glob.glob(os.path.join(folder_path, '*.pdb')) + glob.glob(os.path.join(folder_path, '*.PDB'))\n",
    "\n",
    "    # ç»Ÿè®¡ .pdb æ–‡ä»¶çš„æ•°é‡\n",
    "    pdb_count = len(pdb_files)\n",
    "\n",
    "    print(f\"âœ… æ–‡ä»¶å¤¹ '{folder_name}' ä¸­å…±æœ‰ {pdb_count} ä¸ª .pdb æ–‡ä»¶ã€‚\\n\")\n",
    "\n",
    "    # å°†ç»Ÿè®¡ç»“æœæ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "    stats.append({\n",
    "        'folder': folder_name,\n",
    "        'path': folder_path,\n",
    "        'pdb_file_count': pdb_count\n",
    "    })\n",
    "\n",
    "# åˆ›å»º DataFrame\n",
    "df_stats = pd.DataFrame(stats)\n",
    "\n",
    "# æ˜¾ç¤ºç»Ÿè®¡ç»“æœ\n",
    "print(\"ğŸ“Š å„æ–‡ä»¶å¤¹ä¸­ .pdb æ–‡ä»¶çš„æ•°é‡ç»Ÿè®¡ï¼š\")\n",
    "print(df_stats)\n",
    "\n",
    "# ä¿å­˜ç»Ÿè®¡ç»“æœä¸º CSV æ–‡ä»¶\n",
    "csv_output_path = '/content/drive/MyDrive/AFP_work/pdb/pdb_file_counts.csv'\n",
    "df_stats.to_csv(csv_output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nâœ… ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ° '{csv_output_path}'ã€‚\")\n",
    "\n",
    "# è¯»å–å¹¶æ˜¾ç¤ºä¿å­˜çš„ CSV æ–‡ä»¶å†…å®¹\n",
    "df_loaded_stats = pd.read_csv(csv_output_path)\n",
    "print(\"\\nğŸ“„ ç»Ÿè®¡ç»“æœ CSV æ–‡ä»¶å†…å®¹ï¼š\")\n",
    "print(df_loaded_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNGVugPJmcfN",
    "outputId": "16c0fc58-6fa9-4739-d59e-aa4deb9227b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰€æœ‰ .pdb æ–‡ä»¶åå·²æˆåŠŸä¿å­˜åˆ° '/content/drive/MyDrive/pdb/test_pos_pdb_filenames.csv'ã€‚\n",
      "æ€»å…±ä¿å­˜äº† 308 ä¸ªæ–‡ä»¶åã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# å®šä¹‰ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "\n",
    "# å®šä¹‰è¾“å‡º CSV æ–‡ä»¶çš„ä¿å­˜è·¯å¾„\n",
    "output_csv_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos_pdb_filenames.csv'\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨\n",
    "if not os.path.isdir(train_pos_folder_path):\n",
    "    print(f\"âŒ æ–‡ä»¶å¤¹ '{train_pos_folder_path}' ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\")\n",
    "else:\n",
    "    # ä½¿ç”¨ glob æŸ¥æ‰¾æ‰€æœ‰ .pdb æ–‡ä»¶ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰\n",
    "    pdb_files_lower = glob.glob(os.path.join(train_pos_folder_path, '*.pdb'))\n",
    "    pdb_files_upper = glob.glob(os.path.join(train_pos_folder_path, '*.PDB'))\n",
    "    pdb_files = pdb_files_lower + pdb_files_upper\n",
    "\n",
    "    # æå–æ–‡ä»¶å\n",
    "    pdb_filenames = [os.path.basename(f) for f in pdb_files]\n",
    "\n",
    "    # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ä»»ä½• .pdb æ–‡ä»¶\n",
    "    if not pdb_filenames:\n",
    "        print(f\"âŒ åœ¨æ–‡ä»¶å¤¹ '{train_pos_folder_path}' ä¸­æœªæ‰¾åˆ°ä»»ä½• .pdb æ–‡ä»¶ã€‚\")\n",
    "    else:\n",
    "        # åˆ›å»ºä¸€ä¸ª DataFrame\n",
    "        df = pd.DataFrame({'filename': pdb_filenames})\n",
    "\n",
    "        # ä¿å­˜ä¸º CSV æ–‡ä»¶\n",
    "        try:\n",
    "            df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"âœ… æ‰€æœ‰ .pdb æ–‡ä»¶åå·²æˆåŠŸä¿å­˜åˆ° '{output_csv_path}'ã€‚\")\n",
    "            print(f\"æ€»å…±ä¿å­˜äº† {len(pdb_filenames)} ä¸ªæ–‡ä»¶åã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä¿å­˜ CSV æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFQPsDFXnMF6",
    "outputId": "b30531eb-18e9-426f-97bd-4d92bd3b0ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æå–çš„æ•°å­—å·²æˆåŠŸä¿å­˜åˆ° '/content/drive/MyDrive/pdb/test_pos_pdb_filenames_extracted.csv'ã€‚\n",
      "æ€»å…±ä¿å­˜äº† 308 ä¸ªæ•°å­—ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def save_extracted_numbers_to_csv(folder_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    ä»æŒ‡å®šæ–‡ä»¶å¤¹ä¸­æå– .pdb æ–‡ä»¶åä¸­ '_relaxed_rank_001' ä¹‹å‰çš„æ•°å­—ï¼Œå¹¶ä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        folder_path (str): ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n",
    "        output_csv_path (str): è¾“å‡º CSV æ–‡ä»¶çš„è·¯å¾„ã€‚\n",
    "    \"\"\"\n",
    "    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"âŒ æ–‡ä»¶å¤¹ '{folder_path}' ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\")\n",
    "        return\n",
    "\n",
    "    # ä½¿ç”¨ glob æŸ¥æ‰¾æ‰€æœ‰ .pdb æ–‡ä»¶ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰\n",
    "    pdb_files_lower = glob.glob(os.path.join(folder_path, '*.pdb'))\n",
    "    pdb_files_upper = glob.glob(os.path.join(folder_path, '*.PDB'))\n",
    "    pdb_files = pdb_files_lower + pdb_files_upper\n",
    "\n",
    "    # æå–æ–‡ä»¶å\n",
    "    pdb_filenames = [os.path.basename(f) for f in pdb_files]\n",
    "\n",
    "    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼\n",
    "    # æ–‡ä»¶åæ ¼å¼ï¼šæ•°å­—_relaxed_rank_001_å…¶ä»–ä¿¡æ¯.pdbï¼Œä¾‹å¦‚ï¼š2033_relaxed_rank_001_alphafold2_ptm_model_4_seed_000.pdb\n",
    "    pattern = re.compile(r'^(\\d+)_relaxed_rank_001.*\\.pdb$', re.IGNORECASE)\n",
    "\n",
    "    # åˆå§‹åŒ–åˆ—è¡¨å­˜å‚¨æå–çš„æ•°å­—\n",
    "    extracted_numbers = []\n",
    "\n",
    "    # éå†æ–‡ä»¶åå¹¶æå–æ•°å­—\n",
    "    for filename in pdb_filenames:\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            extracted_numbers.append(int(number))  # è½¬æ¢ä¸ºæ•´æ•°ç±»å‹\n",
    "        else:\n",
    "            print(f\"âš ï¸ æ–‡ä»¶åä¸ç¬¦åˆé¢„æœŸæ¨¡å¼ï¼Œæ— æ³•æå–æ•°å­—: {filename}\")\n",
    "\n",
    "    # æ£€æŸ¥æ˜¯å¦æå–åˆ°äº†ä»»ä½•æ•°å­—\n",
    "    if not extracted_numbers:\n",
    "        print(f\"âŒ åœ¨æ–‡ä»¶å¤¹ '{folder_path}' ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ¨¡å¼çš„ .pdb æ–‡ä»¶ã€‚\")\n",
    "    else:\n",
    "        # åˆ›å»ºä¸€ä¸ª DataFrame\n",
    "        df = pd.DataFrame({'extracted_number': extracted_numbers})\n",
    "\n",
    "        # ä¿å­˜ä¸º CSV æ–‡ä»¶\n",
    "        try:\n",
    "            df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"âœ… æå–çš„æ•°å­—å·²æˆåŠŸä¿å­˜åˆ° '{output_csv_path}'ã€‚\")\n",
    "            print(f\"æ€»å…±ä¿å­˜äº† {len(extracted_numbers)} ä¸ªæ•°å­—ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä¿å­˜ CSV æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "# å®šä¹‰ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "\n",
    "# å®šä¹‰è¾“å‡º CSV æ–‡ä»¶çš„ä¿å­˜è·¯å¾„\n",
    "output_csv_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos_pdb_filenames_extracted.csv'\n",
    "\n",
    "# è°ƒç”¨å‡½æ•°\n",
    "save_extracted_numbers_to_csv(train_pos_folder_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "-zMy7WkUuhOK",
    "outputId": "373a9827-4b39-4f85-bbfd-6996a1312446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†æ–‡ä»¶å¤¹: /content/drive/MyDrive/pdb/train_pos\n",
      "æ–‡ä»¶æ•°é‡: 1200\n",
      "âœ… å·²å¤„ç† 100 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 200 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 300 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 400 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 500 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 600 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 700 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 800 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 900 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 1000 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 1100 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 1200 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å®Œæˆå¤„ç† 1200 ä¸ªæ–‡ä»¶ã€‚\n",
      "å¤„ç†æ–‡ä»¶å¤¹: /content/drive/MyDrive/pdb/train_neg\n",
      "æ–‡ä»¶æ•°é‡: 1200\n",
      "âœ… å·²å¤„ç† 100 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 200 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 300 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 400 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 500 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 600 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 700 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 800 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 900 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 1000 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 1100 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 1200 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å®Œæˆå¤„ç† 1200 ä¸ªæ–‡ä»¶ã€‚\n",
      "å¤„ç†æ–‡ä»¶å¤¹: /content/drive/MyDrive/pdb/test_pos\n",
      "æ–‡ä»¶æ•°é‡: 308\n",
      "âœ… å·²å¤„ç† 100 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 200 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 300 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å®Œæˆå¤„ç† 308 ä¸ªæ–‡ä»¶ã€‚\n",
      "å¤„ç†æ–‡ä»¶å¤¹: /content/drive/MyDrive/pdb/test_neg\n",
      "æ–‡ä»¶æ•°é‡: 308\n",
      "âœ… å·²å¤„ç† 100 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 200 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å·²å¤„ç† 300 ä¸ªæ–‡ä»¶ã€‚\n",
      "âœ… å®Œæˆå¤„ç† 308 ä¸ªæ–‡ä»¶ã€‚\n",
      "æ€»å¤„ç†æ—¶é—´: 947.43 ç§’\n",
      "\n",
      "ä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡:\n",
      "æ–‡ä»¶å¤¹ \"/content/drive/MyDrive/pdb_features/train_pos\" ä¸­ä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡: 1200\n",
      "æ–‡ä»¶å¤¹ \"/content/drive/MyDrive/pdb_features/train_neg\" ä¸­ä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡: 1200\n",
      "æ–‡ä»¶å¤¹ \"/content/drive/MyDrive/pdb_features/test_pos\" ä¸­ä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡: 308\n",
      "æ–‡ä»¶å¤¹ \"/content/drive/MyDrive/pdb_features/test_neg\" ä¸­ä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡: 308\n",
      "\n",
      "æ£€æŸ¥éƒ¨åˆ† JSON æ–‡ä»¶çš„ç»´åº¦ä¿¡æ¯:\n",
      "æ–‡ä»¶: 402_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 13, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 43, æ–¹å‘ç‰¹å¾æ•°é‡: 43, æ—‹è½¬ç‰¹å¾æ•°é‡: 43\n",
      "\n",
      "æ–‡ä»¶: 489_relaxed_rank_001_alphafold2_ptm_model_2_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 13, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 33, æ–¹å‘ç‰¹å¾æ•°é‡: 33, æ—‹è½¬ç‰¹å¾æ•°é‡: 33\n",
      "\n",
      "æ–‡ä»¶: 524_relaxed_rank_001_alphafold2_ptm_model_5_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 13, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 50, æ–¹å‘ç‰¹å¾æ•°é‡: 50, æ—‹è½¬ç‰¹å¾æ•°é‡: 50\n",
      "\n",
      "æ–‡ä»¶: 596_relaxed_rank_001_alphafold2_ptm_model_1_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 13, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 50, æ–¹å‘ç‰¹å¾æ•°é‡: 50, æ—‹è½¬ç‰¹å¾æ•°é‡: 50\n",
      "\n",
      "æ–‡ä»¶: 703_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 13, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 51, æ–¹å‘ç‰¹å¾æ•°é‡: 51, æ—‹è½¬ç‰¹å¾æ•°é‡: 51\n",
      "\n",
      "æ–‡ä»¶: 414_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 78, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 392, æ–¹å‘ç‰¹å¾æ•°é‡: 392, æ—‹è½¬ç‰¹å¾æ•°é‡: 392\n",
      "\n",
      "æ–‡ä»¶: 509_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 78, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 615, æ–¹å‘ç‰¹å¾æ•°é‡: 615, æ—‹è½¬ç‰¹å¾æ•°é‡: 615\n",
      "\n",
      "æ–‡ä»¶: 578_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 78, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 474, æ–¹å‘ç‰¹å¾æ•°é‡: 474, æ—‹è½¬ç‰¹å¾æ•°é‡: 474\n",
      "\n",
      "æ–‡ä»¶: 1089_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 78, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 485, æ–¹å‘ç‰¹å¾æ•°é‡: 485, æ—‹è½¬ç‰¹å¾æ•°é‡: 485\n",
      "\n",
      "æ–‡ä»¶: 1116_relaxed_rank_001_alphafold2_ptm_model_1_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 78, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 511, æ–¹å‘ç‰¹å¾æ•°é‡: 511, æ—‹è½¬ç‰¹å¾æ•°é‡: 511\n",
      "\n",
      "æ–‡ä»¶: 398_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 11, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 42, æ–¹å‘ç‰¹å¾æ•°é‡: 42, æ—‹è½¬ç‰¹å¾æ•°é‡: 42\n",
      "\n",
      "æ–‡ä»¶: 297_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 12, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 45, æ–¹å‘ç‰¹å¾æ•°é‡: 45, æ—‹è½¬ç‰¹å¾æ•°é‡: 45\n",
      "\n",
      "æ–‡ä»¶: 402_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 12, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 33, æ–¹å‘ç‰¹å¾æ•°é‡: 33, æ—‹è½¬ç‰¹å¾æ•°é‡: 33\n",
      "\n",
      "æ–‡ä»¶: 436_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 12, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 44, æ–¹å‘ç‰¹å¾æ•°é‡: 44, æ—‹è½¬ç‰¹å¾æ•°é‡: 44\n",
      "\n",
      "æ–‡ä»¶: 18_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 13, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 52, æ–¹å‘ç‰¹å¾æ•°é‡: 52, æ—‹è½¬ç‰¹å¾æ•°é‡: 52\n",
      "\n",
      "æ–‡ä»¶: 2_relaxed_rank_001_alphafold2_ptm_model_5_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 35, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 215, æ–¹å‘ç‰¹å¾æ•°é‡: 215, æ—‹è½¬ç‰¹å¾æ•°é‡: 215\n",
      "\n",
      "æ–‡ä»¶: 65_relaxed_rank_001_alphafold2_ptm_model_2_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 11, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 41, æ–¹å‘ç‰¹å¾æ•°é‡: 41, æ—‹è½¬ç‰¹å¾æ•°é‡: 41\n",
      "\n",
      "æ–‡ä»¶: 125_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 12, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 45, æ–¹å‘ç‰¹å¾æ•°é‡: 45, æ—‹è½¬ç‰¹å¾æ•°é‡: 45\n",
      "\n",
      "æ–‡ä»¶: 241_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 11, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 40, æ–¹å‘ç‰¹å¾æ•°é‡: 40, æ—‹è½¬ç‰¹å¾æ•°é‡: 40\n",
      "\n",
      "æ–‡ä»¶: 546_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "èŠ‚ç‚¹ç‰¹å¾æ•°é‡: 12, ä½ç½®ç‰¹å¾ç»´åº¦: 3\n",
      "è¾¹ç‰¹å¾æ•°é‡: 45, æ–¹å‘ç‰¹å¾æ•°é‡: 45, æ—‹è½¬ç‰¹å¾æ•°é‡: 45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "\n",
    "# è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ PDB æ–‡ä»¶å¤¹è·¯å¾„\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "\n",
    "# è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "os.makedirs(output_train_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_train_neg_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_neg_folder_path, exist_ok=True)\n",
    "\n",
    "# æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ‰€æœ‰ PDB æ–‡ä»¶\n",
    "train_pos_pdb_files = [f for f in os.listdir(train_pos_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "train_neg_pdb_files = [f for f in os.listdir(train_neg_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "test_pos_pdb_files = [f for f in os.listdir(test_pos_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "test_neg_pdb_files = [f for f in os.listdir(test_neg_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "\n",
    "# åˆå§‹åŒ– PDBParser\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ä¼˜åŒ– PDB æ–‡ä»¶å¤„ç†å‡½æ•°\n",
    "def process_pdb_file(pdb_path):\n",
    "    try:\n",
    "        structure = parser.get_structure('', pdb_path)\n",
    "        residues = [residue for residue in structure.get_residues() if 'CA' in residue]\n",
    "        num_residues = len(residues)\n",
    "\n",
    "        if num_residues == 0:\n",
    "            print(f\"âš ï¸ æ–‡ä»¶ '{pdb_path}' ä¸­æ²¡æœ‰æ‰¾åˆ° CA åŸå­ã€‚\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        # æå–ä½ç½®ç‰¹å¾ã€æ–¹å‘ç‰¹å¾å’Œæ—‹è½¬ç‰¹å¾\n",
    "        positions = np.array([residue['CA'].get_coord() for residue in residues], dtype=np.float64)\n",
    "        edges = []\n",
    "        directions = []\n",
    "        rotations = []\n",
    "\n",
    "        # è®¡ç®—æ¥è§¦å›¾å’Œé™„åŠ ç‰¹å¾\n",
    "        for i in range(num_residues):\n",
    "            for j in range(i + 1, num_residues):\n",
    "                distance = np.linalg.norm(positions[i] - positions[j])\n",
    "                if distance < 10.0:  # é˜ˆå€¼ä¸º10Ã…æ¥å®šä¹‰æ¥è§¦\n",
    "                    edges.append([i, j])\n",
    "                    direction = positions[j] - positions[i]\n",
    "                    norm = np.linalg.norm(direction)\n",
    "                    if norm != 0:\n",
    "                        directions.append(direction / norm)\n",
    "                        rotations.append(float(np.arctan2(direction[1], direction[0])))\n",
    "\n",
    "        return positions, edges, directions, rotations\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¤„ç†æ–‡ä»¶ '{pdb_path}' æ—¶å‡ºé”™: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# å¤„ç† PDB æ–‡ä»¶å¹¶ä¿å­˜ç‰¹å¾\n",
    "def process_and_save(pdb_files, folder_path, output_folder_path, label):\n",
    "    print(f'å¤„ç†æ–‡ä»¶å¤¹: {folder_path}')\n",
    "    print(f'æ–‡ä»¶æ•°é‡: {len(pdb_files)}')\n",
    "\n",
    "    processed_count = 0\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_path = os.path.join(folder_path, pdb_file)\n",
    "        positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "\n",
    "        if positions is None:\n",
    "            continue  # è·³è¿‡å¤„ç†å‡ºé”™çš„æ–‡ä»¶\n",
    "\n",
    "        features = {\n",
    "            \"node_features\": positions.tolist(),\n",
    "            \"edge_features\": {\n",
    "                \"edges\": edges,\n",
    "                \"directions\": [d.tolist() for d in directions],\n",
    "                \"rotations\": [float(rot) for rot in rotations]\n",
    "            },\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "        # å®šä¹‰è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "        output_file_path = os.path.join(output_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "\n",
    "        # ä¿å­˜ç‰¹å¾åˆ°æ–‡ä»¶\n",
    "        try:\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                json.dump(features, output_file)\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä¿å­˜æ–‡ä»¶ '{output_file_path}' æ—¶å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "        # æ¯å¤„ç†100ä¸ªæ–‡ä»¶ï¼Œæ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    "        if processed_count % 100 == 0:\n",
    "            print(f'âœ… å·²å¤„ç† {processed_count} ä¸ªæ–‡ä»¶ã€‚')\n",
    "\n",
    "    print(f'âœ… å®Œæˆå¤„ç† {processed_count} ä¸ªæ–‡ä»¶ã€‚')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# å¤„ç†è®­ç»ƒé›†ä¸­çš„é˜³æ€§ PDB æ–‡ä»¶\n",
    "process_and_save(train_pos_pdb_files, train_pos_folder_path, output_train_pos_folder_path, label=1)\n",
    "# å¤„ç†è®­ç»ƒé›†ä¸­çš„é˜´æ€§ PDB æ–‡ä»¶\n",
    "process_and_save(train_neg_pdb_files, train_neg_folder_path, output_train_neg_folder_path, label=0)\n",
    "# å¤„ç†æµ‹è¯•é›†ä¸­çš„é˜³æ€§ PDB æ–‡ä»¶\n",
    "process_and_save(test_pos_pdb_files, test_pos_folder_path, output_test_pos_folder_path, label=1)\n",
    "# å¤„ç†æµ‹è¯•é›†ä¸­çš„é˜´æ€§ PDB æ–‡ä»¶\n",
    "process_and_save(test_neg_pdb_files, test_neg_folder_path, output_test_neg_folder_path, label=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"æ€»å¤„ç†æ—¶é—´: {end_time - start_time:.2f} ç§’\")\n",
    "\n",
    "\n",
    "\n",
    "# è¾“å‡ºä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡\n",
    "def count_json_files(output_folder_path):\n",
    "    json_files = [f for f in os.listdir(output_folder_path) if f.endswith('.json')]\n",
    "    print(f'æ–‡ä»¶å¤¹ \"{output_folder_path}\" ä¸­ä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡: {len(json_files)}')\n",
    "    return json_files\n",
    "\n",
    "print(\"\\nä¿å­˜çš„ JSON æ–‡ä»¶æ•°é‡:\")\n",
    "count_json_files(output_train_pos_folder_path)\n",
    "count_json_files(output_train_neg_folder_path)\n",
    "count_json_files(output_test_pos_folder_path)\n",
    "count_json_files(output_test_neg_folder_path)\n",
    "\n",
    "# æ£€æŸ¥æ¯ä¸ªä¿å­˜çš„ JSON æ–‡ä»¶ä¸­çš„ç»´åº¦ä¿¡æ¯\n",
    "def check_json_dimensions(output_folder_path):\n",
    "    json_files = [f for f in os.listdir(output_folder_path) if f.endswith('.json')]\n",
    "    for json_file in json_files[:5]:  # ä»…æ£€æŸ¥å‰5ä¸ªæ–‡ä»¶\n",
    "        json_path = os.path.join(output_folder_path, json_file)\n",
    "        with open(json_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            node_features = data.get(\"node_features\", [])\n",
    "            edge_features = data.get(\"edge_features\", {})\n",
    "            edges = edge_features.get(\"edges\", [])\n",
    "            directions = edge_features.get(\"directions\", [])\n",
    "            rotations = edge_features.get(\"rotations\", [])\n",
    "            print(f'æ–‡ä»¶: {json_file}')\n",
    "            print(f'èŠ‚ç‚¹ç‰¹å¾æ•°é‡: {len(node_features)}, ä½ç½®ç‰¹å¾ç»´åº¦: {len(node_features[0]) if node_features else 0}')\n",
    "            print(f'è¾¹ç‰¹å¾æ•°é‡: {len(edges)}, æ–¹å‘ç‰¹å¾æ•°é‡: {len(directions)}, æ—‹è½¬ç‰¹å¾æ•°é‡: {len(rotations)}\\n')\n",
    "\n",
    "print(\"\\næ£€æŸ¥éƒ¨åˆ† JSON æ–‡ä»¶çš„ç»´åº¦ä¿¡æ¯:\")\n",
    "check_json_dimensions(output_train_pos_folder_path)\n",
    "check_json_dimensions(output_train_neg_folder_path)\n",
    "check_json_dimensions(output_test_pos_folder_path)\n",
    "check_json_dimensions(output_test_neg_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maxpswcORjOn",
    "outputId": "411069fe-9c71-4fbf-857c-ad3961235a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶å¤¹ 'train_pos' ä¸­çš„ JSON æ–‡ä»¶æ•°é‡: 1200\n",
      "æ–‡ä»¶å¤¹ 'train_neg' ä¸­çš„ JSON æ–‡ä»¶æ•°é‡: 1200\n",
      "æ–‡ä»¶å¤¹ 'test_pos' ä¸­çš„ JSON æ–‡ä»¶æ•°é‡: 308\n",
      "æ–‡ä»¶å¤¹ 'test_neg' ä¸­çš„ JSON æ–‡ä»¶æ•°é‡: 308\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "# å°†æ–‡ä»¶å¤¹è·¯å¾„å­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸ä¸­ï¼Œä¾¿äºéå†\n",
    "folders = {\n",
    "    'train_pos': output_train_pos_folder_path,\n",
    "    'train_neg': output_train_neg_folder_path,\n",
    "    'test_pos': output_test_pos_folder_path,\n",
    "    'test_neg': output_test_neg_folder_path\n",
    "}\n",
    "\n",
    "# éå†æ¯ä¸ªæ–‡ä»¶å¤¹å¹¶ç»Ÿè®¡ JSON æ–‡ä»¶æ•°é‡\n",
    "for folder_name, folder_path in folders.items():\n",
    "    if os.path.isdir(folder_path):\n",
    "        # ä½¿ç”¨ glob æŸ¥æ‰¾æ‰€æœ‰ .json æ–‡ä»¶ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰\n",
    "        json_files = glob.glob(os.path.join(folder_path, '*.json')) + glob.glob(os.path.join(folder_path, '*.JSON'))\n",
    "        count = len(json_files)\n",
    "        print(f\"æ–‡ä»¶å¤¹ '{folder_name}' ä¸­çš„ JSON æ–‡ä»¶æ•°é‡: {count}\")\n",
    "    else:\n",
    "        print(f\"âŒ æ–‡ä»¶å¤¹ '{folder_name}' ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2hWiwqzc-V-1",
    "outputId": "e321fe47-78cd-44db-b20f-2ea35524cde6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# é…ç½®æ—¥å¿—\n",
    "logging.basicConfig(filename='/content/drive/MyDrive/AFP_work/pdb_features/processing.log',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "output_folders = {\n",
    "    'train_pos': output_train_pos_folder_path,\n",
    "    'train_neg': output_train_neg_folder_path,\n",
    "    'test_pos': output_test_pos_folder_path,\n",
    "    'test_neg': output_test_neg_folder_path\n",
    "}\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# å®šä¹‰å‡½æ•°åŠ è½½å•ä¸ª JSON æ–‡ä»¶\n",
    "def load_single_json(json_file):\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            sample = json.load(f)\n",
    "        logging.info(f\"æˆåŠŸåŠ è½½æ–‡ä»¶: {json_file}\")\n",
    "        return sample\n",
    "    except Exception as e:\n",
    "        logging.error(f\"åŠ è½½æ–‡ä»¶ '{json_file}' æ—¶å‡ºé”™: {e}\")\n",
    "        return None\n",
    "\n",
    "# å®šä¹‰å‡½æ•°å¹¶è¡ŒåŠ è½½ JSON æ–‡ä»¶\n",
    "def load_json_files_parallel(folder_path, max_workers=8):\n",
    "    \"\"\"\n",
    "    å¹¶è¡ŒåŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ JSON æ–‡ä»¶ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        folder_path (str): JSON æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n",
    "        max_workers (int): å¹¶è¡Œå·¥ä½œçš„æœ€å¤§çº¿ç¨‹æ•°ã€‚\n",
    "\n",
    "    è¿”å›:\n",
    "        list: åŒ…å«æ‰€æœ‰æˆåŠŸåŠ è½½çš„æ ·æœ¬æ•°æ®çš„åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    json_files = glob.glob(os.path.join(folder_path, '*.json'))\n",
    "    data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(load_single_json, f): f for f in json_files}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=f'Loading {os.path.basename(folder_path)}'):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                data.append(result)\n",
    "    return data\n",
    "\n",
    "# åˆå§‹åŒ–è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# åŠ è½½è®­ç»ƒé›†æ•°æ®\n",
    "train_pos_data = load_json_files_parallel(output_folders['train_pos'])\n",
    "train_neg_data = load_json_files_parallel(output_folders['train_neg'])\n",
    "train_data = train_pos_data + train_neg_data\n",
    "\n",
    "# åŠ è½½æµ‹è¯•é›†æ•°æ®\n",
    "test_pos_data = load_json_files_parallel(output_folders['test_pos'])\n",
    "test_neg_data = load_json_files_parallel(output_folders['test_neg'])\n",
    "test_data = test_pos_data + test_neg_data\n",
    "\n",
    "print(f\"âœ… è®­ç»ƒé›†æ€»æ ·æœ¬æ•°: {len(train_data)}\")\n",
    "print(f\"âœ… æµ‹è¯•é›†æ€»æ ·æœ¬æ•°: {len(test_data)}\")\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºæ±‡æ€»æ–‡ä»¶çš„è·¯å¾„\n",
    "aggregated_output_folder = '/content/drive/MyDrive/AFP_work/pdb_features/aggregated'\n",
    "os.makedirs(aggregated_output_folder, exist_ok=True)\n",
    "\n",
    "train_output_path = os.path.join(aggregated_output_folder, 'train_dataset.json')\n",
    "test_output_path = os.path.join(aggregated_output_folder, 'test_dataset.json')\n",
    "\n",
    "# ä¿å­˜è®­ç»ƒé›†\n",
    "try:\n",
    "    with open(train_output_path, 'w') as f:\n",
    "        json.dump(train_data, f)\n",
    "    print(f\"âœ… è®­ç»ƒé›†å·²ä¿å­˜åˆ° '{train_output_path}'ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¿å­˜è®­ç»ƒé›†æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "# ä¿å­˜æµ‹è¯•é›†\n",
    "try:\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_data, f)\n",
    "    print(f\"âœ… æµ‹è¯•é›†å·²ä¿å­˜åˆ° '{test_output_path}'ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¿å­˜æµ‹è¯•é›†æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "# éªŒè¯æ±‡æ€»ç»“æœ\n",
    "def load_aggregated_data(file_path):\n",
    "    \"\"\"\n",
    "    ä»æŒ‡å®šçš„ JSON æ–‡ä»¶ä¸­åŠ è½½æ±‡æ€»æ•°æ®ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        file_path (str): æ±‡æ€»æ•°æ®çš„ JSON æ–‡ä»¶è·¯å¾„ã€‚\n",
    "\n",
    "    è¿”å›:\n",
    "        list: åŒ…å«æ‰€æœ‰æ ·æœ¬æ•°æ®çš„åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"âŒ æ–‡ä»¶ '{file_path}' ä¸å­˜åœ¨ã€‚\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"âœ… æˆåŠŸåŠ è½½ '{file_path}'ï¼Œæ ·æœ¬æ•°: {len(data)}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½æ–‡ä»¶ '{file_path}' æ—¶å‡ºé”™: {e}\")\n",
    "        return []\n",
    "\n",
    "# åŠ è½½å¹¶æŸ¥çœ‹è®­ç»ƒé›†\n",
    "train_dataset = load_aggregated_data(train_output_path)\n",
    "if train_dataset:\n",
    "    print(f\"è®­ç»ƒé›†ç¬¬ä¸€ä¸ªæ ·æœ¬å†…å®¹:\")\n",
    "    print(json.dumps(train_dataset[0], indent=2))\n",
    "\n",
    "# åŠ è½½å¹¶æŸ¥çœ‹æµ‹è¯•é›†\n",
    "test_dataset = load_aggregated_data(test_output_path)\n",
    "if test_dataset:\n",
    "    print(f\"æµ‹è¯•é›†ç¬¬ä¸€ä¸ªæ ·æœ¬å†…å®¹:\")\n",
    "    print(json.dumps(test_dataset[0], indent=2))\n",
    "\n",
    "# è½¬æ¢ä¸º Pandas DataFrameï¼ˆå¯é€‰ï¼‰\n",
    "def json_to_dataframe(data):\n",
    "    \"\"\"\n",
    "    å°† JSON æ•°æ®è½¬æ¢ä¸º Pandas DataFrameã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        data (list): åŒ…å«æ‰€æœ‰æ ·æœ¬æ•°æ®çš„åˆ—è¡¨ã€‚\n",
    "\n",
    "    è¿”å›:\n",
    "        pd.DataFrame: åŒ…å«æ ‡ç­¾å’Œç‰¹å¾çš„ DataFrameã€‚\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for sample in data:\n",
    "        record = {}\n",
    "        record['label'] = sample['label']\n",
    "        # ç¤ºä¾‹ï¼šè®¡ç®—èŠ‚ç‚¹ç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®ä½œä¸ºç®€å•ç‰¹å¾\n",
    "        node_features = np.array(sample['node_features'])\n",
    "        record['node_mean_x'] = node_features[:, 0].mean()\n",
    "        record['node_mean_y'] = node_features[:, 1].mean()\n",
    "        record['node_mean_z'] = node_features[:, 2].mean()\n",
    "        record['node_std_x'] = node_features[:, 0].std()\n",
    "        record['node_std_y'] = node_features[:, 1].std()\n",
    "        record['node_std_z'] = node_features[:, 2].std()\n",
    "        # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šç‰¹å¾\n",
    "        records.append(record)\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "# è½¬æ¢è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸º DataFrame\n",
    "train_df = json_to_dataframe(train_dataset)\n",
    "test_df = json_to_dataframe(test_dataset)\n",
    "\n",
    "print(\"è®­ç»ƒé›† DataFrame é¢„è§ˆ:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\næµ‹è¯•é›† DataFrame é¢„è§ˆ:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# ä¿å­˜ä¸º CSV æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰\n",
    "train_csv_path = os.path.join(aggregated_output_folder, 'train_dataset_dataframe.csv')\n",
    "test_csv_path = os.path.join(aggregated_output_folder, 'test_dataset_dataframe.csv')\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False, encoding='utf-8-sig')\n",
    "test_df.to_csv(test_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"âœ… è®­ç»ƒé›† DataFrame å·²ä¿å­˜åˆ° '{train_csv_path}'ã€‚\")\n",
    "print(f\"âœ… æµ‹è¯•é›† DataFrame å·²ä¿å­˜åˆ° '{test_csv_path}'ã€‚\")\n",
    "\n",
    "# å®šä¹‰ PyTorch Geometric æ•°æ®é›†ç±»ï¼ˆå¯é€‰ï¼‰\n",
    "class PDBDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(PDBDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        sample = self.data_list[idx]\n",
    "        node_features = torch.tensor(sample['node_features'], dtype=torch.float)\n",
    "        edge_index = torch.tensor(sample['edge_features']['edges'], dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(sample['edge_features']['directions'], dtype=torch.float)\n",
    "        rotations = torch.tensor(sample['edge_features']['rotations'], dtype=torch.float).unsqueeze(1)\n",
    "        edge_features = torch.cat([edge_attr, rotations], dim=1)  # åˆå¹¶æ–¹å‘å’Œæ—‹è½¬ç‰¹å¾\n",
    "        label = torch.tensor(sample['label'], dtype=torch.long)\n",
    "\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features, y=label)\n",
    "        return data\n",
    "\n",
    "# åˆ›å»º PyTorch Geometric æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰\n",
    "train_pyg_dataset = PDBDataset(train_dataset)\n",
    "test_pyg_dataset = PDBDataset(test_dataset)\n",
    "\n",
    "# åˆ›å»º DataLoaderï¼ˆå¯é€‰ï¼‰\n",
    "train_loader = DataLoader(train_pyg_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_pyg_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"âœ… PyTorch Geometric è®­ç»ƒé›†æ•°æ®é‡: {len(train_pyg_dataset)}\")\n",
    "print(f\"âœ… PyTorch Geometric æµ‹è¯•é›†æ•°æ®é‡: {len(test_pyg_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8orO_EtEEoLh"
   },
   "source": [
    "# ç»“åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 195262,
     "status": "ok",
     "timestamp": 1759391305851,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "INRwfY49Ep95",
    "outputId": "36a55a84-f21f-4d78-d68d-7afdb3840041"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "\n",
    "from torch_geometric.nn import (\n",
    "    GATConv, SAGEConv, GINConv, Set2Set, global_mean_pool\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, precision_recall_fscore_support,\n",
    "    matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import copy\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "esmc_folders = {\n",
    "    'train_pos': '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    'train_neg': '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    'test_pos': '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',\n",
    "    'test_neg': '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "}\n",
    "\n",
    "struct_folders = {\n",
    "    'train': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/train_dataset.json',\n",
    "    'test': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/test_dataset.json'\n",
    "}\n",
    "\n",
    "aggregated_output_folder = '/content/drive/MyDrive/AFP_work/esmc_struct_aggregated'\n",
    "os.makedirs(aggregated_output_folder, exist_ok=True)\n",
    "\n",
    "#==============================æ•°æ®åŠ è½½å’Œé¢„å¤„ç†==============================\n",
    "#***************************åŠ è½½ ESM-C ç‰¹å¾***************************\n",
    "def load_esmc_features(esmc_folder):\n",
    "    logits_path = os.path.join(esmc_folder, 'combined_logits.npy')\n",
    "    embeddings_path = os.path.join(esmc_folder, 'combined_embeddings.npy')\n",
    "    logits = np.load(logits_path, allow_pickle=True)\n",
    "    embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # è°ƒè¯•ï¼šæ£€æŸ¥ logits çš„ç»“æ„\n",
    "    print(f\"Logits[0] ç±»å‹: {type(logits[0])}, å€¼: {logits[0]}\")  #  ç±»å‹ <class 'numpy.ndarray'>\n",
    "    # æ‰“å°æ¯ä¸ªæ ·æœ¬çš„ logits å’Œ embeddings\n",
    "    print(\"Logits sample:\", logits[0])  # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ logits\n",
    "    print(\"Embeddings sample:\", embeddings[0])  # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ embeddings\n",
    "\n",
    "    # ä¿å­˜ç‰¹å¾\n",
    "    # if save_path:\n",
    "    #     np.savetxt(os.path.join(save_path, 'logits.csv'), logits, delimiter=\",\")\n",
    "    #     np.savetxt(os.path.join(save_path, 'embeddings.csv'), embeddings, delimiter=\",\")\n",
    "\n",
    "    # ä» ForwardTrackData ä¸­æå– sequence å¼ é‡å¹¶æ± åŒ–\n",
    "    logits_values = []\n",
    "    for l in logits:\n",
    "        # å‡è®¾ l æ˜¯ä¸€ä¸ªåŒ…å« ForwardTrackData çš„æ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ \n",
    "        forward_data = l[0] if isinstance(l, np.ndarray) else l\n",
    "        sequence_tensor = forward_data.sequence  # è·å–å¼ é‡\n",
    "        # å°†å¼ é‡ç§»åˆ° CPU å¹¶è½¬æ¢ä¸º float32\n",
    "        sequence_tensor = sequence_tensor.to(device='cpu', dtype=torch.float32)\n",
    "        # å¯¹æ‰€æœ‰ç»´åº¦å–å‡å€¼ï¼Œç¡®ä¿æ ‡é‡\n",
    "        pooled_value = sequence_tensor.mean(dim=[0, 1, 2]).item()  # æ± åŒ–ä¸ºæ ‡é‡\n",
    "        logits_values.append(pooled_value)\n",
    "\n",
    "    logits_values = np.array(logits_values, dtype=np.float32).reshape(-1, 1)\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    label = 1 if 'pos' in esmc_folder else 0\n",
    "    labels = np.full((logits_values.shape[0],), label)\n",
    "    return logits_values, embeddings, labels\n",
    "# åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ESM-C ç‰¹å¾\n",
    "train_pos_logits, train_pos_embeddings, train_pos_labels = load_esmc_features(esmc_folders['train_pos'])\n",
    "train_neg_logits, train_neg_embeddings, train_neg_labels = load_esmc_features(esmc_folders['train_neg'])\n",
    "test_pos_logits, test_pos_embeddings, test_pos_labels = load_esmc_features(esmc_folders['test_pos'])\n",
    "test_neg_logits, test_neg_embeddings, test_neg_labels = load_esmc_features(esmc_folders['test_neg'])\n",
    "# åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾\n",
    "train_logits = np.vstack((train_pos_logits, train_neg_logits))\n",
    "train_embeddings = np.vstack((train_pos_embeddings, train_neg_embeddings))\n",
    "train_labels = np.hstack((train_pos_labels, train_neg_labels))\n",
    "\n",
    "test_logits = np.vstack((test_pos_logits, test_neg_logits))\n",
    "test_embeddings = np.vstack((test_pos_embeddings, test_neg_embeddings))\n",
    "test_labels = np.hstack((test_pos_labels, test_neg_labels))\n",
    "\n",
    "\n",
    "#***************************2ã€åŠ è½½ç»“æ„ç‰¹å¾***************************\n",
    "def load_struct_features(json_path, sample_limit=5):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    data_list = []\n",
    "    for idx, sample in enumerate(tqdm(json_data, desc=f'åŠ è½½ç»“æ„ç‰¹å¾ from {json_path}')):\n",
    "        required_keys = ['node_features', 'edge_features', 'label']\n",
    "        if not all(key in sample for key in required_keys):\n",
    "            print(f\"[ERROR] æ ·æœ¬ç¼ºå°‘å¿…è¦çš„é”®: {sample}\")\n",
    "            continue\n",
    "        node_features = sample['node_features']\n",
    "        edge_features = sample['edge_features']\n",
    "        label = sample['label']\n",
    "        edges = edge_features.get('edges', [])\n",
    "        directions = edge_features.get('directions', [])\n",
    "        rotations = edge_features.get('rotations', [])\n",
    "        num_edges = len(edges)\n",
    "        if not (len(directions) == num_edges and len(rotations) == num_edges):\n",
    "            print(f\"[ERROR] è¾¹çš„æ•°é‡ä¸æ–¹å‘æˆ–æ—‹è½¬æ•°é‡ä¸åŒ¹é…: {sample}\")\n",
    "            continue\n",
    "        if num_edges > 0:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            directions = torch.tensor(directions, dtype=torch.float)\n",
    "            rotations = torch.tensor(rotations, dtype=torch.float).unsqueeze(1)\n",
    "            edge_attr = torch.cat([directions, rotations], dim=1)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.empty((0, 4), dtype=torch.float)\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=label)\n",
    "        data_list.append(data)\n",
    "        if idx < sample_limit:\n",
    "            num_nodes = node_features.shape[0]\n",
    "            node_feature_dim = node_features.shape[1]\n",
    "            print(f\"æ ·æœ¬ {idx+1}: èŠ‚ç‚¹æ•°é‡: {num_nodes}, èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {node_feature_dim}, è¾¹æ•°é‡: {num_edges}\")\n",
    "            if num_edges > 0:\n",
    "                print(f\"  è¾¹ç‰¹å¾ç»´åº¦: {edge_attr.shape[1]}\")\n",
    "            print(\"-\" * 50)\n",
    "    unique_node_feature_dims = set([data.x.shape[1] for data in data_list])\n",
    "    unique_edge_feature_dims = set([data.edge_attr.shape[1] for data in data_list if data.edge_attr.shape[0] > 0])\n",
    "    print(f\"\\næ‰€æœ‰æ ·æœ¬ä¸­å”¯ä¸€çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {unique_node_feature_dims}\")  # 3\n",
    "    print(f\"æ‰€æœ‰æ ·æœ¬ä¸­å”¯ä¸€çš„è¾¹ç‰¹å¾ç»´åº¦: {unique_edge_feature_dims}\")  # 4\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = load_struct_features(struct_folders['train'])\n",
    "test_struct_data = load_struct_features(struct_folders['test'])\n",
    "\n",
    "for i in range(min(3, len(train_struct_data))):\n",
    "    data = train_struct_data[i]\n",
    "    print(f\"æ ·æœ¬ {i+1} - èŠ‚ç‚¹ç‰¹å¾: {data.x.shape}, è¾¹ç‰¹å¾: {data.edge_attr.shape}\")\n",
    "\n",
    "##*************************** ç‰¹å¾æ ‡å‡†åŒ– ***************************\n",
    "def normalize_features(train_data_list, test_data_list=None):\n",
    "    node_scaler = StandardScaler()\n",
    "    edge_scaler = StandardScaler()\n",
    "    all_node_features = np.concatenate([data.x.numpy() for data in train_data_list], axis=0)\n",
    "    all_edge_features = np.concatenate([data.edge_attr.numpy() for data in train_data_list if data.edge_attr.shape[0] > 0], axis=0)\n",
    "    node_scaler.fit(all_node_features)\n",
    "    if all_edge_features.size > 0:\n",
    "        edge_scaler.fit(all_edge_features)\n",
    "    for data in train_data_list:\n",
    "        data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "        if data.edge_attr.shape[0] > 0:\n",
    "            data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    if test_data_list:\n",
    "        for data in test_data_list:\n",
    "            data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "            if data.edge_attr.shape[0] > 0:\n",
    "                data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    return train_data_list, test_data_list\n",
    "\n",
    "train_struct_data, test_struct_data = normalize_features(train_struct_data, test_struct_data)\n",
    "\n",
    "# #*************************** æ•´åˆ ESM-C ç‰¹å¾ ***************************\n",
    "def integrate_features(data_list, embeddings, logits):\n",
    "    if len(data_list) != len(embeddings) or len(data_list) != len(logits):\n",
    "        raise ValueError(f\"data_list, embeddings å’Œ logits é•¿åº¦ä¸åŒ¹é…: {len(data_list)} vs {len(embeddings)} vs {len(logits)}\")\n",
    "    for i, data in enumerate(tqdm(data_list, desc='æ•´åˆ ESM-C embeddings å’Œ logits')):\n",
    "        embedding = torch.tensor(embeddings[i], dtype=torch.float)  # [1152]\n",
    "        logit = torch.tensor(logits[i], dtype=torch.float).squeeze()  # [1] -> æ ‡é‡\n",
    "        combined_feature = torch.cat([embedding, logit.unsqueeze(0)], dim=0)  # [1152 + 1 = 1153]\n",
    "        num_nodes = data.x.shape[0]\n",
    "        combined_expanded = combined_feature.unsqueeze(0).repeat(num_nodes, 1)  # [num_nodes, 1153]\n",
    "        data.x = torch.cat([data.x, combined_expanded], dim=1)  # [num_nodes, 3 + 1153 = 1156]\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = integrate_features(train_struct_data, train_embeddings, train_logits)\n",
    "test_struct_data = integrate_features(test_struct_data, test_embeddings, test_logits)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†ç¬¬ä¸€ä¸ªæ ·æœ¬çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ï¼ˆæ•´åˆåï¼‰: {train_struct_data[0].x.shape[1]}\")  # 1156\n",
    "print(f\"æµ‹è¯•é›†ç¬¬ä¸€ä¸ªæ ·æœ¬çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ï¼ˆæ•´åˆåï¼‰: {test_struct_data[0].x.shape[1]}\") # 1156\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(ProteinDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_dataset = ProteinDataset(train_struct_data)\n",
    "test_dataset = ProteinDataset(test_struct_data)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5rxB9opFQZv"
   },
   "outputs": [],
   "source": [
    "class DeepGATModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, hidden_dim, out_dim, num_heads=4, dropout=0.3, num_layers=3):\n",
    "        super(DeepGATModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # è¾¹ç‰¹å¾é¢„å¤„ç†å±‚\n",
    "        self.edge_preprocess = nn.Sequential(\n",
    "            nn.Linear(edge_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # å †å  GAT å±‚\n",
    "        for layer in range(num_layers):\n",
    "            in_dim = node_feature_dim if layer == 0 else hidden_dim * num_heads\n",
    "            self.convs.append(GATConv(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                edge_dim=hidden_dim,  # è°ƒæ•´ä¸ºé¢„å¤„ç†åçš„è¾¹ç‰¹å¾ç»´åº¦\n",
    "                add_self_loops=True  # æ·»åŠ è‡ªç¯ï¼Œå¢å¼ºç¨³å®šæ€§ã€‚èƒ½æå‡ç¨³å®šæ€§ï¼ˆæ¯ä¸ªèŠ‚ç‚¹è‡³å°‘ä¿ç•™è‡ªèº«ä¿¡æ¯ï¼‰\n",
    "            ))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n",
    "\n",
    "        # æ›¿æ¢ Set2Set ä¸ºæ›´ç®€å•çš„æ± åŒ–\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_heads, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        # é¢„å¤„ç†è¾¹ç‰¹å¾\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, out_dim, num_layers=3, dropout=0.5):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.convs.append(SAGEConv(node_feature_dim, hidden_dim))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class GINModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, out_dim, num_layers=3, dropout=0.5):\n",
    "        super(GINModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                nn_lin = nn.Sequential(nn.Linear(node_feature_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "            else:\n",
    "                nn_lin = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.convs.append(GINConv(nn_lin))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# äº¤å‰æ³¨æ„åŠ›èåˆæ¨¡å—\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        # äº¤å‰æ³¨æ„åŠ›æœºåˆ¶\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "        self.fc = nn.Linear(feature_dim, 2)  # æœ€ç»ˆåˆ†ç±»å±‚ï¼Œè¾“å‡º 2 ç±»\n",
    "\n",
    "    def forward(self, features_list):\n",
    "        # features_list: [model1_features, model2_features, model3_features], æ¯ä¸ªå½¢çŠ¶ä¸º [batch_size, feature_dim]\n",
    "        # å †å ç‰¹å¾ä¸º [num_models, batch_size, feature_dim]\n",
    "        feats = torch.stack(features_list, dim=0)   # features_list = [feat_gat, feat_sage, feat_gin]\n",
    "        # åº”ç”¨äº¤å‰æ³¨æ„åŠ›\n",
    "        attn_output, _ = self.attention(feats, feats, feats)\n",
    "        # èåˆç‰¹å¾ï¼šå–å¹³å‡å€¼\n",
    "        fused_feats = attn_output.mean(dim=0)  # [batch_size, feature_dim]\n",
    "        fused_feats = self.norm(fused_feats)\n",
    "        # åˆ†ç±»\n",
    "        out = self.fc(fused_feats)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=10, model_save_path='best_model.pth'):\n",
    "    best_test_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in tqdm(train_loader, desc=f'è®­ç»ƒ Epoch {epoch}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "        train_acc, _, _ = test(model, train_loader, device)\n",
    "        test_acc, test_trues, test_preds = test(model, test_loader, device)\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"æ—©åœï¼šåœ¨ç¬¬ {epoch} è½®è®­ç»ƒåï¼Œæ— æå‡ï¼Œåœæ­¢è®­ç»ƒã€‚\")\n",
    "                break\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return best_test_acc, best_model_wts\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='è¯„ä¼°'):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            trues.extend(data.y.cpu().numpy())\n",
    "            correct += (pred == data.y).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy, trues, preds\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "def detailed_test(model, loader, device, models=None):\n",
    "    model.eval()\n",
    "    preds, trues, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='è¯¦ç»†è¯„ä¼°'):\n",
    "            data = data.to(device)\n",
    "            if isinstance(model, CrossAttentionFusion):  # æ£€æŸ¥æ˜¯å¦ä¸ºèåˆæ¨¡å‹\n",
    "                if models is None:\n",
    "                    raise ValueError(\"models dictionary required for CrossAttentionFusion evaluation\")\n",
    "                # æå–ç‰¹å¾åˆ—è¡¨\n",
    "                feat_gat = models['DeepGATModel'].get_last_layer_features(data)\n",
    "                feat_sage = models['GraphSAGEModel'].get_last_layer_features(data)\n",
    "                feat_gin = models['GINModel'].get_last_layer_features(data)\n",
    "                features_list = [feat_gat, feat_sage, feat_gin]\n",
    "                out = model(features_list)\n",
    "            else:\n",
    "                out = model(data)  # æ™®é€šæ¨¡å‹ç›´æ¥å¤„ç† DataBatch\n",
    "            prob = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "            true = data.y.cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            trues.extend(true)\n",
    "            probs.extend(prob)\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(trues, preds, average='binary')\n",
    "    mcc = matthews_corrcoef(trues, preds)\n",
    "    auc = roc_auc_score(trues, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(trues, preds).ravel()\n",
    "    sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics = {'acc': acc, 'mcc': mcc, 'auc': auc, 'sn': sn, 'sp': sp, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    return metrics\n",
    "\n",
    "def optimize_model(model_class, train_loader, test_loader, device, model_params, n_trials=5):\n",
    "    def objective(trial):\n",
    "        # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 64, 512)\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "\n",
    "        # æ ¹æ®æ¨¡å‹ç±»åˆå§‹åŒ–æ¨¡å‹\n",
    "        if model_class == DeepGATModel:\n",
    "            num_heads = trial.suggest_int('num_heads', 2, 16)\n",
    "            model = DeepGATModel(\n",
    "                node_feature_dim=model_params['node_feature_dim'],\n",
    "                edge_feature_dim=model_params['edge_feature_dim'],\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=model_params['out_dim'],\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                num_layers=num_layers\n",
    "            ).to(device)\n",
    "        elif model_class == GraphSAGEModel:\n",
    "            model = GraphSAGEModel(\n",
    "                node_feature_dim=model_params['node_feature_dim'],\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=model_params['out_dim'],\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "        elif model_class == GINModel:\n",
    "            model = GINModel(\n",
    "                node_feature_dim=model_params['node_feature_dim'],\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=model_params['out_dim'],\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        best_acc, _ = train_model(\n",
    "            model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
    "            device, num_epochs=50, patience=10, model_save_path=f\"best_{model_class.__name__}.pth\"\n",
    "        )\n",
    "        return best_acc\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=5)\n",
    "    print(f\"æ€»è¯•éªŒæ¬¡æ•°: {len(study.trials)}\")\n",
    "    print(f\"{model_class.__name__} æœ€ä½³è¶…å‚æ•°: {study.best_params}\")\n",
    "    for trial in study.trials:\n",
    "      print(f\"Trial {trial.number}: State={trial.state}, Value={trial.value}\")\n",
    "    return study.best_params\n",
    "\n",
    "def explain_features(model, test_loader, device, output_folder):\n",
    "    # ESM-C ç‰¹å¾çš„ SHAP åˆ†æ\n",
    "    print(\"æ­£åœ¨è¿›è¡Œ ESM-C ç‰¹å¾çš„ SHAP åˆ†æ...\")\n",
    "    esmc_features = np.hstack([train_embeddings, train_logits])\n",
    "    labels = train_labels\n",
    "    proxy_model = XGBClassifier()\n",
    "    proxy_model.fit(esmc_features, labels)\n",
    "    explainer = shap.Explainer(proxy_model)\n",
    "    shap_values = explainer(esmc_features)\n",
    "    shap.summary_plot(shap_values, esmc_features, plot_type=\"bar\", show=False)\n",
    "    plt.title(\"ESM-C ç‰¹å¾é‡è¦æ€§ (SHAP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, \"shap_esmc_features.png\"))\n",
    "    plt.close()\n",
    "    print(\"SHAP åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ shap_esmc_features.png\")\n",
    "\n",
    "    #GNNExplainer åˆ†æï¼ˆä»¥ DeepGATModel ä¸ºä¾‹ï¼‰\n",
    "    print(\"æ­£åœ¨è¿›è¡Œ GNNExplainer åˆ†æ...\")\n",
    "    trained_model = models['DeepGATModel']\n",
    "    explainer = GNNExplainer(trained_model, epochs=200, lr=0.01)\n",
    "    for sample_idx in range(min(5, len(test_struct_data))):\n",
    "        data = test_struct_data[sample_idx].to(device)\n",
    "        node_idx = 0  # åˆ†æç¬¬ä¸€ä¸ªèŠ‚ç‚¹\n",
    "        node_feat_mask, edge_mask = explainer.explain_node(node_idx, data.x, data.edge_index, data.edge_attr)\n",
    "        print(f\"æ ·æœ¬ {sample_idx+1} | èŠ‚ç‚¹ 0 ç‰¹å¾é‡è¦æ€§ï¼ˆå‰5ä¸ªï¼‰: {node_feat_mask[:5]} | è¾¹é‡è¦æ€§ï¼ˆå‰5ä¸ªï¼‰: {edge_mask[:5]}\")\n",
    "    print(\"GNNExplainer åˆ†æå®Œæˆ\")\n",
    "\n",
    "    # t-SNE å¯è§†åŒ–\n",
    "    print(\"æ­£åœ¨è¿›è¡Œ t-SNE å¯è§†åŒ–...\")\n",
    "    def get_last_layer_features(model, loader, device):\n",
    "        model.eval()\n",
    "        features = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                feat = model.get_last_layer_features(data)\n",
    "                features.append(feat.cpu().numpy())\n",
    "                labels.append(data.y.cpu().numpy())\n",
    "        return np.vstack(features), np.hstack(labels)\n",
    "\n",
    "    features, labels = get_last_layer_features(trained_model, test_loader, device)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels, cmap='coolwarm', alpha=0.6)\n",
    "    plt.title(\"t-SNE of Last Layer Features (DeepGATModel)\")\n",
    "    plt.colorbar(label='Class')\n",
    "    plt.savefig(os.path.join(output_folder, \"tsne_last_layer.png\"))\n",
    "    plt.close()\n",
    "    print(\"t-SNE å¯è§†åŒ–å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ tsne_last_layer.png\")\n",
    "\n",
    "    return results, models\n",
    "\n",
    "def train_and_evaluate_models(train_loader, test_loader, device, output_folder):\n",
    "    model_params = {\"node_feature_dim\": 1156, \"edge_feature_dim\": 4, \"out_dim\": 2}\n",
    "    best_params = {}\n",
    "\n",
    "    # ä¼˜åŒ–å¹¶è®­ç»ƒä¸‰ä¸ªåŸºç¡€æ¨¡å‹\n",
    "    for model_class in [DeepGATModel, GraphSAGEModel, GINModel]:\n",
    "        print(f\"ä¼˜åŒ– {model_class.__name__}...\")\n",
    "        best_params[model_class.__name__] = optimize_model(model_class, train_loader, test_loader, device, model_params, n_trials=10)\n",
    "\n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    models = {\n",
    "        \"DeepGATModel\": DeepGATModel(\n",
    "            node_feature_dim=1156, edge_feature_dim=4, out_dim=2,\n",
    "            hidden_dim=best_params[\"DeepGATModel\"][\"hidden_dim\"],\n",
    "            num_layers=best_params[\"DeepGATModel\"][\"num_layers\"],\n",
    "            dropout=best_params[\"DeepGATModel\"][\"dropout\"],\n",
    "            num_heads=best_params[\"DeepGATModel\"][\"num_heads\"]\n",
    "        ).to(device),\n",
    "        \"GraphSAGEModel\": GraphSAGEModel(\n",
    "            node_feature_dim=1156, out_dim=2,\n",
    "            hidden_dim=best_params[\"GraphSAGEModel\"][\"hidden_dim\"],\n",
    "            num_layers=best_params[\"GraphSAGEModel\"][\"num_layers\"],\n",
    "            dropout=best_params[\"GraphSAGEModel\"][\"dropout\"]\n",
    "        ).to(device),\n",
    "        \"GINModel\": GINModel(\n",
    "            node_feature_dim=1156, out_dim=2,\n",
    "            hidden_dim=best_params[\"GINModel\"][\"hidden_dim\"],\n",
    "            num_layers=best_params[\"GINModel\"][\"num_layers\"],\n",
    "            dropout=best_params[\"GINModel\"][\"dropout\"]\n",
    "        ).to(device)\n",
    "    }\n",
    "\n",
    "    # è®­ç»ƒå¹¶è¯„ä¼°æ¯ä¸ªæ¨¡å‹\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=best_params[name][\"lr\"], weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        save_path = os.path.join(output_folder, f\"best_{name}.pth\")\n",
    "        best_acc, _ = train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=10, model_save_path=save_path)\n",
    "        metrics = detailed_test(model, test_loader, device)\n",
    "        results[name] = metrics\n",
    "        print(f\"{name} - Acc: {metrics['acc']:.4f}, MCC: {metrics['mcc']:.4f}, AUC: {metrics['auc']:.4f}\")\n",
    "\n",
    "    # æ€§èƒ½å¯¹æ¯”\n",
    "    print(\"\\n### ä¸‰ä¸ªæ¨¡å‹æ€§èƒ½å¯¹æ¯” ###\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"{name}: Acc: {metrics['acc']:.4f}, MCC: {metrics['mcc']:.4f}, AUC: {metrics['auc']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "    # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    for name, model in models.items():\n",
    "        model.load_state_dict(torch.load(os.path.join(output_folder, f\"best_{name}.pth\")))\n",
    "        model.eval()\n",
    "\n",
    "    return results, models\n",
    "\n",
    "# äº¤å‰æ³¨æ„åŠ›èåˆè®­ç»ƒ\n",
    "# åœ¨äº¤å‰æ³¨æ„åŠ›èåˆè®­ç»ƒå‡½æ•°ä¸­ä¿®æ”¹\n",
    "def train_cross_attention_fusion(models, train_loader, test_loader, device, output_folder, num_epochs=50, patience=10):\n",
    "    fusion_module = CrossAttentionFusion(feature_dim=256, num_heads=4, dropout=0.1).to(device)\n",
    "    optimizer_fusion = torch.optim.Adam(fusion_module.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler_fusion = torch.optim.lr_scheduler.StepLR(optimizer_fusion, step_size=10, gamma=0.1)\n",
    "\n",
    "    print(\"\\n### è®­ç»ƒäº¤å‰æ³¨æ„åŠ›èåˆæ¨¡å— ###\")\n",
    "    best_fusion_acc = 0\n",
    "    best_fusion_wts = copy.deepcopy(fusion_module.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        fusion_module.train()\n",
    "        total_loss = 0\n",
    "        for data in tqdm(train_loader, desc=f'èåˆè®­ç»ƒ Epoch {epoch}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                # ç¡®ä¿è·å–çš„æ˜¯çº¯å¼ é‡\n",
    "                feat_gat = models['DeepGATModel'].get_last_layer_features(data)\n",
    "                feat_sage = models['GraphSAGEModel'].get_last_layer_features(data)\n",
    "                feat_gin = models['GINModel'].get_last_layer_features(data)\n",
    "                # è°ƒè¯•ï¼šæ‰“å°ç‰¹å¾å½¢çŠ¶å’Œç±»å‹\n",
    "                print(f\"feat_gat shape: {feat_gat.shape}, type: {type(feat_gat)}\")\n",
    "                print(f\"feat_sage shape: {feat_sage.shape}, type: {type(feat_sage)}\")\n",
    "                print(f\"feat_gin shape: {feat_gin.shape}, type: {type(feat_gin)}\")\n",
    "                # å¦‚æœè¿”å›çš„æ˜¯ DataBatchï¼Œæå–ç‰¹å¾å¼ é‡\n",
    "                if isinstance(feat_gat, torch.Tensor) and feat_gat.dim() == 2:  # ç¡®ä¿æ˜¯ [batch_size, feature_dim]\n",
    "                    features_list = [feat_gat, feat_sage, feat_gin]\n",
    "                else:\n",
    "                    raise ValueError(\"Expected pure tensors from get_last_layer_features, got unexpected type or shape\")\n",
    "            out = fusion_module(features_list)\n",
    "            loss = criterion(out, data.y)\n",
    "            optimizer_fusion.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_fusion.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        scheduler_fusion.step()\n",
    "\n",
    "        fusion_module.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                data = data.to(device)\n",
    "                feat_gat = models['DeepGATModel'].get_last_layer_features(data)\n",
    "                feat_sage = models['GraphSAGEModel'].get_last_layer_features(data)\n",
    "                feat_gin = models['GINModel'].get_last_layer_features(data)\n",
    "                # åŒæ ·çš„æ£€æŸ¥å’Œå¤„ç†\n",
    "                if isinstance(feat_gat, torch.Tensor) and feat_gat.dim() == 2:\n",
    "                    features_list = [feat_gat, feat_sage, feat_gin]\n",
    "                else:\n",
    "                    raise ValueError(\"Expected pure tensors from get_last_layer_features in test loop\")\n",
    "                out = fusion_module(features_list)\n",
    "                pred = out.argmax(dim=1)\n",
    "                preds.extend(pred.cpu().numpy())\n",
    "                trues.extend(data.y.cpu().numpy())\n",
    "        test_acc = accuracy_score(trues, preds)\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {avg_loss:.4f}, Fusion Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        if test_acc > best_fusion_acc:\n",
    "            best_fusion_acc = test_acc\n",
    "            best_fusion_wts = copy.deepcopy(fusion_module.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(fusion_module.state_dict(), os.path.join(output_folder, \"best_cross_attention_fusion.pth\"))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"æ—©åœï¼šåœ¨ç¬¬ {epoch} è½®è®­ç»ƒåï¼Œæ— æå‡ï¼Œåœæ­¢è®­ç»ƒã€‚\")\n",
    "                break\n",
    "\n",
    "    fusion_module.load_state_dict(best_fusion_wts)\n",
    "    fusion_metrics = detailed_test(fusion_module, test_loader, device, models=models)  # ä¼ é€’ models å‚æ•°\n",
    "    return fusion_module, fusion_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3797213,
     "status": "ok",
     "timestamp": 1759399696297,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "hj-siF4M54al",
    "outputId": "0da1ee9e-15d0-4f14-fb2f-d99737acc934"
   },
   "outputs": [],
   "source": [
    "# ### è¿è¡Œå®éªŒ\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "    results, models = train_and_evaluate_models(train_loader, test_loader, device, aggregated_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "v-TWJS8_bYqw",
    "outputId": "c0f3eceb-1170-4cf6-ecf7-5ceae06340e0"
   },
   "outputs": [],
   "source": [
    "fusion_module, fusion_metrics = train_cross_attention_fusion(models, train_loader, test_loader, device, aggregated_output_folder)\n",
    "results[\"CrossAttentionFusion\"] = fusion_metrics\n",
    "print(f\"CrossAttentionFusion - Acc: {fusion_metrics['acc']:.4f}, MCC: {fusion_metrics['mcc']:.4f}, AUC: {fusion_metrics['auc']:.4f}, Precision: {fusion_metrics['precision']:.4f}, Recall: {fusion_metrics['recall']:.4f}, F1: {fusion_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIlrqhJhTvaP"
   },
   "source": [
    "# å•è°ƒGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5BI2ntGwZFJ8",
    "outputId": "46977f40-9e3b-40ca-afc8-250b22c50712"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, precision_recall_fscore_support,\n",
    "    matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import copy\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡ï¼ˆGPU/CPUï¼‰\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# è·¯å¾„é…ç½®\n",
    "esmc_folders = {\n",
    "    'train_pos': '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    'train_neg': '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    'test_pos': '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',\n",
    "    'test_neg': '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "}\n",
    "\n",
    "struct_folders = {\n",
    "    'train': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/train_dataset.json',\n",
    "    'test': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/test_dataset.json'\n",
    "}\n",
    "\n",
    "output_folder = '/content/drive/MyDrive/AFP_work/deepgat_explain_results'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ### æ•°æ®åŠ è½½å’Œé¢„å¤„ç†\n",
    "\n",
    "# **åŠ è½½ ESM-C ç‰¹å¾**\n",
    "def load_esmc_features(esmc_folder, save_path=None):\n",
    "    logits_path = os.path.join(esmc_folder, 'combined_logits.npy')\n",
    "    embeddings_path = os.path.join(esmc_folder, 'combined_embeddings.npy')\n",
    "    logits = np.load(logits_path, allow_pickle=True)\n",
    "    embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # è°ƒè¯•ï¼šæ£€æŸ¥ logits å’Œ embeddings çš„ç»“æ„\n",
    "    print(f\"Logits[0] ç±»å‹: {type(logits[0])}, å€¼: {logits[0]}\")\n",
    "    print(f\"Embeddings[0] å½¢çŠ¶: {embeddings[0].shape}, æ ·æœ¬: {embeddings[0][:5]}\")\n",
    "\n",
    "    # ä» ForwardTrackData ä¸­æå– sequence å¼ é‡å¹¶æ± åŒ–\n",
    "    logits_values = []\n",
    "    for l in logits:\n",
    "        forward_data = l[0] if isinstance(l, np.ndarray) else l\n",
    "        sequence_tensor = forward_data.sequence  # è·å–å¼ é‡\n",
    "        sequence_tensor = sequence_tensor.to(device='cpu', dtype=torch.float32)\n",
    "        pooled_value = sequence_tensor.mean(dim=[0, 1, 2]).item()  # æ± åŒ–ä¸ºæ ‡é‡\n",
    "        logits_values.append(pooled_value)\n",
    "\n",
    "    logits_values = np.array(logits_values, dtype=np.float32).reshape(-1, 1)\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    label = 1 if 'pos' in esmc_folder else 0\n",
    "    labels = np.full((logits_values.shape[0],), label)\n",
    "    return logits_values, embeddings, labels\n",
    "\n",
    "# åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ESM-C ç‰¹å¾\n",
    "train_pos_logits, train_pos_embeddings, train_pos_labels = load_esmc_features(esmc_folders['train_pos'])\n",
    "train_neg_logits, train_neg_embeddings, train_neg_labels = load_esmc_features(esmc_folders['train_neg'])\n",
    "test_pos_logits, test_pos_embeddings, test_pos_labels = load_esmc_features(esmc_folders['test_pos'])\n",
    "test_neg_logits, test_neg_embeddings, test_neg_labels = load_esmc_features(esmc_folders['test_neg'])\n",
    "\n",
    "# åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾\n",
    "train_logits = np.vstack((train_pos_logits, train_neg_logits))\n",
    "train_embeddings = np.vstack((train_pos_embeddings, train_neg_embeddings))\n",
    "train_labels = np.hstack((train_pos_labels, train_neg_labels))\n",
    "\n",
    "test_logits = np.vstack((test_pos_logits, test_neg_logits))\n",
    "test_embeddings = np.vstack((test_pos_embeddings, test_neg_embeddings))\n",
    "test_labels = np.hstack((test_pos_labels, test_neg_labels))\n",
    "\n",
    "print(f\"è®­ç»ƒé›† logits å½¢çŠ¶: {train_logits.shape}\")\n",
    "print(f\"è®­ç»ƒé›† embeddings å½¢çŠ¶: {train_embeddings.shape}\")\n",
    "print(f\"è®­ç»ƒé›† labels å½¢çŠ¶: {train_labels.shape}\")\n",
    "print(f\"æµ‹è¯•é›† logits å½¢çŠ¶: {test_logits.shape}\")\n",
    "print(f\"æµ‹è¯•é›† embeddings å½¢çŠ¶: {test_embeddings.shape}\")\n",
    "print(f\"æµ‹è¯•é›† labels å½¢çŠ¶: {test_labels.shape}\")\n",
    "\n",
    "# **åŠ è½½ç»“æ„ç‰¹å¾**\n",
    "def load_struct_features(json_path, sample_limit=5):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    data_list = []\n",
    "    for idx, sample in enumerate(tqdm(json_data, desc=f'åŠ è½½ç»“æ„ç‰¹å¾ from {json_path}')):\n",
    "        required_keys = ['node_features', 'edge_features', 'label']\n",
    "        if not all(key in sample for key in required_keys):\n",
    "            print(f\" æ ·æœ¬ç¼ºå°‘å¿…è¦çš„é”®: {sample}\")\n",
    "            continue\n",
    "        node_features = sample['node_features']\n",
    "        edge_features = sample['edge_features']\n",
    "        label = sample['label']\n",
    "        edges = edge_features.get('edges', [])\n",
    "        directions = edge_features.get('directions', [])\n",
    "        rotations = edge_features.get('rotations', [])\n",
    "        num_edges = len(edges)\n",
    "        if not (len(directions) == num_edges and len(rotations) == num_edges):\n",
    "            print(f\" è¾¹çš„æ•°é‡ä¸æ–¹å‘æˆ–æ—‹è½¬æ•°é‡ä¸åŒ¹é…: {sample}\")\n",
    "            continue\n",
    "        if num_edges > 0:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            directions = torch.tensor(directions, dtype=torch.float)\n",
    "            rotations = torch.tensor(rotations, dtype=torch.float).unsqueeze(1)\n",
    "            edge_attr = torch.cat([directions, rotations], dim=1)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.empty((0, 4), dtype=torch.float)\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=label)\n",
    "        data_list.append(data)\n",
    "        if idx < sample_limit:\n",
    "            num_nodes = node_features.shape[0]\n",
    "            node_feature_dim = node_features.shape[1]\n",
    "            print(f\"æ ·æœ¬ {idx+1}: èŠ‚ç‚¹æ•°é‡: {num_nodes}, èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {node_feature_dim}, è¾¹æ•°é‡: {num_edges}\")\n",
    "            if num_edges > 0:\n",
    "                print(f\"  è¾¹ç‰¹å¾ç»´åº¦: {edge_attr.shape[1]}\")\n",
    "            print(\"-\" * 50)\n",
    "    print(f\"\\næ‰€æœ‰æ ·æœ¬ä¸­å”¯ä¸€çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {set([data.x.shape[1] for data in data_list])}\")\n",
    "    print(f\"æ‰€æœ‰æ ·æœ¬ä¸­å”¯ä¸€çš„è¾¹ç‰¹å¾ç»´åº¦: {set([data.edge_attr.shape[1] for data in data_list if data.edge_attr.shape[0] > 0])}\")\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = load_struct_features(struct_folders['train'])\n",
    "test_struct_data = load_struct_features(struct_folders['test'])\n",
    "\n",
    "# **ç‰¹å¾æ ‡å‡†åŒ–**\n",
    "def normalize_features(train_data_list, test_data_list=None):\n",
    "    node_scaler = StandardScaler()\n",
    "    edge_scaler = StandardScaler()\n",
    "    all_node_features = np.concatenate([data.x.numpy() for data in train_data_list], axis=0)\n",
    "    all_edge_features = np.concatenate([data.edge_attr.numpy() for data in train_data_list if data.edge_attr.shape[0] > 0], axis=0)\n",
    "    node_scaler.fit(all_node_features)\n",
    "    if all_edge_features.size > 0:\n",
    "        edge_scaler.fit(all_edge_features)\n",
    "    for data in train_data_list:\n",
    "        data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "        if data.edge_attr.shape[0] > 0:\n",
    "            data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    if test_data_list:\n",
    "        for data in test_data_list:\n",
    "            data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "            if data.edge_attr.shape[0] > 0:\n",
    "                data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    return train_data_list, test_data_list\n",
    "\n",
    "train_struct_data, test_struct_data = normalize_features(train_struct_data, test_struct_data)\n",
    "\n",
    "# **æ•´åˆ ESM-C ç‰¹å¾**\n",
    "def integrate_features(data_list, embeddings, logits):\n",
    "    if len(data_list) != len(embeddings) or len(data_list) != len(logits):\n",
    "        raise ValueError(f\"data_list, embeddings å’Œ logits é•¿åº¦ä¸åŒ¹é…: {len(data_list)} vs {len(embeddings)} vs {len(logits)}\")\n",
    "    for i, data in enumerate(tqdm(data_list, desc='æ•´åˆ ESM-C embeddings å’Œ logits')):\n",
    "        embedding = torch.tensor(embeddings[i], dtype=torch.float)  # [1152]\n",
    "        logit = torch.tensor(logits[i], dtype=torch.float).squeeze()  # [1] -> æ ‡é‡\n",
    "        combined_feature = torch.cat([embedding, logit.unsqueeze(0)], dim=0)  # [1153]\n",
    "        num_nodes = data.x.shape[0]\n",
    "        combined_expanded = combined_feature.unsqueeze(0).repeat(num_nodes, 1)  # [num_nodes, 1153]\n",
    "        data.x = torch.cat([data.x, combined_expanded], dim=1)  # [num_nodes, 1156]\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = integrate_features(train_struct_data, train_embeddings, train_logits)\n",
    "test_struct_data = integrate_features(test_struct_data, test_embeddings, test_logits)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†ç¬¬ä¸€ä¸ªæ ·æœ¬çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ï¼ˆæ•´åˆåï¼‰: {train_struct_data[0].x.shape[1]}\")\n",
    "print(f\"æµ‹è¯•é›†ç¬¬ä¸€ä¸ªæ ·æœ¬çš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ï¼ˆæ•´åˆåï¼‰: {test_struct_data[0].x.shape[1]}\")\n",
    "\n",
    "# **åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨**\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(ProteinDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_dataset = ProteinDataset(train_struct_data)\n",
    "test_dataset = ProteinDataset(test_struct_data)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ### è°ƒè¯• DeepGATModel\n",
    "class DeepGATModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, hidden_dim, out_dim, num_heads=4, dropout=0.3, num_layers=3):\n",
    "        super(DeepGATModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # è¾¹ç‰¹å¾é¢„å¤„ç†å±‚\n",
    "        self.edge_preprocess = nn.Sequential(\n",
    "            nn.Linear(edge_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # åˆå§‹åŒ–GATå±‚\n",
    "        for layer in range(num_layers):\n",
    "            in_dim = node_feature_dim if layer == 0 else hidden_dim * num_heads\n",
    "            self.convs.append(GATConv(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                edge_dim=hidden_dim,  # é¢„å¤„ç†åçš„è¾¹ç‰¹å¾ç»´åº¦\n",
    "                add_self_loops=True  # æ·»åŠ è‡ªç¯ï¼Œå¢å¼ºç¨³å®šæ€§\n",
    "            ))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n",
    "\n",
    "        # æ± åŒ–å’Œå…¨è¿æ¥å±‚\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_heads, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data, print_shapes=False):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # ä»…åœ¨ç¬¬ä¸€æ¬¡æ‰¹æ¬¡æ‰“å°å½¢çŠ¶ï¼ˆæ¯è½®è¯•éªŒä¸€æ¬¡ï¼‰\n",
    "        if print_shapes:\n",
    "            print(f\"è¾“å…¥ - èŠ‚ç‚¹ç‰¹å¾å½¢çŠ¶: {x.shape}, è¾¹ç‰¹å¾å½¢çŠ¶: {edge_attr.shape if edge_attr is not None else 'æ— '}, è¾¹ç´¢å¼•å½¢çŠ¶: {edge_index.shape}\")\n",
    "\n",
    "        # é¢„å¤„ç†è¾¹ç‰¹å¾\n",
    "        if edge_attr is not None and edge_attr.shape[0] > 0:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "            if print_shapes:\n",
    "                print(f\"é¢„å¤„ç†åè¾¹ç‰¹å¾å½¢çŠ¶: {edge_attr.shape}\")\n",
    "        else:\n",
    "            edge_attr = None\n",
    "            if print_shapes:\n",
    "                print(\"æ— è¾¹ç‰¹å¾ï¼Œä½¿ç”¨é»˜è®¤è¾¹å¤„ç†\")\n",
    "\n",
    "        # é€å±‚å¤„ç†\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.batch_norms)):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            if print_shapes:\n",
    "                print(f\"GATConvå±‚ {i+1} è¾“å‡ºå½¢çŠ¶: {x.shape}\")\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # æ± åŒ–\n",
    "        x = self.readout(x, batch)\n",
    "        if print_shapes:\n",
    "            print(f\"æ± åŒ–åç‰¹å¾å½¢çŠ¶: {x.shape}\")\n",
    "\n",
    "        # å…¨è¿æ¥å±‚\n",
    "        x = self.fc1(x)\n",
    "        if print_shapes:\n",
    "            print(f\"FC1è¾“å‡ºå½¢çŠ¶: {x.shape}\")\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        if print_shapes:\n",
    "            print(f\"FC2è¾“å‡ºå½¢çŠ¶: {x.shape}\")  # åº”ä¸º [batch_size, 2]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        if edge_attr is not None and edge_attr.shape[0] > 0:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "        else:\n",
    "            edge_attr = None\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# ### è®­ç»ƒå’Œè°ƒè¯• DeepGATModel\n",
    "def train_deepgat(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=10, model_save_path='best_deepgat.pth'):\n",
    "    best_test_acc = 0\n",
    "    best_model_wts = None  # åˆå§‹å€¼è®¾ç½®ä¸ºNone\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        # ä»…åœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ‰“å°å½¢çŠ¶ï¼ˆæ¯è½®è¯•éªŒä¸€æ¬¡ï¼‰\n",
    "        print_shapes = (epoch == 1)  # ä»…ç¬¬ä¸€è½®æ‰“å°\n",
    "        for data in tqdm(train_loader, desc=f'è®­ç»ƒ DeepGAT Epoch {epoch}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data, print_shapes=print_shapes)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            if print_shapes:\n",
    "                print(f\"æ‰¹æ¬¡æŸå¤±: {loss.item():.4f}\")\n",
    "                print_shapes = False  # ä»…æ‰“å°ä¸€æ¬¡\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "        train_acc, train_trues, train_preds = test(model, train_loader, device)\n",
    "        test_acc, test_trues, test_preds = test(model, test_loader, device)\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())  # æ›´æ–°æœ€ä½³æƒé‡\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(best_model_wts, model_save_path)  # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            print(f\"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"æ—©åœï¼šåœ¨ç¬¬ {epoch} è½®è®­ç»ƒåï¼Œæ— æå‡ï¼Œåœæ­¢è®­ç»ƒã€‚\")\n",
    "                break\n",
    "\n",
    "    # åŠ è½½æœ€ä½³æƒé‡\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    else:\n",
    "        print(\"è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ€ä½³æƒé‡ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€ã€‚\")\n",
    "    return best_test_acc, best_model_wts\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='è¯„ä¼° DeepGAT'):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            trues.extend(data.y.cpu().numpy())\n",
    "            correct += (pred == data.y).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy, trues, preds\n",
    "\n",
    "def detailed_test(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, trues, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='è¯¦ç»†è¯„ä¼° DeepGAT'):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            prob = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "            true = data.y.cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            trues.extend(true)\n",
    "            probs.extend(prob)\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(trues, preds, average='binary')\n",
    "    mcc = matthews_corrcoef(trues, preds)\n",
    "    auc = roc_auc_score(trues, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(trues, preds).ravel()\n",
    "    sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics = {'acc': acc, 'mcc': mcc, 'auc': auc, 'sn': sn, 'sp': sp, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    return metrics\n",
    "\n",
    "# ### è¶…å‚æ•°ä¼˜åŒ– (Optuna)\n",
    "\n",
    "def optimize_deepgat(train_loader, test_loader, device, n_trials=10):\n",
    "    def objective(trial):\n",
    "        # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 64, 512)\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "        num_heads = trial.suggest_int('num_heads', 2, 16)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "\n",
    "        # åˆå§‹åŒ– DeepGATModel\n",
    "        model = DeepGATModel(\n",
    "            node_feature_dim=1156,  # æ•´åˆåçš„èŠ‚ç‚¹ç‰¹å¾ç»´åº¦\n",
    "            edge_feature_dim=4,     # è¾¹ç‰¹å¾ç»´åº¦\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=2,             # äºŒåˆ†ç±»\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            num_layers=num_layers\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        # è®­ç»ƒå¹¶è¿”å›æœ€ä½³å‡†ç¡®ç‡å’Œæƒé‡\n",
    "        best_acc, best_wts = train_deepgat(\n",
    "            model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
    "            device, num_epochs=50, patience=10, model_save_path='best_deepgat_optimized.pth'\n",
    "        )\n",
    "        return best_acc\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    print(f\"æ€»è¯•éªŒæ¬¡æ•°: {len(study.trials)}\")\n",
    "    print(f\"DeepGATModel æœ€ä½³è¶…å‚æ•°: {study.best_params}\")\n",
    "    for trial in study.trials:\n",
    "        print(f\"Trial {trial.number}: State={trial.state}, Value={trial.value}\")\n",
    "    return study.best_params\n",
    "\n",
    "# ### å¯è§£é‡Šæ€§åˆ†æå‡½æ•°\n",
    "\n",
    "def explain_deepgat(model, train_loader, test_loader, device, output_folder):\n",
    "    # 1. SHAPåˆ†æï¼ˆESM-Cç‰¹å¾ï¼‰\n",
    "    print(\"æ­£åœ¨è¿›è¡Œ ESM-C ç‰¹å¾çš„ SHAP åˆ†æ...\")\n",
    "    esmc_features = np.hstack([train_embeddings, train_logits])  # [num_samples, 1153]\n",
    "    labels = train_labels\n",
    "    proxy_model = XGBClassifier()\n",
    "    proxy_model.fit(esmc_features, labels)\n",
    "    explainer = shap.Explainer(proxy_model)\n",
    "    shap_values = explainer(esmc_features)\n",
    "    # ç»˜åˆ¶SHAPç‰¹å¾é‡è¦æ€§\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, esmc_features, plot_type=\"bar\", show=False)\n",
    "    plt.title(\"ESM-C ç‰¹å¾é‡è¦æ€§ (SHAP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, \"shap_esmc_features.png\"))\n",
    "    plt.close()\n",
    "    print(\"SHAP åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ shap_esmc_features.png\")\n",
    "\n",
    "    # 2. GNNExplaineråˆ†æï¼ˆèŠ‚ç‚¹å’Œè¾¹çš„é‡è¦æ€§ï¼‰\n",
    "    print(\"æ­£åœ¨è¿›è¡Œ GNNExplainer åˆ†æ...\")\n",
    "    model.eval()\n",
    "    reset(model)  # é‡ç½®æ¨¡å‹å‚æ•°ä»¥ç¡®ä¿è§£é‡Šä¸€è‡´æ€§\n",
    "    explainer = GNNExplainer(model, epochs=200, lr=0.01)\n",
    "    for sample_idx in range(min(5, len(test_struct_data))):  # åˆ†æå‰5ä¸ªæµ‹è¯•æ ·æœ¬\n",
    "        data = test_struct_data[sample_idx].to(device)\n",
    "        node_idx = 0  # åˆ†æç¬¬ä¸€ä¸ªèŠ‚ç‚¹\n",
    "        node_feat_mask, edge_mask = explainer.explain_node(node_idx, data.x, data.edge_index, data.edge_attr)\n",
    "        print(f\"æ ·æœ¬ {sample_idx+1} | èŠ‚ç‚¹ {node_idx} ç‰¹å¾é‡è¦æ€§ï¼ˆå‰5ä¸ªï¼‰: {node_feat_mask[:5]} | è¾¹é‡è¦æ€§ï¼ˆå‰5ä¸ªï¼‰: {edge_mask[:5] if edge_mask is not None else 'æ— '}\")\n",
    "    print(\"GNNExplainer åˆ†æå®Œæˆ\")\n",
    "\n",
    "    # 3. t-SNEå¯è§†åŒ–ï¼ˆæœ€åå±‚ç‰¹å¾åˆ†å¸ƒï¼‰\n",
    "    print(\"æ­£åœ¨è¿›è¡Œ t-SNE å¯è§†åŒ–...\")\n",
    "    def get_last_layer_features(model, loader, device):\n",
    "        model.eval()\n",
    "        features = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                feat = model.get_last_layer_features(data)\n",
    "                features.append(feat.cpu().numpy())\n",
    "                labels.append(data.y.cpu().numpy())\n",
    "        return np.vstack(features), np.hstack(labels)\n",
    "\n",
    "    features, labels = get_last_layer_features(model, test_loader, device)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels, cmap='coolwarm', alpha=0.6)\n",
    "    plt.title(\"DeepGATModel æœ€åå±‚ç‰¹å¾ t-SNE å¯è§†åŒ–\")\n",
    "    plt.colorbar(label='Class (0=Neg, 1=Pos)')\n",
    "    plt.savefig(os.path.join(output_folder, \"tsne_deepgat_features.png\"))\n",
    "    plt.close()\n",
    "    print(\"t-SNE å¯è§†åŒ–å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ tsne_deepgat_features.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    deepgat_model = DeepGATModel(\n",
    "        node_feature_dim=1156,\n",
    "        edge_feature_dim=4,\n",
    "        hidden_dim=256,   \n",
    "        out_dim=2,        \n",
    "        num_heads=4,            \n",
    "        dropout=0.3,            \n",
    "        num_layers=3           \n",
    "    ).to(device)\n",
    "\n",
    "    print(\"å¼€å§‹ä¼˜åŒ– DeepGATModel è¶…å‚æ•°...\")\n",
    "    best_params = optimize_deepgat(train_loader, test_loader, device, n_trials=10)\n",
    "\n",
    "    # ä½¿ç”¨æœ€ä½³è¶…å‚æ•°åˆå§‹åŒ–å¹¶è®­ç»ƒæ¨¡å‹\n",
    "    deepgat_model = DeepGATModel(\n",
    "        node_feature_dim=1156,\n",
    "        edge_feature_dim=4,\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        out_dim=2,\n",
    "        num_heads=best_params['num_heads'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_layers=best_params['num_layers']\n",
    "    ).to(device)\n",
    "\n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(deepgat_model.parameters(), lr=best_params['lr'], weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    best_acc, best_wts = train_deepgat(\n",
    "        deepgat_model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
    "        device, num_epochs=50, patience=10, model_save_path=os.path.join(output_folder, 'best_deepgat_final.pth')\n",
    "    )\n",
    "\n",
    "    # è¯¦ç»†è¯„ä¼°\n",
    "    metrics = detailed_test(deepgat_model, test_loader, device)\n",
    "    print(\"\\n### DeepGATModel è¯¦ç»†æ€§èƒ½ ###\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # ä¿å­˜æ¨¡å‹å’Œç»“æœ\n",
    "    if best_wts is not None:\n",
    "        torch.save(best_wts, os.path.join(output_folder, 'best_deepgat_final.pth'))\n",
    "    with open(os.path.join(output_folder, 'deepgat_metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    # è°ƒè¯•ï¼šæ‰“å°éƒ¨åˆ†é¢„æµ‹ç»“æœ\n",
    "    deepgat_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = deepgat_model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            true = data.y\n",
    "            print(f\"é¢„æµ‹æ ·æœ¬ - çœŸå®æ ‡ç­¾: {true[:5].cpu().numpy()}, é¢„æµ‹æ ‡ç­¾: {pred[:5].cpu().numpy()}\")\n",
    "            break  # ä»…æ‰“å°ç¬¬ä¸€ä¸ªæ‰¹æ¬¡\n",
    "\n",
    "    # å¯è§£é‡Šæ€§åˆ†æ\n",
    "    explain_deepgat(deepgat_model, train_loader, test_loader, device, output_folder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "a8BDbX0lIZSs",
    "6dUEQFzFIb3o",
    "JnqCMjKgIhng",
    "mBHmxnQzo6b6",
    "5Za8fI8fov8b",
    "fz6lWj8Lx6a_",
    "LADKYcKABvi8",
    "WOq4HHPYwv_h",
    "5RnxO1AX2u46",
    "JQqE1ofqsviH",
    "m0LT6AZYEBL3",
    "RQ-gFfgEuGH0",
    "sf4bWPY6cXlK",
    "4dxS0Eu0uKOq",
    "l2bQyS6_wILJ",
    "uYVaYr92Mecm",
    "4_RLor2ULZf0",
    "KrR3SQWpO5qt",
    "-pnBCTCqhdFH",
    "a4R_Fa-4ax-1",
    "a22DaAZnhjD9",
    "qX5pFqxTy1V0",
    "2CVhwiDzSEor",
    "8orO_EtEEoLh",
    "t_sew4v63Cfs",
    "y_AphIUVpAgK",
    "uzBacJwdpFnU",
    "bQ1ZVdoDDeoz",
    "NIlrqhJhTvaP",
    "XpoEzKQMj3J6",
    "gDwMJ97Z7HGb"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
