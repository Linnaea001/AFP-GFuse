{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8161,
     "status": "ok",
     "timestamp": 1759456559185,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "-521mSLjDu2T",
    "outputId": "7427adb3-5d74-4c61-814c-7a2ff11cb88a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8BDbX0lIZSs"
   },
   "source": [
    "# 包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9455,
     "status": "ok",
     "timestamp": 1759456575827,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "eQTzzqA6IaiX",
    "outputId": "d4ec024f-acf1-40a7-ebfa-bf1051bc0ea5"
   },
   "outputs": [],
   "source": [
    "pip install Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 65838,
     "status": "ok",
     "timestamp": 1759456641667,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "plH_EKiYEvX7",
    "outputId": "005f042a-9444-4573-9411-6d333c14ee46"
   },
   "outputs": [],
   "source": [
    "pip install torch torch-geometric optuna tqdm scikit-learn esm umap-learn shap xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 66614,
     "status": "ok",
     "timestamp": 1759456708288,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "GsFMLAa9waEZ",
    "outputId": "d7551d7e-1c0a-4ca6-d0ee-202b030d69b5"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy pandas\n",
    "!pip install numpy==1.26.4 pandas==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2006,
     "status": "ok",
     "timestamp": 1759456710305,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "8E6hDtucnwBm",
    "outputId": "33742803-5469-4518-e61b-2972e08294b1"
   },
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1759456711932,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "I-6FNFN-T9Bt",
    "outputId": "e0daea41-baf8-495e-a005-f8f8909216fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dUEQFzFIb3o"
   },
   "source": [
    "# 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnqCMjKgIhng"
   },
   "source": [
    "## 阳性样本cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13838,
     "status": "ok",
     "timestamp": 1753430799855,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "JJAvM38xIkKp",
    "outputId": "b9f4bde0-5106-4072-b177-ed45ce0ca42e"
   },
   "outputs": [],
   "source": [
    "!apt-get install cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1753430800008,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "geFeazrvKkPZ",
    "outputId": "951ec2e2-2a32-474b-b893-580795ad0d4f"
   },
   "outputs": [],
   "source": [
    "!grep -c \"^>\" AFP.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1753430800087,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "6HMPMZIdJv6t",
    "outputId": "b8917c51-af19-480d-b3a8-4564d0b47b33"
   },
   "outputs": [],
   "source": [
    "!cd-hit -i AFP.fasta -o AFP_clustered.fasta -c 0.8 -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "error",
     "timestamp": 1753430800229,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "nJiSgc8FL6JT",
    "outputId": "7e4df39f-1f80-48cb-c642-4555118e1d24"
   },
   "outputs": [],
   "source": [
    "def convert_to_fasta(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        sequences = file.readlines()\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            sequence = sequence.strip()  \n",
    "            file.write(f\">Sequence_{i + 1}\\n{sequence}\\n\")\n",
    "\n",
    "convert_to_fasta('AFP_2.txt', 'AFP_2.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4XKTJ4scakY"
   },
   "outputs": [],
   "source": [
    "with open(\"AFP.fasta\", \"r\") as file:\n",
    "    sequences = file.read().split('>')\n",
    "    sequences = [seq for seq in sequences if seq.strip()]\n",
    "    unique_sequences = set(sequences)\n",
    "\n",
    "print(f\"Total sequences: {len(sequences)}\")\n",
    "print(f\"Unique sequences: {len(unique_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5Z1Ywo-mU9q"
   },
   "outputs": [],
   "source": [
    "def filter_sequences(input_file, output_file, max_length=100):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        write_sequence = False\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                if write_sequence:\n",
    "                    outfile.write(sequence_header + sequence_data)\n",
    "                sequence_header = line\n",
    "                sequence_data = ''\n",
    "                write_sequence = False  # Reset for next sequence\n",
    "            else:\n",
    "                sequence_data += line\n",
    "                if len(sequence_data.replace('\\n', '')) <= max_length:\n",
    "                    write_sequence = True\n",
    "                else:\n",
    "                    write_sequence = False\n",
    "\n",
    "        # Check last sequence\n",
    "        if write_sequence:\n",
    "            outfile.write(sequence_header + sequence_data)\n",
    "\n",
    "input_filename = 'AFP_clustered.fasta'\n",
    "output_filename = 'AFP_CD-hit.fasta'\n",
    "filter_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5qqxCNwm8BW"
   },
   "outputs": [],
   "source": [
    "def renumber_sequences(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        counter = 1\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                outfile.write(f'>Sequence_{counter}\\n')\n",
    "                counter += 1\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_filename = 'AFP_CD-hit.fasta'  \n",
    "output_filename = 'AFP_renumbered.fasta'  \n",
    "renumber_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBHmxnQzo6b6"
   },
   "source": [
    "## 阴性样本cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu3deDiwqtNb"
   },
   "outputs": [],
   "source": [
    "def convert_to_fasta(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        sequences = file.readlines()\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            sequence = sequence.strip()  \n",
    "            file.write(f\">Sequence_{i + 1}\\n{sequence}\\n\")\n",
    "\n",
    "convert_to_fasta('Non_AFP.txt', 'Non_AFP.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QlQDfafAo9Vf"
   },
   "outputs": [],
   "source": [
    "!cd-hit -i Non_AFP.fasta -o Non_AFP_clustered.fasta -c 0.8 -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDYq6rtxo_zs"
   },
   "outputs": [],
   "source": [
    "def filter_sequences(input_file, output_file, max_length=100):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        write_sequence = False\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                if write_sequence:\n",
    "                    outfile.write(sequence_header + sequence_data)\n",
    "                sequence_header = line\n",
    "                sequence_data = ''\n",
    "                write_sequence = False  # Reset for next sequence\n",
    "            else:\n",
    "                sequence_data += line\n",
    "                if len(sequence_data.replace('\\n', '')) <= max_length:\n",
    "                    write_sequence = True\n",
    "                else:\n",
    "                    write_sequence = False\n",
    "\n",
    "        # Check last sequence\n",
    "        if write_sequence:\n",
    "            outfile.write(sequence_header + sequence_data)\n",
    "\n",
    "input_filename = 'Non_AFP_clustered.fasta'\n",
    "output_filename = 'Non_AFP_CD-hit.fasta'\n",
    "filter_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPXwHb7no_2K"
   },
   "outputs": [],
   "source": [
    "def renumber_sequences(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        counter = 1\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                outfile.write(f'>Sequence_{counter}\\n')\n",
    "                counter += 1\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_filename = 'Non_AFP_CD-hit.fasta' \n",
    "output_filename = 'Non_AFP_renumbered.fasta' \n",
    "renumber_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Za8fI8fov8b"
   },
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5806,
     "status": "ok",
     "timestamp": 1753501578999,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "KI0I5tCstwMz",
    "outputId": "a4ac182d-fd29-4ce0-b3a5-266b77ac098f"
   },
   "outputs": [],
   "source": [
    "pip install Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRHQxHp6oyrJ"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_sequences(file_path, label):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        labels.append(label)  \n",
    "    return sequences, labels\n",
    "\n",
    "def balance_and_split(sequences, labels):\n",
    "    df = pd.DataFrame({\n",
    "        'sequence': sequences,\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "    print(f\"原始数据总量: {df.shape[0]}\")\n",
    "    print(f\"各类样本数量：\\n{df['label'].value_counts()}\")\n",
    "\n",
    "    # 进行欠采样\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = rus.fit_resample(df[['sequence']], df['label'])\n",
    "\n",
    "    print(f\"欠采样后数据总量: {X_res.shape[0]}\")\n",
    "    print(f\"欠采样后各类样本数量：\\n{pd.Series(y_res).value_counts()}\")\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"训练集数量: {X_train.shape[0]}\")\n",
    "    print(f\"测试集数量: {X_test.shape[0]}\")\n",
    "\n",
    "    train_df = pd.DataFrame(X_train, columns=['sequence'])\n",
    "    train_df['label'] = y_train\n",
    "    test_df = pd.DataFrame(X_test, columns=['sequence'])\n",
    "    test_df['label'] = y_test\n",
    "\n",
    "    train_df.to_csv('train_dataset.csv', index=False)\n",
    "    test_df.to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "pos_sequences, pos_labels = load_sequences('AFP_renumbered.fasta', 1)\n",
    "neg_sequences, neg_labels = load_sequences('Non_AFP_renumbered.fasta', 0)\n",
    "\n",
    "print(f\"阳性样本数量: {len(pos_sequences)}\")\n",
    "print(f\"阴性样本数量: {len(neg_sequences)}\")\n",
    "\n",
    "all_sequences = pos_sequences + neg_sequences\n",
    "all_labels = pos_labels + neg_labels\n",
    "\n",
    "train_df, test_df = balance_and_split(all_sequences, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz6lWj8Lx6a_"
   },
   "source": [
    "## 去除非20个标准氨基酸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1757042768681,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "oOlms3L9x-Ct",
    "outputId": "9fa8a027-9033-4f5c-f0ab-ee700f4a62a5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def replace_non_standard_amino_acids(seq):\n",
    "\n",
    "    replacements = {'B': 'D', 'Z': 'E', 'X': 'A', 'J': 'L', 'U': 'C', 'O': 'K'}\n",
    "    for old, new in replacements.items():\n",
    "        seq = seq.replace(old, new)\n",
    "    return seq\n",
    "\n",
    "def load_and_preprocess(file_path, output_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['sequence'] = df['sequence'].apply(replace_non_standard_amino_acids)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "train_df = load_and_preprocess('train_dataset.csv', 'processed_train_dataset.csv')\n",
    "test_df = load_and_preprocess('test_dataset.csv', 'processed_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUJ__KTOivXB"
   },
   "outputs": [],
   "source": [
    "def renumber_sequences(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        counter = 1\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                outfile.write(f'>Sequence_{counter}\\n')\n",
    "                counter += 1\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_filename = 'processed_test_dataset.csv'\n",
    "output_filename = 'ColabFold_test_dataset.csv'\n",
    "renumber_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LADKYcKABvi8"
   },
   "source": [
    "## 分别划分成训练集阳性，训练集阴性，测试集阳性，测试集阴性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taWjz7KIB0a2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def to_fasta(df, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for index, row in df.iterrows():\n",
    "            f.write(f\">{index}\\n{row['sequence']}\\n\")\n",
    "\n",
    "def process_and_save(data_path, output_prefix):\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    pos = df[df['label'] == 1]\n",
    "    neg = df[df['label'] == 0]\n",
    "\n",
    "    to_fasta(pos, f'{output_prefix}_pos.fasta')\n",
    "    to_fasta(neg, f'{output_prefix}_neg.fasta')\n",
    "\n",
    "train_path = 'processed_train_dataset.csv'\n",
    "test_path = 'processed_test_dataset.csv'\n",
    "\n",
    "process_and_save(train_path, 'Colab_train')\n",
    "process_and_save(test_path, 'Colab_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RnxO1AX2u46"
   },
   "source": [
    "# 结构信息\n",
    "*   加载pdb文件\n",
    "*   使用pdb文件提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQqE1ofqsviH"
   },
   "source": [
    "## 加载pdb文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1753502154056,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "-8QFQR61s4ED",
    "outputId": "c7f28949-bb85-4eb3-c6ed-8acfc36ba55d"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "zip_folder_path = '/content/drive/MyDrive/AFP_work/result/test_neg'\n",
    "extract_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'  # 解压后的路径\n",
    "\n",
    "os.makedirs(extract_folder_path, exist_ok=True)\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "for filename in os.listdir(zip_folder_path):\n",
    "    if filename.endswith('.zip'):\n",
    "        zip_path = os.path.join(zip_folder_path, filename)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for pdb_file in zip_ref.namelist():\n",
    "                if pdb_file.endswith('.pdb') and 'relaxed_rank_001' in pdb_file:\n",
    "                    zip_ref.extract(pdb_file, extract_folder_path)\n",
    "                    print(f'解压缩完成：{pdb_file} 从 {filename} 到 {extract_folder_path}')\n",
    "                    file_count += 1\n",
    "\n",
    "print(f'总共解压了 {file_count} 个文件')\n",
    "\n",
    "pdb_files = [f for f in os.listdir(extract_folder_path) if f.endswith('.pdb')]\n",
    "print(f'解压后的文件夹中共有 {len(pdb_files)} 个 PDB 文件')\n",
    "\n",
    "# 删除包含 \"unrelaxed\" 的 PDB 文件\n",
    "for pdb_file in pdb_files:\n",
    "    if 'unrelaxed' in pdb_file:\n",
    "        file_path = os.path.join(extract_folder_path, pdb_file)\n",
    "        os.remove(file_path)\n",
    "        print(f'已删除文件：{file_path}')\n",
    "\n",
    "remaining_pdb_files = [f for f in os.listdir(extract_folder_path) if f.endswith('.pdb')]\n",
    "print(f'删除 unrelaxed 文件后，文件夹中共有 {len(remaining_pdb_files)} 个 PDB 文件')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1754101167178,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "UWRjQiwMTlms",
    "outputId": "3465b49c-86cb-4934-bf02-afdb66fb122a"
   },
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "\n",
    "parser = PDBParser(QUIET=True)\n",
    "ppb = PPBuilder()\n",
    "\n",
    "def extract_sequence_from_pdb(pdb_path):\n",
    "    structure = parser.get_structure('', pdb_path)\n",
    "    sequence = \"\"\n",
    "    for pp in ppb.build_peptides(structure):\n",
    "        sequence += str(pp.get_sequence())\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1754101179783,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "kf2BXidF2290",
    "outputId": "69f64539-8b4d-48ef-fe47-65086479005b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# 四个文件夹路径\n",
    "train_pos_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "test_pos_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "train_neg_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_neg_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "\n",
    "train_pdb_count = len([f for f in os.listdir(train_pos_path) if f.endswith('.pdb')])\n",
    "test_pdb_count = len([f for f in os.listdir(test_pos_path) if f.endswith('.pdb')])\n",
    "train_neg_pdb_count = len([f for f in os.listdir(train_neg_path) if f.endswith('.pdb')])\n",
    "test_neg_pdb_count = len([f for f in os.listdir(test_neg_path) if f.endswith('.pdb')])\n",
    "\n",
    "print(f'训练集中的 PDB 文件数量（阳性）：{train_pdb_count}')\n",
    "print(f'测试集中的 PDB 文件数量（阳性）：{test_pdb_count}')\n",
    "print(f'训练集中的 PDB 文件数量（阴性）：{train_neg_pdb_count}')\n",
    "print(f'测试集中的 PDB 文件数量（阴性）：{test_neg_pdb_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0LT6AZYEBL3"
   },
   "source": [
    "## 使用pdb文件提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10505,
     "status": "error",
     "timestamp": 1754102535596,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "xaF1V0LzEElS",
    "outputId": "5e52bcad-f684-4966-d61b-cab31de5d693"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "output_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features'\n",
    "\n",
    "os.makedirs(output_train_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_train_neg_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_neg_folder_path, exist_ok=True)\n",
    "\n",
    "# 提取训练集和测试集中的所有 PDB 文件\n",
    "train_pos_pdb_files = [f for f in os.listdir(train_pos_folder_path) if f.endswith('.pdb')]\n",
    "train_neg_pdb_files = [f for f in os.listdir(train_neg_folder_path) if f.endswith('.pdb')]\n",
    "test_pos_pdb_files = [f for f in os.listdir(test_pos_folder_path) if f.endswith('.pdb')]\n",
    "test_neg_pdb_files = [f for f in os.listdir(test_neg_folder_path) if f.endswith('.pdb')]\n",
    "\n",
    "# 提取选中的 PDB 文件的结构信息\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 优化 PDB 文件处理函数\n",
    "def process_pdb_file(pdb_path):\n",
    "    structure = parser.get_structure('', pdb_path)\n",
    "    residues = [residue for residue in structure.get_residues() if 'CA' in residue]\n",
    "    num_residues = len(residues)\n",
    "\n",
    "    # 提取位置特征、方向特征和旋转特征\n",
    "    positions = np.array([residue['CA'].get_coord() for residue in residues], dtype=np.float64)\n",
    "    edges = []\n",
    "    directions = []\n",
    "    rotations = []\n",
    "\n",
    "    # 计算接触图和附加特征\n",
    "    for i in range(num_residues):\n",
    "        for j in range(i + 1, num_residues):\n",
    "            distance = np.linalg.norm(positions[i] - positions[j])\n",
    "            if distance < 10.0:  # 阈值为10Å来定义接触\n",
    "                edges.append([i, j])\n",
    "                direction = positions[j] - positions[i]\n",
    "                norm = np.linalg.norm(direction)\n",
    "                if norm != 0:\n",
    "                    directions.append(direction / norm)\n",
    "                    rotations.append(float(np.arctan2(direction[1], direction[0])))\n",
    "\n",
    "    return positions, edges, directions, rotations\n",
    "\n",
    "# 处理训练集中的 PDB 文件并添加标签\n",
    "print(f'训练集中的 PDB 文件数量 (阳性): {len(train_pos_pdb_files)}')\n",
    "print(f'训练集中的 PDB 文件数量 (阴性): {len(train_neg_pdb_files)}')\n",
    "\n",
    "for pdb_file in train_pos_pdb_files:\n",
    "    pdb_path = os.path.join(train_pos_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 1  # 阳性样本标签\n",
    "    }\n",
    "    # 保存特征到文件\n",
    "    output_file_path = os.path.join(output_train_pos_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json') if features['label'] == 1 else os.path.join(output_train_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "for pdb_file in train_neg_pdb_files:\n",
    "    pdb_path = os.path.join(train_neg_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 0  # 阴性样本标签\n",
    "    }\n",
    "    # 保存特征到文件\n",
    "    output_file_path = os.path.join(output_train_neg_folder_path if features['label'] == 1 else output_test_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "# 处理测试集中的 PDB 文件并添加标签\n",
    "print(f'测试集中的 PDB 文件数量 (阳性): {len(test_pos_pdb_files)}')\n",
    "print(f'测试集中的 PDB 文件数量 (阴性): {len(test_neg_pdb_files)}')\n",
    "\n",
    "for pdb_file in test_pos_pdb_files:\n",
    "    pdb_path = os.path.join(test_pos_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 1  # 阳性样本标签\n",
    "    }\n",
    "    # 保存特征到文件\n",
    "    output_file_path = os.path.join(output_test_pos_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json') if features['label'] == 1 else os.path.join(output_test_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "for pdb_file in test_neg_pdb_files:\n",
    "    pdb_path = os.path.join(test_neg_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 0  # 阴性样本标签\n",
    "    }\n",
    "    # 保存特征到文件\n",
    "    output_file_path = os.path.join(output_test_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "# 输出保存的 JSON 文件数量\n",
    "# json_files = [f for f in os.listdir(output_folder_path) if f.endswith('.json')]\n",
    "# print(f'保存的 JSON 文件数量: {len(json_files)}')\n",
    "\n",
    "# 检查每个保存的 JSON 文件中的维度信息\n",
    "# for json_file in json_files:\n",
    "#     json_path = os.path.join(output_folder_path, json_file)\n",
    "#     with open(json_path, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "#         print(f'文件: {json_file}')\n",
    "#         print(f'节点特征数量: {len(data[\"node_features\"])}, 位置特征维度: {len(data[\"node_features\"][0])}')\n",
    "#         print(f'边特征数量: {len(data[\"edge_features\"][\"edges\"])}, 方向特征数量: {len(data[\"edge_features\"][\"directions\"])}, 旋转特征数量: {len(data[\"edge_features\"][\"rotations\"])}')\n",
    "\n",
    "\n",
    "output_train_neg_folder_path\n",
    "\n",
    "# 输出保存的 JSON 文件数量\n",
    "json_files = [f for f in os.listdir(output_train_neg_folder_path) if f.endswith('.json')]\n",
    "print(f'output_train_neg_folder_path 保存的 JSON 文件数量: {len(json_files)}')\n",
    "\n",
    "# 检查每个保存的 JSON 文件中的维度信息\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(output_train_neg_folder_path, json_file)\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        print(f'文件: {json_file}')\n",
    "        print(f'节点特征数量: {len(data[\"node_features\"])}, 位置特征维度: {len(data[\"node_features\"][0])}')\n",
    "        print(f'边特征数量: {len(data[\"edge_features\"][\"edges\"])}, 方向特征数量: {len(data[\"edge_features\"][\"directions\"])}, 旋转特征数量: {len(data[\"edge_features\"][\"rotations\"])}')\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"总处理时间: {end_time - start_time:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2bQyS6_wILJ"
   },
   "source": [
    "# ESM-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9004,
     "status": "ok",
     "timestamp": 1754191779238,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "qmCgs2eb6Mjq",
    "outputId": "be6b644f-bcca-4899-8578-9c1e2d07d56e"
   },
   "outputs": [],
   "source": [
    "pip install esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WEZikzFcP-h",
    "outputId": "850ecb7f-dde9-435a-f9f4-7c06afe9c5e3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import SeqIO\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 定义 FASTA 文件路径\n",
    "#train_pos_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_train_pos.fasta'\n",
    "#train_neg_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_train_neg.fasta'\n",
    "test_pos_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_test_pos.fasta'\n",
    "#test_neg_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_test_neg.fasta'\n",
    "\n",
    "output_feature_path = '/content/drive/MyDrive/esmc_600_test_pos'\n",
    "\n",
    "os.makedirs(output_feature_path, exist_ok=True)\n",
    "\n",
    "def read_fasta_sequences(fasta_file):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        seq_id = record.id\n",
    "        sequence = str(record.seq).replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        sequences.append((seq_id, sequence))\n",
    "    return sequences\n",
    "\n",
    "def save_features(features, output_dir):\n",
    "    for seq_id, feature_dict in features.items():\n",
    "        logits = feature_dict['logits']\n",
    "        embeddings = feature_dict['embeddings']\n",
    "\n",
    "        # 定义文件路径\n",
    "        logits_path = os.path.join(output_dir, f\"{seq_id}_logits.npy\")\n",
    "        embeddings_path = os.path.join(output_dir, f\"{seq_id}_embeddings.npy\")\n",
    "\n",
    "        # 保存 logits 和 embeddings\n",
    "        np.save(logits_path, logits)\n",
    "        np.save(embeddings_path, embeddings)\n",
    "\n",
    "def extract_features_individual(client, sequences):\n",
    "    features = {}\n",
    "    for seq_id, seq in tqdm(sequences, desc=\"提取特征\"):\n",
    "        try:\n",
    "            # 创建 ESMProtein 实例\n",
    "            protein = ESMProtein(sequence=seq)\n",
    "\n",
    "            # 编码蛋白质序列\n",
    "            protein_tensor = client.encode(protein)\n",
    "\n",
    "            # 获取 logits 和 embeddings\n",
    "            logits_output = client.logits(\n",
    "                protein_tensor,\n",
    "                LogitsConfig(sequence=True, return_embeddings=True)\n",
    "            )\n",
    "\n",
    "            logits = logits_output.logits  # 需要根据实际情况修改\n",
    "            embeddings = logits_output.embeddings  # 需要根据实际情况修改\n",
    "\n",
    "            # 检查 logits 和 embeddings 的类型\n",
    "            if isinstance(logits, torch.Tensor):\n",
    "                logits = logits.cpu().numpy()\n",
    "            elif isinstance(logits, np.ndarray):\n",
    "                pass  # 已经是 NumPy 数组\n",
    "            else:\n",
    "                # 如果不是 tensor 或 ndarray，尝试其他转换\n",
    "                logits = np.array(logits)\n",
    "\n",
    "            if isinstance(embeddings, torch.Tensor):\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "            elif isinstance(embeddings, np.ndarray):\n",
    "                pass  # 已经是 NumPy 数组\n",
    "            else:\n",
    "                # 如果不是 tensor 或 ndarray，尝试其他转换\n",
    "                embeddings = np.array(embeddings)\n",
    "\n",
    "            # 保存特征\n",
    "            features[seq_id] = {\n",
    "                'logits': logits,\n",
    "                'embeddings': embeddings\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"处理序列 {seq_id} 时出错: {e}\")\n",
    "            continue\n",
    "    return features\n",
    "\n",
    "# 初始化 ESMC 客户端\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "client = ESMC.from_pretrained(\"esmc_600m\").to(device)\n",
    "client.eval()  # 设置为评估模式\n",
    "\n",
    "fasta_files = [\n",
    "    #train_pos_seq_file,\n",
    "    #train_neg_seq_file,\n",
    "    test_pos_seq_file,\n",
    "    #test_neg_seq_file\n",
    "]\n",
    "\n",
    "for fasta_file in fasta_files:\n",
    "    print(f\"正在处理文件: {fasta_file}\")\n",
    "\n",
    "    # 读取序列\n",
    "    sequences = read_fasta_sequences(fasta_file)\n",
    "    print(f\"序列数量: {len(sequences)}\")\n",
    "\n",
    "    # 提取特征（逐条处理）\n",
    "    features = extract_features_individual(client, sequences)\n",
    "\n",
    "    # 保存特征\n",
    "    save_features(features, output_feature_path)\n",
    "\n",
    "    print(f\"特征已保存: {fasta_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW7S96HwO3Vg"
   },
   "source": [
    "# 处理序列结构信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrR3SQWpO5qt"
   },
   "source": [
    "## 处理序列信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16799,
     "status": "ok",
     "timestamp": 1754199247054,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "i_J1oT45O1XL",
    "outputId": "f49d5193-029a-4e64-9aed-6ffe5dfa2eab"
   },
   "outputs": [],
   "source": [
    "# 2. 导入必要的库\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 3. 定义特征输出文件夹列表\n",
    "feature_dirs = [\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',  # 请确认此路径是否正确\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "    # 如果有其他文件夹，如 esmc_600_test_pos，请在此添加\n",
    "]\n",
    "\n",
    "# 4. 定义函数以查看汇总文件的维度\n",
    "def inspect_combined_files(feature_dirs):\n",
    "    \"\"\"\n",
    "    遍历每个特征文件夹，加载并打印 combined_logits.npy 和 combined_embeddings.npy 的维度。\n",
    "\n",
    "    参数:\n",
    "        feature_dirs (List[str]): 特征文件夹路径列表。\n",
    "    \"\"\"\n",
    "    for feature_dir in feature_dirs:\n",
    "        print(f\"正在处理文件夹: {feature_dir}\")\n",
    "\n",
    "        # 检查文件夹是否存在\n",
    "        if not os.path.isdir(feature_dir):\n",
    "            print(f\"文件夹 '{feature_dir}' 不存在，跳过。\\n\")\n",
    "            continue\n",
    "\n",
    "        # 定义 combined_logits.npy 和 combined_embeddings.npy 的路径\n",
    "        logits_path = os.path.join(feature_dir, 'combined_logits.npy')\n",
    "        embeddings_path = os.path.join(feature_dir, 'combined_embeddings.npy')\n",
    "\n",
    "        # 检查 combined_logits.npy 是否存在\n",
    "        if os.path.isfile(logits_path):\n",
    "            try:\n",
    "                combined_logits = np.load(logits_path, allow_pickle=True)\n",
    "                print(f\" - {os.path.basename(logits_path)} 的形状: {combined_logits.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\" - 加载 {os.path.basename(logits_path)} 时出错: {e}\")\n",
    "        else:\n",
    "            print(f\" - {os.path.basename(logits_path)} 不存在。\")\n",
    "\n",
    "        # 检查 combined_embeddings.npy 是否存在\n",
    "        if os.path.isfile(embeddings_path):\n",
    "            try:\n",
    "                combined_embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "                print(f\" - {os.path.basename(embeddings_path)} 的形状: {combined_embeddings.shape}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\" - 加载 {os.path.basename(embeddings_path)} 时出错: {e}\\n\")\n",
    "        else:\n",
    "            print(f\" - {os.path.basename(embeddings_path)} 不存在。\\n\")\n",
    "\n",
    "        # mxlin添加\n",
    "        sample = combined_embeddings[0]  # 取第一个样本\n",
    "        print(\"数据级别：\",sample.shape)  # 输出 (1152,)，则是序列级别\n",
    "\n",
    "# 5. 运行函数以查看维度\n",
    "inspect_combined_files(feature_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1754199247189,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "uSFwZslPPyip",
    "outputId": "e3407317-896e-43bf-f9f8-be1b19590f6c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "feature_dirs = [\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "]\n",
    "\n",
    "# 统计每个特征文件夹中 logits 和 embeddings 文件的数量，并验证是否匹配。\n",
    "def count_logit_embedding_files(feature_dirs):\n",
    "    for feature_dir in feature_dirs:\n",
    "\n",
    "        if not os.path.isdir(feature_dir):\n",
    "            continue\n",
    "\n",
    "        logits_files = sorted(glob.glob(os.path.join(feature_dir, '*_logits.npy')))\n",
    "        embeddings_files = sorted(glob.glob(os.path.join(feature_dir, '*_embeddings.npy')))\n",
    "\n",
    "        num_logits = len(logits_files)\n",
    "        num_embeddings = len(embeddings_files)\n",
    "\n",
    "        if num_logits == num_embeddings:\n",
    "            print(f\"数量一致。\\n\")\n",
    "        else:\n",
    "            print(f\"数量不一致。\")\n",
    "            print(f\"- logits 文件数量: {num_logits}\")\n",
    "            print(f\"- embeddings 文件数量: {num_embeddings}\")\n",
    "\n",
    "            # 找出缺失或多余的文件\n",
    "            logits_indices = set([os.path.basename(f).split('_')[0] for f in logits_files])\n",
    "            embeddings_indices = set([os.path.basename(f).split('_')[0] for f in embeddings_files])\n",
    "\n",
    "            missing_in_embeddings = logits_indices - embeddings_indices\n",
    "            missing_in_logits = embeddings_indices - logits_indices\n",
    "\n",
    "            if missing_in_embeddings:\n",
    "                print(f\"在 embeddings 文件夹中缺失: {sorted(missing_in_embeddings)}\")\n",
    "            if missing_in_logits:\n",
    "                print(f\"在 logits 文件夹中缺失: {sorted(missing_in_logits)}\")\n",
    "            print()\n",
    "\n",
    "count_logit_embedding_files(feature_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1270,
     "status": "ok",
     "timestamp": 1754199254991,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "3GLMRuQjP5vC",
    "outputId": "694847e4-d1e1-4865-ac25-22d488c4466a"
   },
   "outputs": [],
   "source": [
    "# 遍历每个特征文件夹，加载并打印 combined_logits.npy 和 combined_embeddings.npy 的维度\n",
    "def inspect_combined_files(feature_dirs):\n",
    "    \"\"\"\n",
    "    遍历每个特征文件夹，加载并打印 combined_logits.npy 和 combined_embeddings.npy 的维度。\n",
    "\n",
    "    参数:\n",
    "        feature_dirs (List[str]): 特征文件夹路径列表。\n",
    "    \"\"\"\n",
    "    for feature_dir in feature_dirs:\n",
    "        print(f\"正在检查文件夹: {feature_dir}\")\n",
    "\n",
    "        # 定义 combined_logits.npy 和 combined_embeddings.npy 的路径\n",
    "        combined_logits_path = os.path.join(feature_dir, 'combined_logits.npy')\n",
    "        combined_embeddings_path = os.path.join(feature_dir, 'combined_embeddings.npy')\n",
    "\n",
    "        # 检查并加载 combined_logits.npy\n",
    "        if os.path.isfile(combined_logits_path):\n",
    "            try:\n",
    "                combined_logits = np.load(combined_logits_path, allow_pickle=True)\n",
    "                print(f\"  {os.path.basename(combined_logits_path)} 的形状: {combined_logits.shape}\")\n",
    "                print(f\"  {os.path.basename(combined_logits_path)} 的前5行数据：\\n{combined_logits[:5]}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"  加载 {os.path.basename(combined_logits_path)} 时出错: {e}\")\n",
    "        else:\n",
    "            print(f\" {os.path.basename(combined_logits_path)} 不存在。\")\n",
    "\n",
    "        # 检查并加载 combined_embeddings.npy\n",
    "        if os.path.isfile(combined_embeddings_path):\n",
    "            try:\n",
    "                combined_embeddings = np.load(combined_embeddings_path, allow_pickle=True)\n",
    "                print(f\" {feature_dir} {os.path.basename(combined_embeddings_path)} 的形状: {combined_embeddings.shape}\")\n",
    "                print(f\" {feature_dir} {os.path.basename(combined_embeddings_path)} 的前5行数据：\\n{combined_embeddings[:5]}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"加载 {os.path.basename(combined_embeddings_path)} 时出错: {e}\\n\")\n",
    "        else:\n",
    "            print(f\"{os.path.basename(combined_embeddings_path)} 不存在。\\n\")\n",
    "            \n",
    "inspect_combined_files(feature_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pnBCTCqhdFH"
   },
   "source": [
    "## 处理结构信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1754200816480,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "HVvzEYr8hfQt",
    "outputId": "963779f9-1fe3-415b-805a-4dfa9b9c6033"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# 定义目标文件夹路径\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "\n",
    "# 将文件夹路径存储在一个字典中，便于遍历\n",
    "folder_paths = {\n",
    "    'train_pos': train_pos_folder_path,\n",
    "    'train_neg': train_neg_folder_path,\n",
    "    'test_pos': test_pos_folder_path,\n",
    "    'test_neg': test_neg_folder_path\n",
    "}\n",
    "\n",
    "# 初始化一个空列表，用于存储统计结果\n",
    "stats = []\n",
    "\n",
    "# 遍历每个文件夹，统计个数\n",
    "for folder_name, folder_path in folder_paths.items():\n",
    "    print(f\"正在处理文件夹: {folder_path}\")\n",
    "\n",
    "    # 检查文件夹是否存在\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\" 文件夹 '{folder_path}' 不存在。请检查路径是否正确。\\n\")\n",
    "        stats.append({\n",
    "            'folder': folder_name,\n",
    "            'path': folder_path,\n",
    "            'pdb_file_count': '文件夹不存在'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # 使用 glob 查找所有 .pdb 文件（不区分大小写）\n",
    "    pdb_files = glob.glob(os.path.join(folder_path, '*.pdb')) + glob.glob(os.path.join(folder_path, '*.PDB'))\n",
    "\n",
    "    # 统计 .pdb 文件的数量\n",
    "    pdb_count = len(pdb_files)\n",
    "\n",
    "    print(f\" 文件夹 '{folder_name}' 中共有 {pdb_count} 个 .pdb 文件。\\n\")\n",
    "\n",
    "    # 将统计结果添加到列表中\n",
    "    stats.append({\n",
    "        'folder': folder_name,\n",
    "        'path': folder_path,\n",
    "        'pdb_file_count': pdb_count\n",
    "    })\n",
    "\n",
    "# 创建 DataFrame\n",
    "df_stats = pd.DataFrame(stats)\n",
    "\n",
    "# 显示统计结果\n",
    "print(\" 各文件夹中 .pdb 文件的数量统计：\")\n",
    "print(df_stats)\n",
    "\n",
    "# 保存统计结果为 CSV 文件\n",
    "csv_output_path = '/content/drive/MyDrive/AFP_work/pdb/pdb_file_counts.csv'\n",
    "df_stats.to_csv(csv_output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n统计结果已保存到 '{csv_output_path}'。\")\n",
    "\n",
    "# 读取并显示保存的 CSV 文件内容\n",
    "df_loaded_stats = pd.read_csv(csv_output_path)\n",
    "print(\"\\n统计结果 CSV 文件内容：\")\n",
    "print(df_loaded_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNGVugPJmcfN",
    "outputId": "16c0fc58-6fa9-4739-d59e-aa4deb9227b1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# 定义目标文件夹路径\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "\n",
    "# 定义输出 CSV 文件的保存路径\n",
    "output_csv_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos_pdb_filenames.csv'\n",
    "\n",
    "# 检查文件夹是否存在\n",
    "if not os.path.isdir(train_pos_folder_path):\n",
    "    print(f\" 文件夹 '{train_pos_folder_path}' 不存在。请检查路径是否正确。\")\n",
    "else:\n",
    "    # 使用 glob 查找所有 .pdb 文件（不区分大小写）\n",
    "    pdb_files_lower = glob.glob(os.path.join(train_pos_folder_path, '*.pdb'))\n",
    "    pdb_files_upper = glob.glob(os.path.join(train_pos_folder_path, '*.PDB'))\n",
    "    pdb_files = pdb_files_lower + pdb_files_upper\n",
    "\n",
    "    # 提取文件名\n",
    "    pdb_filenames = [os.path.basename(f) for f in pdb_files]\n",
    "\n",
    "    # 检查是否找到任何 .pdb 文件\n",
    "    if not pdb_filenames:\n",
    "        print(f\" 在文件夹 '{train_pos_folder_path}' 中未找到任何 .pdb 文件。\")\n",
    "    else:\n",
    "        # 创建一个 DataFrame\n",
    "        df = pd.DataFrame({'filename': pdb_filenames})\n",
    "\n",
    "        # 保存为 CSV 文件\n",
    "        try:\n",
    "            df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\" 所有 .pdb 文件名已成功保存到 '{output_csv_path}'。\")\n",
    "            print(f\"总共保存了 {len(pdb_filenames)} 个文件名。\")\n",
    "        except Exception as e:\n",
    "            print(f\" 保存 CSV 文件时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFQPsDFXnMF6",
    "outputId": "b30531eb-18e9-426f-97bd-4d92bd3b0ba3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def save_extracted_numbers_to_csv(folder_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    从指定文件夹中提取 .pdb 文件名中 '_relaxed_rank_001' 之前的数字，并保存到 CSV 文件中。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): 目标文件夹路径。\n",
    "        output_csv_path (str): 输出 CSV 文件的路径。\n",
    "    \"\"\"\n",
    "    # 检查文件夹是否存在\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\" 文件夹 '{folder_path}' 不存在。请检查路径是否正确。\")\n",
    "        return\n",
    "\n",
    "    # 使用 glob 查找所有 .pdb 文件（不区分大小写）\n",
    "    pdb_files_lower = glob.glob(os.path.join(folder_path, '*.pdb'))\n",
    "    pdb_files_upper = glob.glob(os.path.join(folder_path, '*.PDB'))\n",
    "    pdb_files = pdb_files_lower + pdb_files_upper\n",
    "\n",
    "    # 提取文件名\n",
    "    pdb_filenames = [os.path.basename(f) for f in pdb_files]\n",
    "\n",
    "    # 定义正则表达式模式\n",
    "    # 文件名格式：数字_relaxed_rank_001_其他信息.pdb，例如：2033_relaxed_rank_001_alphafold2_ptm_model_4_seed_000.pdb\n",
    "    pattern = re.compile(r'^(\\d+)_relaxed_rank_001.*\\.pdb$', re.IGNORECASE)\n",
    "\n",
    "    # 初始化列表存储提取的数字\n",
    "    extracted_numbers = []\n",
    "\n",
    "    # 遍历文件名并提取数字\n",
    "    for filename in pdb_filenames:\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            extracted_numbers.append(int(number))  # 转换为整数类型\n",
    "        else:\n",
    "            print(f\" 文件名不符合预期模式，无法提取数字: {filename}\")\n",
    "\n",
    "    # 检查是否提取到了任何数字\n",
    "    if not extracted_numbers:\n",
    "        print(f\" 在文件夹 '{folder_path}' 中未找到符合模式的 .pdb 文件。\")\n",
    "    else:\n",
    "        # 创建一个 DataFrame\n",
    "        df = pd.DataFrame({'extracted_number': extracted_numbers})\n",
    "\n",
    "        # 保存为 CSV 文件\n",
    "        try:\n",
    "            df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"提取的数字已成功保存到 '{output_csv_path}'。\")\n",
    "            print(f\"总共保存了 {len(extracted_numbers)} 个数字。\")\n",
    "        except Exception as e:\n",
    "            print(f\"保存 CSV 文件时出错: {e}\")\n",
    "\n",
    "# 定义目标文件夹路径\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "\n",
    "# 定义输出 CSV 文件的保存路径\n",
    "output_csv_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos_pdb_filenames_extracted.csv'\n",
    "\n",
    "# 调用函数\n",
    "save_extracted_numbers_to_csv(train_pos_folder_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "-zMy7WkUuhOK",
    "outputId": "373a9827-4b39-4f85-bbfd-6996a1312446"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "\n",
    "# 训练集和测试集的 PDB 文件夹路径\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "\n",
    "# 输出文件夹路径\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "# 创建输出文件夹（如果不存在）\n",
    "os.makedirs(output_train_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_train_neg_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_neg_folder_path, exist_ok=True)\n",
    "\n",
    "# 提取训练集和测试集中的所有 PDB 文件\n",
    "train_pos_pdb_files = [f for f in os.listdir(train_pos_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "train_neg_pdb_files = [f for f in os.listdir(train_neg_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "test_pos_pdb_files = [f for f in os.listdir(test_pos_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "test_neg_pdb_files = [f for f in os.listdir(test_neg_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "\n",
    "# 初始化 PDBParser\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 优化 PDB 文件处理函数\n",
    "def process_pdb_file(pdb_path):\n",
    "    try:\n",
    "        structure = parser.get_structure('', pdb_path)\n",
    "        residues = [residue for residue in structure.get_residues() if 'CA' in residue]\n",
    "        num_residues = len(residues)\n",
    "\n",
    "        if num_residues == 0:\n",
    "            print(f\" 文件 '{pdb_path}' 中没有找到 CA 原子。\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        # 提取位置特征、方向特征和旋转特征\n",
    "        positions = np.array([residue['CA'].get_coord() for residue in residues], dtype=np.float64)\n",
    "        edges = []\n",
    "        directions = []\n",
    "        rotations = []\n",
    "\n",
    "        # 计算接触图和附加特征\n",
    "        for i in range(num_residues):\n",
    "            for j in range(i + 1, num_residues):\n",
    "                distance = np.linalg.norm(positions[i] - positions[j])\n",
    "                if distance < 10.0:  # 阈值为10Å来定义接触\n",
    "                    edges.append([i, j])\n",
    "                    direction = positions[j] - positions[i]\n",
    "                    norm = np.linalg.norm(direction)\n",
    "                    if norm != 0:\n",
    "                        directions.append(direction / norm)\n",
    "                        rotations.append(float(np.arctan2(direction[1], direction[0])))\n",
    "\n",
    "        return positions, edges, directions, rotations\n",
    "    except Exception as e:\n",
    "        print(f\" 处理文件 '{pdb_path}' 时出错: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# 处理 PDB 文件并保存特征\n",
    "def process_and_save(pdb_files, folder_path, output_folder_path, label):\n",
    "    print(f'处理文件夹: {folder_path}')\n",
    "    print(f'文件数量: {len(pdb_files)}')\n",
    "\n",
    "    processed_count = 0\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_path = os.path.join(folder_path, pdb_file)\n",
    "        positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "\n",
    "        if positions is None:\n",
    "            continue  # 跳过处理出错的文件\n",
    "\n",
    "        features = {\n",
    "            \"node_features\": positions.tolist(),\n",
    "            \"edge_features\": {\n",
    "                \"edges\": edges,\n",
    "                \"directions\": [d.tolist() for d in directions],\n",
    "                \"rotations\": [float(rot) for rot in rotations]\n",
    "            },\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "        # 定义输出文件路径\n",
    "        output_file_path = os.path.join(output_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "\n",
    "        # 保存特征到文件\n",
    "        try:\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                json.dump(features, output_file)\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"保存文件 '{output_file_path}' 时出错: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 每处理100个文件，打印一次进度\n",
    "        if processed_count % 100 == 0:\n",
    "            print(f'已处理 {processed_count} 个文件。')\n",
    "\n",
    "    print(f'完成处理 {processed_count} 个文件。')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 处理训练集中的阳性 PDB 文件\n",
    "process_and_save(train_pos_pdb_files, train_pos_folder_path, output_train_pos_folder_path, label=1)\n",
    "# 处理训练集中的阴性 PDB 文件\n",
    "process_and_save(train_neg_pdb_files, train_neg_folder_path, output_train_neg_folder_path, label=0)\n",
    "# 处理测试集中的阳性 PDB 文件\n",
    "process_and_save(test_pos_pdb_files, test_pos_folder_path, output_test_pos_folder_path, label=1)\n",
    "# 处理测试集中的阴性 PDB 文件\n",
    "process_and_save(test_neg_pdb_files, test_neg_folder_path, output_test_neg_folder_path, label=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"总处理时间: {end_time - start_time:.2f} 秒\")\n",
    "\n",
    "\n",
    "\n",
    "# 输出保存的 JSON 文件数量\n",
    "def count_json_files(output_folder_path):\n",
    "    json_files = [f for f in os.listdir(output_folder_path) if f.endswith('.json')]\n",
    "    print(f'文件夹 \"{output_folder_path}\" 中保存的 JSON 文件数量: {len(json_files)}')\n",
    "    return json_files\n",
    "\n",
    "print(\"\\n保存的 JSON 文件数量:\")\n",
    "count_json_files(output_train_pos_folder_path)\n",
    "count_json_files(output_train_neg_folder_path)\n",
    "count_json_files(output_test_pos_folder_path)\n",
    "count_json_files(output_test_neg_folder_path)\n",
    "\n",
    "# 检查每个保存的 JSON 文件中的维度信息\n",
    "def check_json_dimensions(output_folder_path):\n",
    "    json_files = [f for f in os.listdir(output_folder_path) if f.endswith('.json')]\n",
    "    for json_file in json_files[:5]:  # 仅检查前5个文件\n",
    "        json_path = os.path.join(output_folder_path, json_file)\n",
    "        with open(json_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            node_features = data.get(\"node_features\", [])\n",
    "            edge_features = data.get(\"edge_features\", {})\n",
    "            edges = edge_features.get(\"edges\", [])\n",
    "            directions = edge_features.get(\"directions\", [])\n",
    "            rotations = edge_features.get(\"rotations\", [])\n",
    "            print(f'文件: {json_file}')\n",
    "            print(f'节点特征数量: {len(node_features)}, 位置特征维度: {len(node_features[0]) if node_features else 0}')\n",
    "            print(f'边特征数量: {len(edges)}, 方向特征数量: {len(directions)}, 旋转特征数量: {len(rotations)}\\n')\n",
    "\n",
    "print(\"\\n检查部分 JSON 文件的维度信息:\")\n",
    "check_json_dimensions(output_train_pos_folder_path)\n",
    "check_json_dimensions(output_train_neg_folder_path)\n",
    "check_json_dimensions(output_test_pos_folder_path)\n",
    "check_json_dimensions(output_test_neg_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maxpswcORjOn",
    "outputId": "411069fe-9c71-4fbf-857c-ad3961235a3e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# 定义输出文件夹路径\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "# 将文件夹路径存储在一个字典中，便于遍历\n",
    "folders = {\n",
    "    'train_pos': output_train_pos_folder_path,\n",
    "    'train_neg': output_train_neg_folder_path,\n",
    "    'test_pos': output_test_pos_folder_path,\n",
    "    'test_neg': output_test_neg_folder_path\n",
    "}\n",
    "\n",
    "# 遍历每个文件夹并统计 JSON 文件数量\n",
    "for folder_name, folder_path in folders.items():\n",
    "    if os.path.isdir(folder_path):\n",
    "        # 使用 glob 查找所有 .json 文件（不区分大小写）\n",
    "        json_files = glob.glob(os.path.join(folder_path, '*.json')) + glob.glob(os.path.join(folder_path, '*.JSON'))\n",
    "        count = len(json_files)\n",
    "        print(f\"文件夹 '{folder_name}' 中的 JSON 文件数量: {count}\")\n",
    "    else:\n",
    "        print(f\" 文件夹 '{folder_name}' 不存在。请检查路径是否正确。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2hWiwqzc-V-1",
    "outputId": "e321fe47-78cd-44db-b20f-2ea35524cde6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(filename='/content/drive/MyDrive/AFP_work/pdb_features/processing.log',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# 定义输出文件夹路径\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "output_folders = {\n",
    "    'train_pos': output_train_pos_folder_path,\n",
    "    'train_neg': output_train_neg_folder_path,\n",
    "    'test_pos': output_test_pos_folder_path,\n",
    "    'test_neg': output_test_neg_folder_path\n",
    "}\n",
    "\n",
    "# 创建输出文件夹（如果不存在）\n",
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 定义函数加载单个 JSON 文件\n",
    "def load_single_json(json_file):\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            sample = json.load(f)\n",
    "        logging.info(f\"成功加载文件: {json_file}\")\n",
    "        return sample\n",
    "    except Exception as e:\n",
    "        logging.error(f\"加载文件 '{json_file}' 时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "# 定义函数并行加载 JSON 文件\n",
    "def load_json_files_parallel(folder_path, max_workers=8):\n",
    "    \"\"\"\n",
    "    并行加载指定文件夹中的所有 JSON 文件。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): JSON 文件所在的文件夹路径。\n",
    "        max_workers (int): 并行工作的最大线程数。\n",
    "\n",
    "    返回:\n",
    "        list: 包含所有成功加载的样本数据的列表。\n",
    "    \"\"\"\n",
    "    json_files = glob.glob(os.path.join(folder_path, '*.json'))\n",
    "    data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(load_single_json, f): f for f in json_files}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=f'Loading {os.path.basename(folder_path)}'):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                data.append(result)\n",
    "    return data\n",
    "\n",
    "# 初始化训练集和测试集\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# 加载训练集数据\n",
    "train_pos_data = load_json_files_parallel(output_folders['train_pos'])\n",
    "train_neg_data = load_json_files_parallel(output_folders['train_neg'])\n",
    "train_data = train_pos_data + train_neg_data\n",
    "\n",
    "# 加载测试集数据\n",
    "test_pos_data = load_json_files_parallel(output_folders['test_pos'])\n",
    "test_neg_data = load_json_files_parallel(output_folders['test_neg'])\n",
    "test_data = test_pos_data + test_neg_data\n",
    "\n",
    "print(f\"训练集总样本数: {len(train_data)}\")\n",
    "print(f\"测试集总样本数: {len(test_data)}\")\n",
    "\n",
    "# 定义输出汇总文件的路径\n",
    "aggregated_output_folder = '/content/drive/MyDrive/AFP_work/pdb_features/aggregated'\n",
    "os.makedirs(aggregated_output_folder, exist_ok=True)\n",
    "\n",
    "train_output_path = os.path.join(aggregated_output_folder, 'train_dataset.json')\n",
    "test_output_path = os.path.join(aggregated_output_folder, 'test_dataset.json')\n",
    "\n",
    "# 保存训练集\n",
    "try:\n",
    "    with open(train_output_path, 'w') as f:\n",
    "        json.dump(train_data, f)\n",
    "    print(f\"训练集已保存到 '{train_output_path}'。\")\n",
    "except Exception as e:\n",
    "    print(f\"保存训练集时出错: {e}\")\n",
    "\n",
    "# 保存测试集\n",
    "try:\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_data, f)\n",
    "    print(f\"测试集已保存到 '{test_output_path}'。\")\n",
    "except Exception as e:\n",
    "    print(f\"保存测试集时出错: {e}\")\n",
    "\n",
    "# 验证汇总结果\n",
    "def load_aggregated_data(file_path):\n",
    "    \"\"\"\n",
    "    从指定的 JSON 文件中加载汇总数据。\n",
    "\n",
    "    参数:\n",
    "        file_path (str): 汇总数据的 JSON 文件路径。\n",
    "\n",
    "    返回:\n",
    "        list: 包含所有样本数据的列表。\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\" 文件 '{file_path}' 不存在。\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"成功加载 '{file_path}'，样本数: {len(data)}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\" 加载文件 '{file_path}' 时出错: {e}\")\n",
    "        return []\n",
    "\n",
    "# 加载并查看训练集\n",
    "train_dataset = load_aggregated_data(train_output_path)\n",
    "if train_dataset:\n",
    "    print(f\"训练集第一个样本内容:\")\n",
    "    print(json.dumps(train_dataset[0], indent=2))\n",
    "\n",
    "# 加载并查看测试集\n",
    "test_dataset = load_aggregated_data(test_output_path)\n",
    "if test_dataset:\n",
    "    print(f\"测试集第一个样本内容:\")\n",
    "    print(json.dumps(test_dataset[0], indent=2))\n",
    "\n",
    "# 转换为 Pandas DataFrame（可选）\n",
    "def json_to_dataframe(data):\n",
    "    \"\"\"\n",
    "    将 JSON 数据转换为 Pandas DataFrame。\n",
    "\n",
    "    参数:\n",
    "        data (list): 包含所有样本数据的列表。\n",
    "\n",
    "    返回:\n",
    "        pd.DataFrame: 包含标签和特征的 DataFrame。\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for sample in data:\n",
    "        record = {}\n",
    "        record['label'] = sample['label']\n",
    "        # 示例：计算节点特征的均值和标准差作为简单特征\n",
    "        node_features = np.array(sample['node_features'])\n",
    "        record['node_mean_x'] = node_features[:, 0].mean()\n",
    "        record['node_mean_y'] = node_features[:, 1].mean()\n",
    "        record['node_mean_z'] = node_features[:, 2].mean()\n",
    "        record['node_std_x'] = node_features[:, 0].std()\n",
    "        record['node_std_y'] = node_features[:, 1].std()\n",
    "        record['node_std_z'] = node_features[:, 2].std()\n",
    "        # 可以根据需要添加更多特征\n",
    "        records.append(record)\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "# 转换训练集和测试集为 DataFrame\n",
    "train_df = json_to_dataframe(train_dataset)\n",
    "test_df = json_to_dataframe(test_dataset)\n",
    "\n",
    "print(\"训练集 DataFrame 预览:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n测试集 DataFrame 预览:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# 保存为 CSV 文件（可选）\n",
    "train_csv_path = os.path.join(aggregated_output_folder, 'train_dataset_dataframe.csv')\n",
    "test_csv_path = os.path.join(aggregated_output_folder, 'test_dataset_dataframe.csv')\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False, encoding='utf-8-sig')\n",
    "test_df.to_csv(test_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"训练集 DataFrame 已保存到 '{train_csv_path}'。\")\n",
    "print(f\"测试集 DataFrame 已保存到 '{test_csv_path}'。\")\n",
    "\n",
    "# 定义 PyTorch Geometric 数据集类（可选）\n",
    "class PDBDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(PDBDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        sample = self.data_list[idx]\n",
    "        node_features = torch.tensor(sample['node_features'], dtype=torch.float)\n",
    "        edge_index = torch.tensor(sample['edge_features']['edges'], dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(sample['edge_features']['directions'], dtype=torch.float)\n",
    "        rotations = torch.tensor(sample['edge_features']['rotations'], dtype=torch.float).unsqueeze(1)\n",
    "        edge_features = torch.cat([edge_attr, rotations], dim=1)  # 合并方向和旋转特征\n",
    "        label = torch.tensor(sample['label'], dtype=torch.long)\n",
    "\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features, y=label)\n",
    "        return data\n",
    "\n",
    "# 创建 PyTorch Geometric 数据集（可选）\n",
    "train_pyg_dataset = PDBDataset(train_dataset)\n",
    "test_pyg_dataset = PDBDataset(test_dataset)\n",
    "\n",
    "# 创建 DataLoader（可选）\n",
    "train_loader = DataLoader(train_pyg_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_pyg_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"PyTorch Geometric 训练集数据量: {len(train_pyg_dataset)}\")\n",
    "print(f\"PyTorch Geometric 测试集数据量: {len(test_pyg_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8orO_EtEEoLh"
   },
   "source": [
    "# 结合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 195262,
     "status": "ok",
     "timestamp": 1759391305851,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "INRwfY49Ep95",
    "outputId": "36a55a84-f21f-4d78-d68d-7afdb3840041"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "\n",
    "from torch_geometric.nn import (\n",
    "    GATConv, SAGEConv, GINConv, Set2Set, global_mean_pool\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, precision_recall_fscore_support,\n",
    "    matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import copy\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "esmc_folders = {\n",
    "    'train_pos': '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    'train_neg': '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    'test_pos': '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',\n",
    "    'test_neg': '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "}\n",
    "\n",
    "struct_folders = {\n",
    "    'train': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/train_dataset.json',\n",
    "    'test': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/test_dataset.json'\n",
    "}\n",
    "\n",
    "aggregated_output_folder = '/content/drive/MyDrive/AFP_work/esmc_struct_aggregated'\n",
    "os.makedirs(aggregated_output_folder, exist_ok=True)\n",
    "\n",
    "#==============================数据加载和预处理==============================\n",
    "#***************************加载 ESM-C 特征***************************\n",
    "def load_esmc_features(esmc_folder):\n",
    "    logits_path = os.path.join(esmc_folder, 'combined_logits.npy')\n",
    "    embeddings_path = os.path.join(esmc_folder, 'combined_embeddings.npy')\n",
    "    logits = np.load(logits_path, allow_pickle=True)\n",
    "    embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # 调试：检查 logits 的结构\n",
    "    print(f\"Logits[0] 类型: {type(logits[0])}, 值: {logits[0]}\")  #  类型 <class 'numpy.ndarray'>\n",
    "    # 打印每个样本的 logits 和 embeddings\n",
    "    print(\"Logits sample:\", logits[0])  # 打印第一个样本的 logits\n",
    "    print(\"Embeddings sample:\", embeddings[0])  # 打印第一个样本的 embeddings\n",
    "\n",
    "    # 保存特征\n",
    "    # if save_path:\n",
    "    #     np.savetxt(os.path.join(save_path, 'logits.csv'), logits, delimiter=\",\")\n",
    "    #     np.savetxt(os.path.join(save_path, 'embeddings.csv'), embeddings, delimiter=\",\")\n",
    "\n",
    "    # 从 ForwardTrackData 中提取 sequence 张量并池化\n",
    "    logits_values = []\n",
    "    for l in logits:\n",
    "        # 假设 l 是一个包含 ForwardTrackData 的数组，取第一个元素\n",
    "        forward_data = l[0] if isinstance(l, np.ndarray) else l\n",
    "        sequence_tensor = forward_data.sequence  # 获取张量\n",
    "        # 将张量移到 CPU 并转换为 float32\n",
    "        sequence_tensor = sequence_tensor.to(device='cpu', dtype=torch.float32)\n",
    "        # 对所有维度取均值，确保标量\n",
    "        pooled_value = sequence_tensor.mean(dim=[0, 1, 2]).item()  # 池化为标量\n",
    "        logits_values.append(pooled_value)\n",
    "\n",
    "    logits_values = np.array(logits_values, dtype=np.float32).reshape(-1, 1)\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    label = 1 if 'pos' in esmc_folder else 0\n",
    "    labels = np.full((logits_values.shape[0],), label)\n",
    "    return logits_values, embeddings, labels\n",
    "# 加载训练集和测试集的 ESM-C 特征\n",
    "train_pos_logits, train_pos_embeddings, train_pos_labels = load_esmc_features(esmc_folders['train_pos'])\n",
    "train_neg_logits, train_neg_embeddings, train_neg_labels = load_esmc_features(esmc_folders['train_neg'])\n",
    "test_pos_logits, test_pos_embeddings, test_pos_labels = load_esmc_features(esmc_folders['test_pos'])\n",
    "test_neg_logits, test_neg_embeddings, test_neg_labels = load_esmc_features(esmc_folders['test_neg'])\n",
    "# 合并训练集和测试集特征\n",
    "train_logits = np.vstack((train_pos_logits, train_neg_logits))\n",
    "train_embeddings = np.vstack((train_pos_embeddings, train_neg_embeddings))\n",
    "train_labels = np.hstack((train_pos_labels, train_neg_labels))\n",
    "\n",
    "test_logits = np.vstack((test_pos_logits, test_neg_logits))\n",
    "test_embeddings = np.vstack((test_pos_embeddings, test_neg_embeddings))\n",
    "test_labels = np.hstack((test_pos_labels, test_neg_labels))\n",
    "\n",
    "\n",
    "#***************************2、加载结构特征***************************\n",
    "def load_struct_features(json_path, sample_limit=5):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    data_list = []\n",
    "    for idx, sample in enumerate(tqdm(json_data, desc=f'加载结构特征 from {json_path}')):\n",
    "        required_keys = ['node_features', 'edge_features', 'label']\n",
    "        if not all(key in sample for key in required_keys):\n",
    "            print(f\"[ERROR] 样本缺少必要的键: {sample}\")\n",
    "            continue\n",
    "        node_features = sample['node_features']\n",
    "        edge_features = sample['edge_features']\n",
    "        label = sample['label']\n",
    "        edges = edge_features.get('edges', [])\n",
    "        directions = edge_features.get('directions', [])\n",
    "        rotations = edge_features.get('rotations', [])\n",
    "        num_edges = len(edges)\n",
    "        if not (len(directions) == num_edges and len(rotations) == num_edges):\n",
    "            print(f\"[ERROR] 边的数量与方向或旋转数量不匹配: {sample}\")\n",
    "            continue\n",
    "        if num_edges > 0:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            directions = torch.tensor(directions, dtype=torch.float)\n",
    "            rotations = torch.tensor(rotations, dtype=torch.float).unsqueeze(1)\n",
    "            edge_attr = torch.cat([directions, rotations], dim=1)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.empty((0, 4), dtype=torch.float)\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=label)\n",
    "        data_list.append(data)\n",
    "        if idx < sample_limit:\n",
    "            num_nodes = node_features.shape[0]\n",
    "            node_feature_dim = node_features.shape[1]\n",
    "            print(f\"样本 {idx+1}: 节点数量: {num_nodes}, 节点特征维度: {node_feature_dim}, 边数量: {num_edges}\")\n",
    "            if num_edges > 0:\n",
    "                print(f\"  边特征维度: {edge_attr.shape[1]}\")\n",
    "            print(\"-\" * 50)\n",
    "    unique_node_feature_dims = set([data.x.shape[1] for data in data_list])\n",
    "    unique_edge_feature_dims = set([data.edge_attr.shape[1] for data in data_list if data.edge_attr.shape[0] > 0])\n",
    "    print(f\"\\n所有样本中唯一的节点特征维度: {unique_node_feature_dims}\")  # 3\n",
    "    print(f\"所有样本中唯一的边特征维度: {unique_edge_feature_dims}\")  # 4\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = load_struct_features(struct_folders['train'])\n",
    "test_struct_data = load_struct_features(struct_folders['test'])\n",
    "\n",
    "for i in range(min(3, len(train_struct_data))):\n",
    "    data = train_struct_data[i]\n",
    "    print(f\"样本 {i+1} - 节点特征: {data.x.shape}, 边特征: {data.edge_attr.shape}\")\n",
    "\n",
    "##*************************** 特征标准化 ***************************\n",
    "def normalize_features(train_data_list, test_data_list=None):\n",
    "    node_scaler = StandardScaler()\n",
    "    edge_scaler = StandardScaler()\n",
    "    all_node_features = np.concatenate([data.x.numpy() for data in train_data_list], axis=0)\n",
    "    all_edge_features = np.concatenate([data.edge_attr.numpy() for data in train_data_list if data.edge_attr.shape[0] > 0], axis=0)\n",
    "    node_scaler.fit(all_node_features)\n",
    "    if all_edge_features.size > 0:\n",
    "        edge_scaler.fit(all_edge_features)\n",
    "    for data in train_data_list:\n",
    "        data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "        if data.edge_attr.shape[0] > 0:\n",
    "            data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    if test_data_list:\n",
    "        for data in test_data_list:\n",
    "            data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "            if data.edge_attr.shape[0] > 0:\n",
    "                data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    return train_data_list, test_data_list\n",
    "\n",
    "train_struct_data, test_struct_data = normalize_features(train_struct_data, test_struct_data)\n",
    "\n",
    "# #*************************** 整合 ESM-C 特征 ***************************\n",
    "def integrate_features(data_list, embeddings, logits):\n",
    "    if len(data_list) != len(embeddings) or len(data_list) != len(logits):\n",
    "        raise ValueError(f\"data_list, embeddings 和 logits 长度不匹配: {len(data_list)} vs {len(embeddings)} vs {len(logits)}\")\n",
    "    for i, data in enumerate(tqdm(data_list, desc='整合 ESM-C embeddings 和 logits')):\n",
    "        embedding = torch.tensor(embeddings[i], dtype=torch.float)  # [1152]\n",
    "        logit = torch.tensor(logits[i], dtype=torch.float).squeeze()  # [1] -> 标量\n",
    "        combined_feature = torch.cat([embedding, logit.unsqueeze(0)], dim=0)  # [1152 + 1 = 1153]\n",
    "        num_nodes = data.x.shape[0]\n",
    "        combined_expanded = combined_feature.unsqueeze(0).repeat(num_nodes, 1)  # [num_nodes, 1153]\n",
    "        data.x = torch.cat([data.x, combined_expanded], dim=1)  # [num_nodes, 3 + 1153 = 1156]\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = integrate_features(train_struct_data, train_embeddings, train_logits)\n",
    "test_struct_data = integrate_features(test_struct_data, test_embeddings, test_logits)\n",
    "\n",
    "print(f\"训练集第一个样本的节点特征维度（整合后）: {train_struct_data[0].x.shape[1]}\")  # 1156\n",
    "print(f\"测试集第一个样本的节点特征维度（整合后）: {test_struct_data[0].x.shape[1]}\") # 1156\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(ProteinDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_dataset = ProteinDataset(train_struct_data)\n",
    "test_dataset = ProteinDataset(test_struct_data)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5rxB9opFQZv"
   },
   "outputs": [],
   "source": [
    "class DeepGATModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, hidden_dim, out_dim, num_heads=4, dropout=0.3, num_layers=3):\n",
    "        super(DeepGATModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 边特征预处理层\n",
    "        self.edge_preprocess = nn.Sequential(\n",
    "            nn.Linear(edge_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # 堆叠 GAT 层\n",
    "        for layer in range(num_layers):\n",
    "            in_dim = node_feature_dim if layer == 0 else hidden_dim * num_heads\n",
    "            self.convs.append(GATConv(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                edge_dim=hidden_dim,  # 调整为预处理后的边特征维度\n",
    "                add_self_loops=True  # 添加自环，增强稳定性。能提升稳定性（每个节点至少保留自身信息）\n",
    "            ))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n",
    "\n",
    "        # 替换 Set2Set 为更简单的池化\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_heads, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        # 预处理边特征\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, out_dim, num_layers=3, dropout=0.5):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.convs.append(SAGEConv(node_feature_dim, hidden_dim))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class GINModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, out_dim, num_layers=3, dropout=0.5):\n",
    "        super(GINModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                nn_lin = nn.Sequential(nn.Linear(node_feature_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "            else:\n",
    "                nn_lin = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.convs.append(GINConv(nn_lin))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# 交叉注意力融合模块\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        # 交叉注意力机制\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "        self.fc = nn.Linear(feature_dim, 2)  # 最终分类层，输出 2 类\n",
    "\n",
    "    def forward(self, features_list):\n",
    "        # features_list: [model1_features, model2_features, model3_features], 每个形状为 [batch_size, feature_dim]\n",
    "        # 堆叠特征为 [num_models, batch_size, feature_dim]\n",
    "        feats = torch.stack(features_list, dim=0)   # features_list = [feat_gat, feat_sage, feat_gin]\n",
    "        # 应用交叉注意力\n",
    "        attn_output, _ = self.attention(feats, feats, feats)\n",
    "        # 融合特征：取平均值\n",
    "        fused_feats = attn_output.mean(dim=0)  # [batch_size, feature_dim]\n",
    "        fused_feats = self.norm(fused_feats)\n",
    "        # 分类\n",
    "        out = self.fc(fused_feats)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=10, model_save_path='best_model.pth'):\n",
    "    best_test_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in tqdm(train_loader, desc=f'训练 Epoch {epoch}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "        train_acc, _, _ = test(model, train_loader, device)\n",
    "        test_acc, test_trues, test_preds = test(model, test_loader, device)\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"早停：在第 {epoch} 轮训练后，无提升，停止训练。\")\n",
    "                break\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return best_test_acc, best_model_wts\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='评估'):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            trues.extend(data.y.cpu().numpy())\n",
    "            correct += (pred == data.y).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy, trues, preds\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "def detailed_test(model, loader, device, models=None):\n",
    "    model.eval()\n",
    "    preds, trues, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='详细评估'):\n",
    "            data = data.to(device)\n",
    "            if isinstance(model, CrossAttentionFusion):  # 检查是否为融合模型\n",
    "                if models is None:\n",
    "                    raise ValueError(\"models dictionary required for CrossAttentionFusion evaluation\")\n",
    "                # 提取特征列表\n",
    "                feat_gat = models['DeepGATModel'].get_last_layer_features(data)\n",
    "                feat_sage = models['GraphSAGEModel'].get_last_layer_features(data)\n",
    "                feat_gin = models['GINModel'].get_last_layer_features(data)\n",
    "                features_list = [feat_gat, feat_sage, feat_gin]\n",
    "                out = model(features_list)\n",
    "            else:\n",
    "                out = model(data)  # 普通模型直接处理 DataBatch\n",
    "            prob = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "            true = data.y.cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            trues.extend(true)\n",
    "            probs.extend(prob)\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(trues, preds, average='binary')\n",
    "    mcc = matthews_corrcoef(trues, preds)\n",
    "    auc = roc_auc_score(trues, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(trues, preds).ravel()\n",
    "    sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics = {'acc': acc, 'mcc': mcc, 'auc': auc, 'sn': sn, 'sp': sp, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    return metrics\n",
    "\n",
    "def optimize_model(model_class, train_loader, test_loader, device, model_params, n_trials=5):\n",
    "    def objective(trial):\n",
    "        # 定义超参数搜索空间\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 64, 512)\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "\n",
    "        # 根据模型类初始化模型\n",
    "        if model_class == DeepGATModel:\n",
    "            num_heads = trial.suggest_int('num_heads', 2, 16)\n",
    "            model = DeepGATModel(\n",
    "                node_feature_dim=model_params['node_feature_dim'],\n",
    "                edge_feature_dim=model_params['edge_feature_dim'],\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=model_params['out_dim'],\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                num_layers=num_layers\n",
    "            ).to(device)\n",
    "        elif model_class == GraphSAGEModel:\n",
    "            model = GraphSAGEModel(\n",
    "                node_feature_dim=model_params['node_feature_dim'],\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=model_params['out_dim'],\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "        elif model_class == GINModel:\n",
    "            model = GINModel(\n",
    "                node_feature_dim=model_params['node_feature_dim'],\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=model_params['out_dim'],\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        best_acc, _ = train_model(\n",
    "            model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
    "            device, num_epochs=50, patience=10, model_save_path=f\"best_{model_class.__name__}.pth\"\n",
    "        )\n",
    "        return best_acc\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=5)\n",
    "    print(f\"总试验次数: {len(study.trials)}\")\n",
    "    print(f\"{model_class.__name__} 最佳超参数: {study.best_params}\")\n",
    "    for trial in study.trials:\n",
    "      print(f\"Trial {trial.number}: State={trial.state}, Value={trial.value}\")\n",
    "    return study.best_params\n",
    "\n",
    "def explain_features(model, test_loader, device, output_folder):\n",
    "    # ESM-C 特征的 SHAP 分析\n",
    "    print(\"正在进行 ESM-C 特征的 SHAP 分析...\")\n",
    "    esmc_features = np.hstack([train_embeddings, train_logits])\n",
    "    labels = train_labels\n",
    "    proxy_model = XGBClassifier()\n",
    "    proxy_model.fit(esmc_features, labels)\n",
    "    explainer = shap.Explainer(proxy_model)\n",
    "    shap_values = explainer(esmc_features)\n",
    "    shap.summary_plot(shap_values, esmc_features, plot_type=\"bar\", show=False)\n",
    "    plt.title(\"ESM-C 特征重要性 (SHAP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, \"shap_esmc_features.png\"))\n",
    "    plt.close()\n",
    "    print(\"SHAP 分析完成，结果已保存至 shap_esmc_features.png\")\n",
    "\n",
    "    #GNNExplainer 分析（以 DeepGATModel 为例）\n",
    "    print(\"正在进行 GNNExplainer 分析...\")\n",
    "    trained_model = models['DeepGATModel']\n",
    "    explainer = GNNExplainer(trained_model, epochs=200, lr=0.01)\n",
    "    for sample_idx in range(min(5, len(test_struct_data))):\n",
    "        data = test_struct_data[sample_idx].to(device)\n",
    "        node_idx = 0  # 分析第一个节点\n",
    "        node_feat_mask, edge_mask = explainer.explain_node(node_idx, data.x, data.edge_index, data.edge_attr)\n",
    "        print(f\"样本 {sample_idx+1} | 节点 0 特征重要性（前5个）: {node_feat_mask[:5]} | 边重要性（前5个）: {edge_mask[:5]}\")\n",
    "    print(\"GNNExplainer 分析完成\")\n",
    "\n",
    "    # t-SNE 可视化\n",
    "    print(\"正在进行 t-SNE 可视化...\")\n",
    "    def get_last_layer_features(model, loader, device):\n",
    "        model.eval()\n",
    "        features = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                feat = model.get_last_layer_features(data)\n",
    "                features.append(feat.cpu().numpy())\n",
    "                labels.append(data.y.cpu().numpy())\n",
    "        return np.vstack(features), np.hstack(labels)\n",
    "\n",
    "    features, labels = get_last_layer_features(trained_model, test_loader, device)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels, cmap='coolwarm', alpha=0.6)\n",
    "    plt.title(\"t-SNE of Last Layer Features (DeepGATModel)\")\n",
    "    plt.colorbar(label='Class')\n",
    "    plt.savefig(os.path.join(output_folder, \"tsne_last_layer.png\"))\n",
    "    plt.close()\n",
    "    print(\"t-SNE 可视化完成，结果已保存至 tsne_last_layer.png\")\n",
    "\n",
    "    return results, models\n",
    "\n",
    "def train_and_evaluate_models(train_loader, test_loader, device, output_folder):\n",
    "    model_params = {\"node_feature_dim\": 1156, \"edge_feature_dim\": 4, \"out_dim\": 2}\n",
    "    best_params = {}\n",
    "\n",
    "    # 优化并训练三个基础模型\n",
    "    for model_class in [DeepGATModel, GraphSAGEModel, GINModel]:\n",
    "        print(f\"优化 {model_class.__name__}...\")\n",
    "        best_params[model_class.__name__] = optimize_model(model_class, train_loader, test_loader, device, model_params, n_trials=10)\n",
    "\n",
    "    # 初始化模型\n",
    "    models = {\n",
    "        \"DeepGATModel\": DeepGATModel(\n",
    "            node_feature_dim=1156, edge_feature_dim=4, out_dim=2,\n",
    "            hidden_dim=best_params[\"DeepGATModel\"][\"hidden_dim\"],\n",
    "            num_layers=best_params[\"DeepGATModel\"][\"num_layers\"],\n",
    "            dropout=best_params[\"DeepGATModel\"][\"dropout\"],\n",
    "            num_heads=best_params[\"DeepGATModel\"][\"num_heads\"]\n",
    "        ).to(device),\n",
    "        \"GraphSAGEModel\": GraphSAGEModel(\n",
    "            node_feature_dim=1156, out_dim=2,\n",
    "            hidden_dim=best_params[\"GraphSAGEModel\"][\"hidden_dim\"],\n",
    "            num_layers=best_params[\"GraphSAGEModel\"][\"num_layers\"],\n",
    "            dropout=best_params[\"GraphSAGEModel\"][\"dropout\"]\n",
    "        ).to(device),\n",
    "        \"GINModel\": GINModel(\n",
    "            node_feature_dim=1156, out_dim=2,\n",
    "            hidden_dim=best_params[\"GINModel\"][\"hidden_dim\"],\n",
    "            num_layers=best_params[\"GINModel\"][\"num_layers\"],\n",
    "            dropout=best_params[\"GINModel\"][\"dropout\"]\n",
    "        ).to(device)\n",
    "    }\n",
    "\n",
    "    # 训练并评估每个模型\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=best_params[name][\"lr\"], weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        save_path = os.path.join(output_folder, f\"best_{name}.pth\")\n",
    "        best_acc, _ = train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=10, model_save_path=save_path)\n",
    "        metrics = detailed_test(model, test_loader, device)\n",
    "        results[name] = metrics\n",
    "        print(f\"{name} - Acc: {metrics['acc']:.4f}, MCC: {metrics['mcc']:.4f}, AUC: {metrics['auc']:.4f}\")\n",
    "\n",
    "    # 性能对比\n",
    "    print(\"\\n### 三个模型性能对比 ###\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"{name}: Acc: {metrics['acc']:.4f}, MCC: {metrics['mcc']:.4f}, AUC: {metrics['auc']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "    # 加载最佳模型\n",
    "    for name, model in models.items():\n",
    "        model.load_state_dict(torch.load(os.path.join(output_folder, f\"best_{name}.pth\")))\n",
    "        model.eval()\n",
    "\n",
    "    return results, models\n",
    "\n",
    "# 交叉注意力融合训练\n",
    "# 在交叉注意力融合训练函数中修改\n",
    "def train_cross_attention_fusion(models, train_loader, test_loader, device, output_folder, num_epochs=50, patience=10):\n",
    "    fusion_module = CrossAttentionFusion(feature_dim=256, num_heads=4, dropout=0.1).to(device)\n",
    "    optimizer_fusion = torch.optim.Adam(fusion_module.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler_fusion = torch.optim.lr_scheduler.StepLR(optimizer_fusion, step_size=10, gamma=0.1)\n",
    "\n",
    "    print(\"\\n### 训练交叉注意力融合模块 ###\")\n",
    "    best_fusion_acc = 0\n",
    "    best_fusion_wts = copy.deepcopy(fusion_module.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        fusion_module.train()\n",
    "        total_loss = 0\n",
    "        for data in tqdm(train_loader, desc=f'融合训练 Epoch {epoch}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                # 确保获取的是纯张量\n",
    "                feat_gat = models['DeepGATModel'].get_last_layer_features(data)\n",
    "                feat_sage = models['GraphSAGEModel'].get_last_layer_features(data)\n",
    "                feat_gin = models['GINModel'].get_last_layer_features(data)\n",
    "                # 调试：打印特征形状和类型\n",
    "                print(f\"feat_gat shape: {feat_gat.shape}, type: {type(feat_gat)}\")\n",
    "                print(f\"feat_sage shape: {feat_sage.shape}, type: {type(feat_sage)}\")\n",
    "                print(f\"feat_gin shape: {feat_gin.shape}, type: {type(feat_gin)}\")\n",
    "                # 如果返回的是 DataBatch，提取特征张量\n",
    "                if isinstance(feat_gat, torch.Tensor) and feat_gat.dim() == 2:  # 确保是 [batch_size, feature_dim]\n",
    "                    features_list = [feat_gat, feat_sage, feat_gin]\n",
    "                else:\n",
    "                    raise ValueError(\"Expected pure tensors from get_last_layer_features, got unexpected type or shape\")\n",
    "            out = fusion_module(features_list)\n",
    "            loss = criterion(out, data.y)\n",
    "            optimizer_fusion.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_fusion.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        scheduler_fusion.step()\n",
    "\n",
    "        fusion_module.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                data = data.to(device)\n",
    "                feat_gat = models['DeepGATModel'].get_last_layer_features(data)\n",
    "                feat_sage = models['GraphSAGEModel'].get_last_layer_features(data)\n",
    "                feat_gin = models['GINModel'].get_last_layer_features(data)\n",
    "                # 同样的检查和处理\n",
    "                if isinstance(feat_gat, torch.Tensor) and feat_gat.dim() == 2:\n",
    "                    features_list = [feat_gat, feat_sage, feat_gin]\n",
    "                else:\n",
    "                    raise ValueError(\"Expected pure tensors from get_last_layer_features in test loop\")\n",
    "                out = fusion_module(features_list)\n",
    "                pred = out.argmax(dim=1)\n",
    "                preds.extend(pred.cpu().numpy())\n",
    "                trues.extend(data.y.cpu().numpy())\n",
    "        test_acc = accuracy_score(trues, preds)\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {avg_loss:.4f}, Fusion Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        if test_acc > best_fusion_acc:\n",
    "            best_fusion_acc = test_acc\n",
    "            best_fusion_wts = copy.deepcopy(fusion_module.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(fusion_module.state_dict(), os.path.join(output_folder, \"best_cross_attention_fusion.pth\"))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"早停：在第 {epoch} 轮训练后，无提升，停止训练。\")\n",
    "                break\n",
    "\n",
    "    fusion_module.load_state_dict(best_fusion_wts)\n",
    "    fusion_metrics = detailed_test(fusion_module, test_loader, device, models=models)  # 传递 models 参数\n",
    "    return fusion_module, fusion_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3797213,
     "status": "ok",
     "timestamp": 1759399696297,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "hj-siF4M54al",
    "outputId": "0da1ee9e-15d0-4f14-fb2f-d99737acc934"
   },
   "outputs": [],
   "source": [
    "# ### 运行实验\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    results, models = train_and_evaluate_models(train_loader, test_loader, device, aggregated_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "v-TWJS8_bYqw",
    "outputId": "c0f3eceb-1170-4cf6-ecf7-5ceae06340e0"
   },
   "outputs": [],
   "source": [
    "fusion_module, fusion_metrics = train_cross_attention_fusion(models, train_loader, test_loader, device, aggregated_output_folder)\n",
    "results[\"CrossAttentionFusion\"] = fusion_metrics\n",
    "print(f\"CrossAttentionFusion - Acc: {fusion_metrics['acc']:.4f}, MCC: {fusion_metrics['mcc']:.4f}, AUC: {fusion_metrics['auc']:.4f}, Precision: {fusion_metrics['precision']:.4f}, Recall: {fusion_metrics['recall']:.4f}, F1: {fusion_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIlrqhJhTvaP"
   },
   "source": [
    "# 单调GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5BI2ntGwZFJ8",
    "outputId": "46977f40-9e3b-40ca-afc8-250b22c50712"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, precision_recall_fscore_support,\n",
    "    matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import copy\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置设备（GPU/CPU）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 路径配置\n",
    "esmc_folders = {\n",
    "    'train_pos': '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    'train_neg': '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    'test_pos': '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',\n",
    "    'test_neg': '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "}\n",
    "\n",
    "struct_folders = {\n",
    "    'train': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/train_dataset.json',\n",
    "    'test': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/test_dataset.json'\n",
    "}\n",
    "\n",
    "output_folder = '/content/drive/MyDrive/AFP_work/deepgat_explain_results'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ### 数据加载和预处理\n",
    "\n",
    "# **加载 ESM-C 特征**\n",
    "def load_esmc_features(esmc_folder, save_path=None):\n",
    "    logits_path = os.path.join(esmc_folder, 'combined_logits.npy')\n",
    "    embeddings_path = os.path.join(esmc_folder, 'combined_embeddings.npy')\n",
    "    logits = np.load(logits_path, allow_pickle=True)\n",
    "    embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # 调试：检查 logits 和 embeddings 的结构\n",
    "    print(f\"Logits[0] 类型: {type(logits[0])}, 值: {logits[0]}\")\n",
    "    print(f\"Embeddings[0] 形状: {embeddings[0].shape}, 样本: {embeddings[0][:5]}\")\n",
    "\n",
    "    # 从 ForwardTrackData 中提取 sequence 张量并池化\n",
    "    logits_values = []\n",
    "    for l in logits:\n",
    "        forward_data = l[0] if isinstance(l, np.ndarray) else l\n",
    "        sequence_tensor = forward_data.sequence  # 获取张量\n",
    "        sequence_tensor = sequence_tensor.to(device='cpu', dtype=torch.float32)\n",
    "        pooled_value = sequence_tensor.mean(dim=[0, 1, 2]).item()  # 池化为标量\n",
    "        logits_values.append(pooled_value)\n",
    "\n",
    "    logits_values = np.array(logits_values, dtype=np.float32).reshape(-1, 1)\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    label = 1 if 'pos' in esmc_folder else 0\n",
    "    labels = np.full((logits_values.shape[0],), label)\n",
    "    return logits_values, embeddings, labels\n",
    "\n",
    "# 加载训练集和测试集的 ESM-C 特征\n",
    "train_pos_logits, train_pos_embeddings, train_pos_labels = load_esmc_features(esmc_folders['train_pos'])\n",
    "train_neg_logits, train_neg_embeddings, train_neg_labels = load_esmc_features(esmc_folders['train_neg'])\n",
    "test_pos_logits, test_pos_embeddings, test_pos_labels = load_esmc_features(esmc_folders['test_pos'])\n",
    "test_neg_logits, test_neg_embeddings, test_neg_labels = load_esmc_features(esmc_folders['test_neg'])\n",
    "\n",
    "# 合并训练集和测试集特征\n",
    "train_logits = np.vstack((train_pos_logits, train_neg_logits))\n",
    "train_embeddings = np.vstack((train_pos_embeddings, train_neg_embeddings))\n",
    "train_labels = np.hstack((train_pos_labels, train_neg_labels))\n",
    "\n",
    "test_logits = np.vstack((test_pos_logits, test_neg_logits))\n",
    "test_embeddings = np.vstack((test_pos_embeddings, test_neg_embeddings))\n",
    "test_labels = np.hstack((test_pos_labels, test_neg_labels))\n",
    "\n",
    "print(f\"训练集 logits 形状: {train_logits.shape}\")\n",
    "print(f\"训练集 embeddings 形状: {train_embeddings.shape}\")\n",
    "print(f\"训练集 labels 形状: {train_labels.shape}\")\n",
    "print(f\"测试集 logits 形状: {test_logits.shape}\")\n",
    "print(f\"测试集 embeddings 形状: {test_embeddings.shape}\")\n",
    "print(f\"测试集 labels 形状: {test_labels.shape}\")\n",
    "\n",
    "# **加载结构特征**\n",
    "def load_struct_features(json_path, sample_limit=5):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    data_list = []\n",
    "    for idx, sample in enumerate(tqdm(json_data, desc=f'加载结构特征 from {json_path}')):\n",
    "        required_keys = ['node_features', 'edge_features', 'label']\n",
    "        if not all(key in sample for key in required_keys):\n",
    "            print(f\" 样本缺少必要的键: {sample}\")\n",
    "            continue\n",
    "        node_features = sample['node_features']\n",
    "        edge_features = sample['edge_features']\n",
    "        label = sample['label']\n",
    "        edges = edge_features.get('edges', [])\n",
    "        directions = edge_features.get('directions', [])\n",
    "        rotations = edge_features.get('rotations', [])\n",
    "        num_edges = len(edges)\n",
    "        if not (len(directions) == num_edges and len(rotations) == num_edges):\n",
    "            print(f\" 边的数量与方向或旋转数量不匹配: {sample}\")\n",
    "            continue\n",
    "        if num_edges > 0:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            directions = torch.tensor(directions, dtype=torch.float)\n",
    "            rotations = torch.tensor(rotations, dtype=torch.float).unsqueeze(1)\n",
    "            edge_attr = torch.cat([directions, rotations], dim=1)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.empty((0, 4), dtype=torch.float)\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=label)\n",
    "        data_list.append(data)\n",
    "        if idx < sample_limit:\n",
    "            num_nodes = node_features.shape[0]\n",
    "            node_feature_dim = node_features.shape[1]\n",
    "            print(f\"样本 {idx+1}: 节点数量: {num_nodes}, 节点特征维度: {node_feature_dim}, 边数量: {num_edges}\")\n",
    "            if num_edges > 0:\n",
    "                print(f\"  边特征维度: {edge_attr.shape[1]}\")\n",
    "            print(\"-\" * 50)\n",
    "    print(f\"\\n所有样本中唯一的节点特征维度: {set([data.x.shape[1] for data in data_list])}\")\n",
    "    print(f\"所有样本中唯一的边特征维度: {set([data.edge_attr.shape[1] for data in data_list if data.edge_attr.shape[0] > 0])}\")\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = load_struct_features(struct_folders['train'])\n",
    "test_struct_data = load_struct_features(struct_folders['test'])\n",
    "\n",
    "# **特征标准化**\n",
    "def normalize_features(train_data_list, test_data_list=None):\n",
    "    node_scaler = StandardScaler()\n",
    "    edge_scaler = StandardScaler()\n",
    "    all_node_features = np.concatenate([data.x.numpy() for data in train_data_list], axis=0)\n",
    "    all_edge_features = np.concatenate([data.edge_attr.numpy() for data in train_data_list if data.edge_attr.shape[0] > 0], axis=0)\n",
    "    node_scaler.fit(all_node_features)\n",
    "    if all_edge_features.size > 0:\n",
    "        edge_scaler.fit(all_edge_features)\n",
    "    for data in train_data_list:\n",
    "        data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "        if data.edge_attr.shape[0] > 0:\n",
    "            data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    if test_data_list:\n",
    "        for data in test_data_list:\n",
    "            data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "            if data.edge_attr.shape[0] > 0:\n",
    "                data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    return train_data_list, test_data_list\n",
    "\n",
    "train_struct_data, test_struct_data = normalize_features(train_struct_data, test_struct_data)\n",
    "\n",
    "# **整合 ESM-C 特征**\n",
    "def integrate_features(data_list, embeddings, logits):\n",
    "    if len(data_list) != len(embeddings) or len(data_list) != len(logits):\n",
    "        raise ValueError(f\"data_list, embeddings 和 logits 长度不匹配: {len(data_list)} vs {len(embeddings)} vs {len(logits)}\")\n",
    "    for i, data in enumerate(tqdm(data_list, desc='整合 ESM-C embeddings 和 logits')):\n",
    "        embedding = torch.tensor(embeddings[i], dtype=torch.float)  # [1152]\n",
    "        logit = torch.tensor(logits[i], dtype=torch.float).squeeze()  # [1] -> 标量\n",
    "        combined_feature = torch.cat([embedding, logit.unsqueeze(0)], dim=0)  # [1153]\n",
    "        num_nodes = data.x.shape[0]\n",
    "        combined_expanded = combined_feature.unsqueeze(0).repeat(num_nodes, 1)  # [num_nodes, 1153]\n",
    "        data.x = torch.cat([data.x, combined_expanded], dim=1)  # [num_nodes, 1156]\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = integrate_features(train_struct_data, train_embeddings, train_logits)\n",
    "test_struct_data = integrate_features(test_struct_data, test_embeddings, test_logits)\n",
    "\n",
    "print(f\"训练集第一个样本的节点特征维度（整合后）: {train_struct_data[0].x.shape[1]}\")\n",
    "print(f\"测试集第一个样本的节点特征维度（整合后）: {test_struct_data[0].x.shape[1]}\")\n",
    "\n",
    "# **创建数据集和数据加载器**\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(ProteinDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_dataset = ProteinDataset(train_struct_data)\n",
    "test_dataset = ProteinDataset(test_struct_data)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ### 调试 DeepGATModel\n",
    "class DeepGATModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, hidden_dim, out_dim, num_heads=4, dropout=0.3, num_layers=3):\n",
    "        super(DeepGATModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 边特征预处理层\n",
    "        self.edge_preprocess = nn.Sequential(\n",
    "            nn.Linear(edge_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # 初始化GAT层\n",
    "        for layer in range(num_layers):\n",
    "            in_dim = node_feature_dim if layer == 0 else hidden_dim * num_heads\n",
    "            self.convs.append(GATConv(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                edge_dim=hidden_dim,  # 预处理后的边特征维度\n",
    "                add_self_loops=True  # 添加自环，增强稳定性\n",
    "            ))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n",
    "\n",
    "        # 池化和全连接层\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_heads, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data, print_shapes=False):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # 仅在第一次批次打印形状（每轮试验一次）\n",
    "        if print_shapes:\n",
    "            print(f\"输入 - 节点特征形状: {x.shape}, 边特征形状: {edge_attr.shape if edge_attr is not None else '无'}, 边索引形状: {edge_index.shape}\")\n",
    "\n",
    "        # 预处理边特征\n",
    "        if edge_attr is not None and edge_attr.shape[0] > 0:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "            if print_shapes:\n",
    "                print(f\"预处理后边特征形状: {edge_attr.shape}\")\n",
    "        else:\n",
    "            edge_attr = None\n",
    "            if print_shapes:\n",
    "                print(\"无边特征，使用默认边处理\")\n",
    "\n",
    "        # 逐层处理\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.batch_norms)):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            if print_shapes:\n",
    "                print(f\"GATConv层 {i+1} 输出形状: {x.shape}\")\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 池化\n",
    "        x = self.readout(x, batch)\n",
    "        if print_shapes:\n",
    "            print(f\"池化后特征形状: {x.shape}\")\n",
    "\n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        if print_shapes:\n",
    "            print(f\"FC1输出形状: {x.shape}\")\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        if print_shapes:\n",
    "            print(f\"FC2输出形状: {x.shape}\")  # 应为 [batch_size, 2]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        if edge_attr is not None and edge_attr.shape[0] > 0:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "        else:\n",
    "            edge_attr = None\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# ### 训练和调试 DeepGATModel\n",
    "def train_deepgat(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=10, model_save_path='best_deepgat.pth'):\n",
    "    best_test_acc = 0\n",
    "    best_model_wts = None  # 初始值设置为None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        # 仅在第一个批次打印形状（每轮试验一次）\n",
    "        print_shapes = (epoch == 1)  # 仅第一轮打印\n",
    "        for data in tqdm(train_loader, desc=f'训练 DeepGAT Epoch {epoch}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data, print_shapes=print_shapes)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            if print_shapes:\n",
    "                print(f\"批次损失: {loss.item():.4f}\")\n",
    "                print_shapes = False  # 仅打印一次\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "        train_acc, train_trues, train_preds = test(model, train_loader, device)\n",
    "        test_acc, test_trues, test_preds = test(model, test_loader, device)\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())  # 更新最佳权重\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(best_model_wts, model_save_path)  # 保存最佳模型\n",
    "            print(f\"保存最佳模型，测试准确率: {test_acc:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"早停：在第 {epoch} 轮训练后，无提升，停止训练。\")\n",
    "                break\n",
    "\n",
    "    # 加载最佳权重\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    else:\n",
    "        print(\"警告：未找到最佳权重，使用当前模型状态。\")\n",
    "    return best_test_acc, best_model_wts\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='评估 DeepGAT'):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            trues.extend(data.y.cpu().numpy())\n",
    "            correct += (pred == data.y).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy, trues, preds\n",
    "\n",
    "def detailed_test(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, trues, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='详细评估 DeepGAT'):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            prob = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "            true = data.y.cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            trues.extend(true)\n",
    "            probs.extend(prob)\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(trues, preds, average='binary')\n",
    "    mcc = matthews_corrcoef(trues, preds)\n",
    "    auc = roc_auc_score(trues, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(trues, preds).ravel()\n",
    "    sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics = {'acc': acc, 'mcc': mcc, 'auc': auc, 'sn': sn, 'sp': sp, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    return metrics\n",
    "\n",
    "# ### 超参数优化 (Optuna)\n",
    "\n",
    "def optimize_deepgat(train_loader, test_loader, device, n_trials=10):\n",
    "    def objective(trial):\n",
    "        # 定义超参数搜索空间\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 64, 512)\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "        num_heads = trial.suggest_int('num_heads', 2, 16)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "\n",
    "        # 初始化 DeepGATModel\n",
    "        model = DeepGATModel(\n",
    "            node_feature_dim=1156,  # 整合后的节点特征维度\n",
    "            edge_feature_dim=4,     # 边特征维度\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=2,             # 二分类\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            num_layers=num_layers\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        # 训练并返回最佳准确率和权重\n",
    "        best_acc, best_wts = train_deepgat(\n",
    "            model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
    "            device, num_epochs=50, patience=10, model_save_path='best_deepgat_optimized.pth'\n",
    "        )\n",
    "        return best_acc\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    print(f\"总试验次数: {len(study.trials)}\")\n",
    "    print(f\"DeepGATModel 最佳超参数: {study.best_params}\")\n",
    "    for trial in study.trials:\n",
    "        print(f\"Trial {trial.number}: State={trial.state}, Value={trial.value}\")\n",
    "    return study.best_params\n",
    "\n",
    "# ### 可解释性分析函数\n",
    "\n",
    "def explain_deepgat(model, train_loader, test_loader, device, output_folder):\n",
    "    # 1. SHAP分析（ESM-C特征）\n",
    "    print(\"正在进行 ESM-C 特征的 SHAP 分析...\")\n",
    "    esmc_features = np.hstack([train_embeddings, train_logits])  # [num_samples, 1153]\n",
    "    labels = train_labels\n",
    "    proxy_model = XGBClassifier()\n",
    "    proxy_model.fit(esmc_features, labels)\n",
    "    explainer = shap.Explainer(proxy_model)\n",
    "    shap_values = explainer(esmc_features)\n",
    "    # 绘制SHAP特征重要性\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, esmc_features, plot_type=\"bar\", show=False)\n",
    "    plt.title(\"ESM-C 特征重要性 (SHAP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, \"shap_esmc_features.png\"))\n",
    "    plt.close()\n",
    "    print(\"SHAP 分析完成，结果已保存至 shap_esmc_features.png\")\n",
    "\n",
    "    # 2. GNNExplainer分析（节点和边的重要性）\n",
    "    print(\"正在进行 GNNExplainer 分析...\")\n",
    "    model.eval()\n",
    "    reset(model)  # 重置模型参数以确保解释一致性\n",
    "    explainer = GNNExplainer(model, epochs=200, lr=0.01)\n",
    "    for sample_idx in range(min(5, len(test_struct_data))):  # 分析前5个测试样本\n",
    "        data = test_struct_data[sample_idx].to(device)\n",
    "        node_idx = 0  # 分析第一个节点\n",
    "        node_feat_mask, edge_mask = explainer.explain_node(node_idx, data.x, data.edge_index, data.edge_attr)\n",
    "        print(f\"样本 {sample_idx+1} | 节点 {node_idx} 特征重要性（前5个）: {node_feat_mask[:5]} | 边重要性（前5个）: {edge_mask[:5] if edge_mask is not None else '无'}\")\n",
    "    print(\"GNNExplainer 分析完成\")\n",
    "\n",
    "    # 3. t-SNE可视化（最后层特征分布）\n",
    "    print(\"正在进行 t-SNE 可视化...\")\n",
    "    def get_last_layer_features(model, loader, device):\n",
    "        model.eval()\n",
    "        features = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                feat = model.get_last_layer_features(data)\n",
    "                features.append(feat.cpu().numpy())\n",
    "                labels.append(data.y.cpu().numpy())\n",
    "        return np.vstack(features), np.hstack(labels)\n",
    "\n",
    "    features, labels = get_last_layer_features(model, test_loader, device)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels, cmap='coolwarm', alpha=0.6)\n",
    "    plt.title(\"DeepGATModel 最后层特征 t-SNE 可视化\")\n",
    "    plt.colorbar(label='Class (0=Neg, 1=Pos)')\n",
    "    plt.savefig(os.path.join(output_folder, \"tsne_deepgat_features.png\"))\n",
    "    plt.close()\n",
    "    print(\"t-SNE 可视化完成，结果已保存至 tsne_deepgat_features.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    deepgat_model = DeepGATModel(\n",
    "        node_feature_dim=1156,\n",
    "        edge_feature_dim=4,\n",
    "        hidden_dim=256,   \n",
    "        out_dim=2,        \n",
    "        num_heads=4,            \n",
    "        dropout=0.3,            \n",
    "        num_layers=3           \n",
    "    ).to(device)\n",
    "\n",
    "    print(\"开始优化 DeepGATModel 超参数...\")\n",
    "    best_params = optimize_deepgat(train_loader, test_loader, device, n_trials=10)\n",
    "\n",
    "    # 使用最佳超参数初始化并训练模型\n",
    "    deepgat_model = DeepGATModel(\n",
    "        node_feature_dim=1156,\n",
    "        edge_feature_dim=4,\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        out_dim=2,\n",
    "        num_heads=best_params['num_heads'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_layers=best_params['num_layers']\n",
    "    ).to(device)\n",
    "\n",
    "    # 训练模型\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(deepgat_model.parameters(), lr=best_params['lr'], weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    best_acc, best_wts = train_deepgat(\n",
    "        deepgat_model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
    "        device, num_epochs=50, patience=10, model_save_path=os.path.join(output_folder, 'best_deepgat_final.pth')\n",
    "    )\n",
    "\n",
    "    # 详细评估\n",
    "    metrics = detailed_test(deepgat_model, test_loader, device)\n",
    "    print(\"\\n### DeepGATModel 详细性能 ###\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # 保存模型和结果\n",
    "    if best_wts is not None:\n",
    "        torch.save(best_wts, os.path.join(output_folder, 'best_deepgat_final.pth'))\n",
    "    with open(os.path.join(output_folder, 'deepgat_metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    # 调试：打印部分预测结果\n",
    "    deepgat_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = deepgat_model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            true = data.y\n",
    "            print(f\"预测样本 - 真实标签: {true[:5].cpu().numpy()}, 预测标签: {pred[:5].cpu().numpy()}\")\n",
    "            break  # 仅打印第一个批次\n",
    "\n",
    "    # 可解释性分析\n",
    "    explain_deepgat(deepgat_model, train_loader, test_loader, device, output_folder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "a8BDbX0lIZSs",
    "6dUEQFzFIb3o",
    "JnqCMjKgIhng",
    "mBHmxnQzo6b6",
    "5Za8fI8fov8b",
    "fz6lWj8Lx6a_",
    "LADKYcKABvi8",
    "WOq4HHPYwv_h",
    "5RnxO1AX2u46",
    "JQqE1ofqsviH",
    "m0LT6AZYEBL3",
    "RQ-gFfgEuGH0",
    "sf4bWPY6cXlK",
    "4dxS0Eu0uKOq",
    "l2bQyS6_wILJ",
    "uYVaYr92Mecm",
    "4_RLor2ULZf0",
    "KrR3SQWpO5qt",
    "-pnBCTCqhdFH",
    "a4R_Fa-4ax-1",
    "a22DaAZnhjD9",
    "qX5pFqxTy1V0",
    "2CVhwiDzSEor",
    "8orO_EtEEoLh",
    "t_sew4v63Cfs",
    "y_AphIUVpAgK",
    "uzBacJwdpFnU",
    "bQ1ZVdoDDeoz",
    "NIlrqhJhTvaP",
    "XpoEzKQMj3J6",
    "gDwMJ97Z7HGb"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
