{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8161,
     "status": "ok",
     "timestamp": 1759456559185,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "-521mSLjDu2T",
    "outputId": "7427adb3-5d74-4c61-814c-7a2ff11cb88a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8BDbX0lIZSs"
   },
   "source": [
    "# 包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9455,
     "status": "ok",
     "timestamp": 1759456575827,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "eQTzzqA6IaiX",
    "outputId": "d4ec024f-acf1-40a7-ebfa-bf1051bc0ea5"
   },
   "outputs": [],
   "source": [
    "pip install Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 65838,
     "status": "ok",
     "timestamp": 1759456641667,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "plH_EKiYEvX7",
    "outputId": "005f042a-9444-4573-9411-6d333c14ee46"
   },
   "outputs": [],
   "source": [
    "pip install torch torch-geometric optuna tqdm scikit-learn esm umap-learn shap xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 66614,
     "status": "ok",
     "timestamp": 1759456708288,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "GsFMLAa9waEZ",
    "outputId": "d7551d7e-1c0a-4ca6-d0ee-202b030d69b5"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy pandas\n",
    "!pip install numpy==1.26.4 pandas==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2006,
     "status": "ok",
     "timestamp": 1759456710305,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "8E6hDtucnwBm",
    "outputId": "33742803-5469-4518-e61b-2972e08294b1"
   },
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1759456711932,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "I-6FNFN-T9Bt",
    "outputId": "e0daea41-baf8-495e-a005-f8f8909216fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dUEQFzFIb3o"
   },
   "source": [
    "# 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnqCMjKgIhng"
   },
   "source": [
    "## 阳性样本cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13838,
     "status": "ok",
     "timestamp": 1753430799855,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "JJAvM38xIkKp",
    "outputId": "b9f4bde0-5106-4072-b177-ed45ce0ca42e"
   },
   "outputs": [],
   "source": [
    "!apt-get install cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1753430800008,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "geFeazrvKkPZ",
    "outputId": "951ec2e2-2a32-474b-b893-580795ad0d4f"
   },
   "outputs": [],
   "source": [
    "!grep -c \"^>\" AFP.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1753430800087,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "6HMPMZIdJv6t",
    "outputId": "b8917c51-af19-480d-b3a8-4564d0b47b33"
   },
   "outputs": [],
   "source": [
    "!cd-hit -i AFP.fasta -o AFP_clustered.fasta -c 0.8 -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "error",
     "timestamp": 1753430800229,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "nJiSgc8FL6JT",
    "outputId": "7e4df39f-1f80-48cb-c642-4555118e1d24"
   },
   "outputs": [],
   "source": [
    "def convert_to_fasta(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        sequences = file.readlines()\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            sequence = sequence.strip()  \n",
    "            file.write(f\">Sequence_{i + 1}\\n{sequence}\\n\")\n",
    "\n",
    "convert_to_fasta('AFP_2.txt', 'AFP_2.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4XKTJ4scakY"
   },
   "outputs": [],
   "source": [
    "with open(\"AFP.fasta\", \"r\") as file:\n",
    "    sequences = file.read().split('>')\n",
    "    sequences = [seq for seq in sequences if seq.strip()]\n",
    "    unique_sequences = set(sequences)\n",
    "\n",
    "print(f\"Total sequences: {len(sequences)}\")\n",
    "print(f\"Unique sequences: {len(unique_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5Z1Ywo-mU9q"
   },
   "outputs": [],
   "source": [
    "def filter_sequences(input_file, output_file, max_length=100):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        write_sequence = False\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                if write_sequence:\n",
    "                    outfile.write(sequence_header + sequence_data)\n",
    "                sequence_header = line\n",
    "                sequence_data = ''\n",
    "                write_sequence = False  # Reset for next sequence\n",
    "            else:\n",
    "                sequence_data += line\n",
    "                if len(sequence_data.replace('\\n', '')) <= max_length:\n",
    "                    write_sequence = True\n",
    "                else:\n",
    "                    write_sequence = False\n",
    "\n",
    "        # Check last sequence\n",
    "        if write_sequence:\n",
    "            outfile.write(sequence_header + sequence_data)\n",
    "\n",
    "input_filename = 'AFP_clustered.fasta'\n",
    "output_filename = 'AFP_CD-hit.fasta'\n",
    "filter_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5qqxCNwm8BW"
   },
   "outputs": [],
   "source": [
    "def renumber_sequences(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        counter = 1\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                outfile.write(f'>Sequence_{counter}\\n')\n",
    "                counter += 1\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_filename = 'AFP_CD-hit.fasta'  \n",
    "output_filename = 'AFP_renumbered.fasta'  \n",
    "renumber_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBHmxnQzo6b6"
   },
   "source": [
    "## 阴性样本cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu3deDiwqtNb"
   },
   "outputs": [],
   "source": [
    "def convert_to_fasta(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        sequences = file.readlines()\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            sequence = sequence.strip()  \n",
    "            file.write(f\">Sequence_{i + 1}\\n{sequence}\\n\")\n",
    "\n",
    "convert_to_fasta('Non_AFP.txt', 'Non_AFP.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QlQDfafAo9Vf"
   },
   "outputs": [],
   "source": [
    "!cd-hit -i Non_AFP.fasta -o Non_AFP_clustered.fasta -c 0.8 -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDYq6rtxo_zs"
   },
   "outputs": [],
   "source": [
    "def filter_sequences(input_file, output_file, max_length=100):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        write_sequence = False\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                if write_sequence:\n",
    "                    outfile.write(sequence_header + sequence_data)\n",
    "                sequence_header = line\n",
    "                sequence_data = ''\n",
    "                write_sequence = False  # Reset for next sequence\n",
    "            else:\n",
    "                sequence_data += line\n",
    "                if len(sequence_data.replace('\\n', '')) <= max_length:\n",
    "                    write_sequence = True\n",
    "                else:\n",
    "                    write_sequence = False\n",
    "\n",
    "        # Check last sequence\n",
    "        if write_sequence:\n",
    "            outfile.write(sequence_header + sequence_data)\n",
    "\n",
    "input_filename = 'Non_AFP_clustered.fasta'\n",
    "output_filename = 'Non_AFP_CD-hit.fasta'\n",
    "filter_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPXwHb7no_2K"
   },
   "outputs": [],
   "source": [
    "def renumber_sequences(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        counter = 1\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                outfile.write(f'>Sequence_{counter}\\n')\n",
    "                counter += 1\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_filename = 'Non_AFP_CD-hit.fasta' \n",
    "output_filename = 'Non_AFP_renumbered.fasta' \n",
    "renumber_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Za8fI8fov8b"
   },
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5806,
     "status": "ok",
     "timestamp": 1753501578999,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "KI0I5tCstwMz",
    "outputId": "a4ac182d-fd29-4ce0-b3a5-266b77ac098f"
   },
   "outputs": [],
   "source": [
    "pip install Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRHQxHp6oyrJ"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_sequences(file_path, label):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        labels.append(label)  \n",
    "    return sequences, labels\n",
    "\n",
    "def balance_and_split(sequences, labels):\n",
    "    df = pd.DataFrame({\n",
    "        'sequence': sequences,\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "    print(f\"原始数据总量: {df.shape[0]}\")\n",
    "    print(f\"各类样本数量：\\n{df['label'].value_counts()}\")\n",
    "\n",
    "    # 进行欠采样\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = rus.fit_resample(df[['sequence']], df['label'])\n",
    "\n",
    "    print(f\"欠采样后数据总量: {X_res.shape[0]}\")\n",
    "    print(f\"欠采样后各类样本数量：\\n{pd.Series(y_res).value_counts()}\")\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"训练集数量: {X_train.shape[0]}\")\n",
    "    print(f\"测试集数量: {X_test.shape[0]}\")\n",
    "\n",
    "    train_df = pd.DataFrame(X_train, columns=['sequence'])\n",
    "    train_df['label'] = y_train\n",
    "    test_df = pd.DataFrame(X_test, columns=['sequence'])\n",
    "    test_df['label'] = y_test\n",
    "\n",
    "    train_df.to_csv('train_dataset.csv', index=False)\n",
    "    test_df.to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "pos_sequences, pos_labels = load_sequences('AFP_renumbered.fasta', 1)\n",
    "neg_sequences, neg_labels = load_sequences('Non_AFP_renumbered.fasta', 0)\n",
    "\n",
    "print(f\"阳性样本数量: {len(pos_sequences)}\")\n",
    "print(f\"阴性样本数量: {len(neg_sequences)}\")\n",
    "\n",
    "all_sequences = pos_sequences + neg_sequences\n",
    "all_labels = pos_labels + neg_labels\n",
    "\n",
    "train_df, test_df = balance_and_split(all_sequences, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz6lWj8Lx6a_"
   },
   "source": [
    "## 去除非20个标准氨基酸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1757042768681,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "oOlms3L9x-Ct",
    "outputId": "9fa8a027-9033-4f5c-f0ab-ee700f4a62a5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def replace_non_standard_amino_acids(seq):\n",
    "\n",
    "    replacements = {'B': 'D', 'Z': 'E', 'X': 'A', 'J': 'L', 'U': 'C', 'O': 'K'}\n",
    "    for old, new in replacements.items():\n",
    "        seq = seq.replace(old, new)\n",
    "    return seq\n",
    "\n",
    "def load_and_preprocess(file_path, output_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['sequence'] = df['sequence'].apply(replace_non_standard_amino_acids)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "train_df = load_and_preprocess('train_dataset.csv', 'processed_train_dataset.csv')\n",
    "test_df = load_and_preprocess('test_dataset.csv', 'processed_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUJ__KTOivXB"
   },
   "outputs": [],
   "source": [
    "def renumber_sequences(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        counter = 1\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                outfile.write(f'>Sequence_{counter}\\n')\n",
    "                counter += 1\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "input_filename = 'processed_test_dataset.csv'\n",
    "output_filename = 'ColabFold_test_dataset.csv'\n",
    "renumber_sequences(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LADKYcKABvi8"
   },
   "source": [
    "## 分别划分成训练集阳性，训练集阴性，测试集阳性，测试集阴性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taWjz7KIB0a2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def to_fasta(df, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for index, row in df.iterrows():\n",
    "            f.write(f\">{index}\\n{row['sequence']}\\n\")\n",
    "\n",
    "def process_and_save(data_path, output_prefix):\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    pos = df[df['label'] == 1]\n",
    "    neg = df[df['label'] == 0]\n",
    "\n",
    "    to_fasta(pos, f'{output_prefix}_pos.fasta')\n",
    "    to_fasta(neg, f'{output_prefix}_neg.fasta')\n",
    "\n",
    "train_path = 'processed_train_dataset.csv'\n",
    "test_path = 'processed_test_dataset.csv'\n",
    "\n",
    "process_and_save(train_path, 'Colab_train')\n",
    "process_and_save(test_path, 'Colab_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RnxO1AX2u46"
   },
   "source": [
    "# 结构信息\n",
    "*   加载pdb文件\n",
    "*   使用pdb文件提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQqE1ofqsviH"
   },
   "source": [
    "## 加载pdb文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1753502154056,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "-8QFQR61s4ED",
    "outputId": "c7f28949-bb85-4eb3-c6ed-8acfc36ba55d"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "zip_folder_path = '/content/drive/MyDrive/AFP_work/result/test_neg'\n",
    "extract_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'  # 解压后的路径\n",
    "\n",
    "os.makedirs(extract_folder_path, exist_ok=True)\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "for filename in os.listdir(zip_folder_path):\n",
    "    if filename.endswith('.zip'):\n",
    "        zip_path = os.path.join(zip_folder_path, filename)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for pdb_file in zip_ref.namelist():\n",
    "                if pdb_file.endswith('.pdb') and 'relaxed_rank_001' in pdb_file:\n",
    "                    zip_ref.extract(pdb_file, extract_folder_path)\n",
    "                    print(f'解压缩完成：{pdb_file} 从 {filename} 到 {extract_folder_path}')\n",
    "                    file_count += 1\n",
    "\n",
    "print(f'总共解压了 {file_count} 个文件')\n",
    "\n",
    "pdb_files = [f for f in os.listdir(extract_folder_path) if f.endswith('.pdb')]\n",
    "print(f'解压后的文件夹中共有 {len(pdb_files)} 个 PDB 文件')\n",
    "\n",
    "# 删除包含 \"unrelaxed\" 的 PDB 文件\n",
    "for pdb_file in pdb_files:\n",
    "    if 'unrelaxed' in pdb_file:\n",
    "        file_path = os.path.join(extract_folder_path, pdb_file)\n",
    "        os.remove(file_path)\n",
    "        print(f'已删除文件：{file_path}')\n",
    "\n",
    "remaining_pdb_files = [f for f in os.listdir(extract_folder_path) if f.endswith('.pdb')]\n",
    "print(f'删除 unrelaxed 文件后，文件夹中共有 {len(remaining_pdb_files)} 个 PDB 文件')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1754101167178,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "UWRjQiwMTlms",
    "outputId": "3465b49c-86cb-4934-bf02-afdb66fb122a"
   },
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "\n",
    "parser = PDBParser(QUIET=True)\n",
    "ppb = PPBuilder()\n",
    "\n",
    "def extract_sequence_from_pdb(pdb_path):\n",
    "    structure = parser.get_structure('', pdb_path)\n",
    "    sequence = \"\"\n",
    "    for pp in ppb.build_peptides(structure):\n",
    "        sequence += str(pp.get_sequence())\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1754101179783,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "kf2BXidF2290",
    "outputId": "69f64539-8b4d-48ef-fe47-65086479005b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# 四个文件夹路径\n",
    "train_pos_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "test_pos_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "train_neg_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_neg_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "\n",
    "train_pdb_count = len([f for f in os.listdir(train_pos_path) if f.endswith('.pdb')])\n",
    "test_pdb_count = len([f for f in os.listdir(test_pos_path) if f.endswith('.pdb')])\n",
    "train_neg_pdb_count = len([f for f in os.listdir(train_neg_path) if f.endswith('.pdb')])\n",
    "test_neg_pdb_count = len([f for f in os.listdir(test_neg_path) if f.endswith('.pdb')])\n",
    "\n",
    "print(f'训练集中的 PDB 文件数量（阳性）：{train_pdb_count}')\n",
    "print(f'测试集中的 PDB 文件数量（阳性）：{test_pdb_count}')\n",
    "print(f'训练集中的 PDB 文件数量（阴性）：{train_neg_pdb_count}')\n",
    "print(f'测试集中的 PDB 文件数量（阴性）：{test_neg_pdb_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0LT6AZYEBL3"
   },
   "source": [
    "## 使用pdb文件提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10505,
     "status": "error",
     "timestamp": 1754102535596,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "xaF1V0LzEElS",
    "outputId": "5e52bcad-f684-4966-d61b-cab31de5d693"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "output_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features'\n",
    "\n",
    "os.makedirs(output_train_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_train_neg_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_neg_folder_path, exist_ok=True)\n",
    "\n",
    "# 提取训练集和测试集中的所有 PDB 文件\n",
    "train_pos_pdb_files = [f for f in os.listdir(train_pos_folder_path) if f.endswith('.pdb')]\n",
    "train_neg_pdb_files = [f for f in os.listdir(train_neg_folder_path) if f.endswith('.pdb')]\n",
    "test_pos_pdb_files = [f for f in os.listdir(test_pos_folder_path) if f.endswith('.pdb')]\n",
    "test_neg_pdb_files = [f for f in os.listdir(test_neg_folder_path) if f.endswith('.pdb')]\n",
    "\n",
    "# 提取选中的 PDB 文件的结构信息\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 优化 PDB 文件处理函数\n",
    "def process_pdb_file(pdb_path):\n",
    "    structure = parser.get_structure('', pdb_path)\n",
    "    residues = [residue for residue in structure.get_residues() if 'CA' in residue]\n",
    "    num_residues = len(residues)\n",
    "\n",
    "    # 提取位置特征、方向特征和旋转特征\n",
    "    positions = np.array([residue['CA'].get_coord() for residue in residues], dtype=np.float64)\n",
    "    edges = []\n",
    "    directions = []\n",
    "    rotations = []\n",
    "\n",
    "    # 计算接触图和附加特征\n",
    "    for i in range(num_residues):\n",
    "        for j in range(i + 1, num_residues):\n",
    "            distance = np.linalg.norm(positions[i] - positions[j])\n",
    "            if distance < 10.0:  # 阈值为10Å来定义接触\n",
    "                edges.append([i, j])\n",
    "                direction = positions[j] - positions[i]\n",
    "                norm = np.linalg.norm(direction)\n",
    "                if norm != 0:\n",
    "                    directions.append(direction / norm)\n",
    "                    rotations.append(float(np.arctan2(direction[1], direction[0])))\n",
    "\n",
    "    return positions, edges, directions, rotations\n",
    "\n",
    "# 处理训练集中的 PDB 文件并添加标签\n",
    "print(f'训练集中的 PDB 文件数量 (阳性): {len(train_pos_pdb_files)}')\n",
    "print(f'训练集中的 PDB 文件数量 (阴性): {len(train_neg_pdb_files)}')\n",
    "\n",
    "for pdb_file in train_pos_pdb_files:\n",
    "    pdb_path = os.path.join(train_pos_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 1  # 阳性样本标签\n",
    "    }\n",
    "    # 保存特征到文件\n",
    "    output_file_path = os.path.join(output_train_pos_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json') if features['label'] == 1 else os.path.join(output_train_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "for pdb_file in train_neg_pdb_files:\n",
    "    pdb_path = os.path.join(train_neg_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 0  # 阴性样本标签\n",
    "    }\n",
    "    # 保存特征到文件\n",
    "    output_file_path = os.path.join(output_train_neg_folder_path if features['label'] == 1 else output_test_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "# 处理测试集中的 PDB 文件并添加标签\n",
    "print(f'测试集中的 PDB 文件数量 (阳性): {len(test_pos_pdb_files)}')\n",
    "print(f'测试集中的 PDB 文件数量 (阴性): {len(test_neg_pdb_files)}')\n",
    "\n",
    "for pdb_file in test_pos_pdb_files:\n",
    "    pdb_path = os.path.join(test_pos_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 1  # 阳性样本标签\n",
    "    }\n",
    "    # 保存特征到文件\n",
    "    output_file_path = os.path.join(output_test_pos_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json') if features['label'] == 1 else os.path.join(output_test_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "for pdb_file in test_neg_pdb_files:\n",
    "    pdb_path = os.path.join(test_neg_folder_path, pdb_file)\n",
    "    positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "    features = {\n",
    "        \"node_features\": positions.tolist(),\n",
    "        \"edge_features\": {\n",
    "            \"edges\": edges,\n",
    "            \"directions\": [d.tolist() for d in directions],\n",
    "            \"rotations\": [float(rot) for rot in rotations]\n",
    "        },\n",
    "        \"label\": 0  # 阴性样本标签\n",
    "    }\n",
    "    # 保存特征到文件\n",
    "    output_file_path = os.path.join(output_test_neg_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(features, output_file)\n",
    "\n",
    "# 输出保存的 JSON 文件数量\n",
    "# json_files = [f for f in os.listdir(output_folder_path) if f.endswith('.json')]\n",
    "# print(f'保存的 JSON 文件数量: {len(json_files)}')\n",
    "\n",
    "# 检查每个保存的 JSON 文件中的维度信息\n",
    "# for json_file in json_files:\n",
    "#     json_path = os.path.join(output_folder_path, json_file)\n",
    "#     with open(json_path, 'r') as file:\n",
    "#         data = json.load(file)\n",
    "#         print(f'文件: {json_file}')\n",
    "#         print(f'节点特征数量: {len(data[\"node_features\"])}, 位置特征维度: {len(data[\"node_features\"][0])}')\n",
    "#         print(f'边特征数量: {len(data[\"edge_features\"][\"edges\"])}, 方向特征数量: {len(data[\"edge_features\"][\"directions\"])}, 旋转特征数量: {len(data[\"edge_features\"][\"rotations\"])}')\n",
    "\n",
    "\n",
    "output_train_neg_folder_path\n",
    "\n",
    "# 输出保存的 JSON 文件数量\n",
    "json_files = [f for f in os.listdir(output_train_neg_folder_path) if f.endswith('.json')]\n",
    "print(f'output_train_neg_folder_path 保存的 JSON 文件数量: {len(json_files)}')\n",
    "\n",
    "# 检查每个保存的 JSON 文件中的维度信息\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(output_train_neg_folder_path, json_file)\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        print(f'文件: {json_file}')\n",
    "        print(f'节点特征数量: {len(data[\"node_features\"])}, 位置特征维度: {len(data[\"node_features\"][0])}')\n",
    "        print(f'边特征数量: {len(data[\"edge_features\"][\"edges\"])}, 方向特征数量: {len(data[\"edge_features\"][\"directions\"])}, 旋转特征数量: {len(data[\"edge_features\"][\"rotations\"])}')\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"总处理时间: {end_time - start_time:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2bQyS6_wILJ"
   },
   "source": [
    "# ESM-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9004,
     "status": "ok",
     "timestamp": 1754191779238,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "qmCgs2eb6Mjq",
    "outputId": "be6b644f-bcca-4899-8578-9c1e2d07d56e"
   },
   "outputs": [],
   "source": [
    "pip install esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WEZikzFcP-h",
    "outputId": "850ecb7f-dde9-435a-f9f4-7c06afe9c5e3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import SeqIO\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "# 定义 FASTA 文件路径\n",
    "#train_pos_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_train_pos.fasta'\n",
    "#train_neg_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_train_neg.fasta'\n",
    "test_pos_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_test_pos.fasta'\n",
    "#test_neg_seq_file = '/content/drive/MyDrive/AFP_work/seq/Colab_test_neg.fasta'\n",
    "\n",
    "output_feature_path = '/content/drive/MyDrive/esmc_600_test_pos'\n",
    "\n",
    "os.makedirs(output_feature_path, exist_ok=True)\n",
    "\n",
    "def read_fasta_sequences(fasta_file):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        seq_id = record.id\n",
    "        sequence = str(record.seq).replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        sequences.append((seq_id, sequence))\n",
    "    return sequences\n",
    "\n",
    "def save_features(features, output_dir):\n",
    "    for seq_id, feature_dict in features.items():\n",
    "        logits = feature_dict['logits']\n",
    "        embeddings = feature_dict['embeddings']\n",
    "\n",
    "        # 定义文件路径\n",
    "        logits_path = os.path.join(output_dir, f\"{seq_id}_logits.npy\")\n",
    "        embeddings_path = os.path.join(output_dir, f\"{seq_id}_embeddings.npy\")\n",
    "\n",
    "        # 保存 logits 和 embeddings\n",
    "        np.save(logits_path, logits)\n",
    "        np.save(embeddings_path, embeddings)\n",
    "\n",
    "def extract_features_individual(client, sequences):\n",
    "    features = {}\n",
    "    for seq_id, seq in tqdm(sequences, desc=\"提取特征\"):\n",
    "        try:\n",
    "            # 创建 ESMProtein 实例\n",
    "            protein = ESMProtein(sequence=seq)\n",
    "\n",
    "            # 编码蛋白质序列\n",
    "            protein_tensor = client.encode(protein)\n",
    "\n",
    "            # 获取 logits 和 embeddings\n",
    "            logits_output = client.logits(\n",
    "                protein_tensor,\n",
    "                LogitsConfig(sequence=True, return_embeddings=True)\n",
    "            )\n",
    "\n",
    "            logits = logits_output.logits  # 需要根据实际情况修改\n",
    "            embeddings = logits_output.embeddings  # 需要根据实际情况修改\n",
    "\n",
    "            # 检查 logits 和 embeddings 的类型\n",
    "            if isinstance(logits, torch.Tensor):\n",
    "                logits = logits.cpu().numpy()\n",
    "            elif isinstance(logits, np.ndarray):\n",
    "                pass  # 已经是 NumPy 数组\n",
    "            else:\n",
    "                # 如果不是 tensor 或 ndarray，尝试其他转换\n",
    "                logits = np.array(logits)\n",
    "\n",
    "            if isinstance(embeddings, torch.Tensor):\n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "            elif isinstance(embeddings, np.ndarray):\n",
    "                pass  # 已经是 NumPy 数组\n",
    "            else:\n",
    "                # 如果不是 tensor 或 ndarray，尝试其他转换\n",
    "                embeddings = np.array(embeddings)\n",
    "\n",
    "            # 保存特征\n",
    "            features[seq_id] = {\n",
    "                'logits': logits,\n",
    "                'embeddings': embeddings\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"处理序列 {seq_id} 时出错: {e}\")\n",
    "            continue\n",
    "    return features\n",
    "\n",
    "# 初始化 ESMC 客户端\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "client = ESMC.from_pretrained(\"esmc_600m\").to(device)\n",
    "client.eval()  # 设置为评估模式\n",
    "\n",
    "fasta_files = [\n",
    "    #train_pos_seq_file,\n",
    "    #train_neg_seq_file,\n",
    "    test_pos_seq_file,\n",
    "    #test_neg_seq_file\n",
    "]\n",
    "\n",
    "for fasta_file in fasta_files:\n",
    "    print(f\"正在处理文件: {fasta_file}\")\n",
    "\n",
    "    # 读取序列\n",
    "    sequences = read_fasta_sequences(fasta_file)\n",
    "    print(f\"序列数量: {len(sequences)}\")\n",
    "\n",
    "    # 提取特征（逐条处理）\n",
    "    features = extract_features_individual(client, sequences)\n",
    "\n",
    "    # 保存特征\n",
    "    save_features(features, output_feature_path)\n",
    "\n",
    "    print(f\"特征已保存: {fasta_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW7S96HwO3Vg"
   },
   "source": [
    "# 处理序列结构信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrR3SQWpO5qt"
   },
   "source": [
    "## 处理序列信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16799,
     "status": "ok",
     "timestamp": 1754199247054,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "i_J1oT45O1XL",
    "outputId": "f49d5193-029a-4e64-9aed-6ffe5dfa2eab"
   },
   "outputs": [],
   "source": [
    "# 2. 导入必要的库\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 3. 定义特征输出文件夹列表\n",
    "feature_dirs = [\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',  # 请确认此路径是否正确\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "    # 如果有其他文件夹，如 esmc_600_test_pos，请在此添加\n",
    "]\n",
    "\n",
    "# 4. 定义函数以查看汇总文件的维度\n",
    "def inspect_combined_files(feature_dirs):\n",
    "    \"\"\"\n",
    "    遍历每个特征文件夹，加载并打印 combined_logits.npy 和 combined_embeddings.npy 的维度。\n",
    "\n",
    "    参数:\n",
    "        feature_dirs (List[str]): 特征文件夹路径列表。\n",
    "    \"\"\"\n",
    "    for feature_dir in feature_dirs:\n",
    "        print(f\"正在处理文件夹: {feature_dir}\")\n",
    "\n",
    "        # 检查文件夹是否存在\n",
    "        if not os.path.isdir(feature_dir):\n",
    "            print(f\"文件夹 '{feature_dir}' 不存在，跳过。\\n\")\n",
    "            continue\n",
    "\n",
    "        # 定义 combined_logits.npy 和 combined_embeddings.npy 的路径\n",
    "        logits_path = os.path.join(feature_dir, 'combined_logits.npy')\n",
    "        embeddings_path = os.path.join(feature_dir, 'combined_embeddings.npy')\n",
    "\n",
    "        # 检查 combined_logits.npy 是否存在\n",
    "        if os.path.isfile(logits_path):\n",
    "            try:\n",
    "                combined_logits = np.load(logits_path, allow_pickle=True)\n",
    "                print(f\" - {os.path.basename(logits_path)} 的形状: {combined_logits.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\" - 加载 {os.path.basename(logits_path)} 时出错: {e}\")\n",
    "        else:\n",
    "            print(f\" - {os.path.basename(logits_path)} 不存在。\")\n",
    "\n",
    "        # 检查 combined_embeddings.npy 是否存在\n",
    "        if os.path.isfile(embeddings_path):\n",
    "            try:\n",
    "                combined_embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "                print(f\" - {os.path.basename(embeddings_path)} 的形状: {combined_embeddings.shape}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\" - 加载 {os.path.basename(embeddings_path)} 时出错: {e}\\n\")\n",
    "        else:\n",
    "            print(f\" - {os.path.basename(embeddings_path)} 不存在。\\n\")\n",
    "\n",
    "        # mxlin添加\n",
    "        sample = combined_embeddings[0]  # 取第一个样本\n",
    "        print(\"数据级别：\",sample.shape)  # 输出 (1152,)，则是序列级别\n",
    "\n",
    "# 5. 运行函数以查看维度\n",
    "inspect_combined_files(feature_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1754199247189,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "uSFwZslPPyip",
    "outputId": "e3407317-896e-43bf-f9f8-be1b19590f6c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "feature_dirs = [\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',\n",
    "    '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "]\n",
    "\n",
    "# 统计每个特征文件夹中 logits 和 embeddings 文件的数量，并验证是否匹配。\n",
    "def count_logit_embedding_files(feature_dirs):\n",
    "    for feature_dir in feature_dirs:\n",
    "\n",
    "        if not os.path.isdir(feature_dir):\n",
    "            continue\n",
    "\n",
    "        logits_files = sorted(glob.glob(os.path.join(feature_dir, '*_logits.npy')))\n",
    "        embeddings_files = sorted(glob.glob(os.path.join(feature_dir, '*_embeddings.npy')))\n",
    "\n",
    "        num_logits = len(logits_files)\n",
    "        num_embeddings = len(embeddings_files)\n",
    "\n",
    "        if num_logits == num_embeddings:\n",
    "            print(f\"数量一致。\\n\")\n",
    "        else:\n",
    "            print(f\"数量不一致。\")\n",
    "            print(f\"- logits 文件数量: {num_logits}\")\n",
    "            print(f\"- embeddings 文件数量: {num_embeddings}\")\n",
    "\n",
    "            # 找出缺失或多余的文件\n",
    "            logits_indices = set([os.path.basename(f).split('_')[0] for f in logits_files])\n",
    "            embeddings_indices = set([os.path.basename(f).split('_')[0] for f in embeddings_files])\n",
    "\n",
    "            missing_in_embeddings = logits_indices - embeddings_indices\n",
    "            missing_in_logits = embeddings_indices - logits_indices\n",
    "\n",
    "            if missing_in_embeddings:\n",
    "                print(f\"在 embeddings 文件夹中缺失: {sorted(missing_in_embeddings)}\")\n",
    "            if missing_in_logits:\n",
    "                print(f\"在 logits 文件夹中缺失: {sorted(missing_in_logits)}\")\n",
    "            print()\n",
    "\n",
    "count_logit_embedding_files(feature_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1270,
     "status": "ok",
     "timestamp": 1754199254991,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "3GLMRuQjP5vC",
    "outputId": "694847e4-d1e1-4865-ac25-22d488c4466a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在检查文件夹: /content/drive/MyDrive/AFP_work/esmc_600_train_pos\n",
      "  📊 combined_logits.npy 的形状: (1200, 1)\n",
      "  📄 combined_logits.npy 的前5行数据：\n",
      "[[ForwardTrackData(sequence=tensor([[[-21.2500, -21.1250, -21.2500,  ..., -21.2500, -21.2500, -21.2500],\n",
      "           [-26.6250, -26.6250, -26.6250,  ..., -26.6250, -26.6250, -26.6250],\n",
      "           [-28.3750, -28.2500, -28.3750,  ..., -28.3750, -28.3750, -28.2500],\n",
      "           ...,\n",
      "           [-24.7500, -24.6250, -24.7500,  ..., -24.7500, -24.7500, -24.7500],\n",
      "           [-21.1250, -21.0000, -21.1250,  ..., -21.1250, -21.1250, -21.1250],\n",
      "           [-20.0000, -19.8750, -20.0000,  ..., -19.8750, -20.0000, -20.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.7500, -22.6250, -22.7500,  ..., -22.7500, -22.7500, -22.7500],\n",
      "           [-26.3750, -26.2500, -26.3750,  ..., -26.2500, -26.3750, -26.3750],\n",
      "           [-24.2500, -24.2500, -24.3750,  ..., -24.2500, -24.2500, -24.2500],\n",
      "           ...,\n",
      "           [-24.8750, -24.7500, -24.8750,  ..., -24.8750, -24.8750, -24.8750],\n",
      "           [-22.0000, -21.8750, -22.0000,  ..., -22.0000, -22.0000, -22.0000],\n",
      "           [-22.1250, -21.8750, -22.0000,  ..., -22.0000, -22.1250, -22.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-23.0000, -22.8750, -23.0000,  ..., -23.0000, -23.0000, -23.0000],\n",
      "           [-30.2500, -30.1250, -30.2500,  ..., -30.2500, -30.3750, -30.2500],\n",
      "           [-29.2500, -29.1250, -29.2500,  ..., -29.2500, -29.3750, -29.2500],\n",
      "           ...,\n",
      "           [-29.6250, -29.5000, -29.6250,  ..., -29.6250, -29.6250, -29.6250],\n",
      "           [-27.0000, -26.7500, -26.8750,  ..., -26.8750, -27.0000, -26.8750],\n",
      "           [-24.5000, -24.5000, -24.5000,  ..., -24.5000, -24.6250, -24.5000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.8750, -22.7500, -22.8750,  ..., -22.8750, -23.0000, -23.0000],\n",
      "           [-25.8750, -25.7500, -25.8750,  ..., -25.7500, -25.8750, -25.8750],\n",
      "           [-26.3750, -26.2500, -26.3750,  ..., -26.2500, -26.3750, -26.3750],\n",
      "           ...,\n",
      "           [-27.2500, -27.1250, -27.2500,  ..., -27.1250, -27.2500, -27.2500],\n",
      "           [-24.1250, -24.0000, -24.1250,  ..., -24.1250, -24.1250, -24.1250],\n",
      "           [-23.6250, -23.5000, -23.6250,  ..., -23.5000, -23.6250, -23.6250]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-20.8750, -20.7500, -20.8750,  ..., -20.8750, -20.8750, -20.8750],\n",
      "           [-23.7500, -23.5000, -23.5000,  ..., -23.6250, -23.7500, -23.7500],\n",
      "           [-24.8750, -24.7500, -24.7500,  ..., -24.8750, -24.8750, -24.7500],\n",
      "           ...,\n",
      "           [-26.1250, -26.1250, -26.2500,  ..., -26.1250, -26.2500, -26.2500],\n",
      "           [-17.2500, -17.1250, -17.2500,  ..., -17.2500, -17.2500, -17.1250],\n",
      "           [-18.5000, -18.3750, -18.5000,  ..., -18.5000, -18.6250, -18.5000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]]\n",
      "\n",
      "  📊 /content/drive/MyDrive/AFP_work/esmc_600_train_pos combined_embeddings.npy 的形状: (1200, 1152)\n",
      "  📄 /content/drive/MyDrive/AFP_work/esmc_600_train_pos combined_embeddings.npy 的前5行数据：\n",
      "[[-0.00485136  0.01375214 -0.01221031 ... -0.00554397  0.00420806\n",
      "   0.00168132]\n",
      " [-0.00074414  0.02556396 -0.01695777 ... -0.00902656  0.00836704\n",
      "   0.01187631]\n",
      " [-0.01295047  0.02892382 -0.02908008 ... -0.0041075  -0.00061104\n",
      "   0.00659786]\n",
      " [-0.00783114  0.026164   -0.02013981 ... -0.0024441  -0.0033889\n",
      "   0.00785554]\n",
      " [ 0.00682404 -0.01013107 -0.01611771 ... -0.01646785 -0.01872984\n",
      "   0.00195817]]\n",
      "\n",
      "正在检查文件夹: /content/drive/MyDrive/AFP_work/esmc_600_train_neg\n",
      "  📊 combined_logits.npy 的形状: (1200, 1)\n",
      "  📄 combined_logits.npy 的前5行数据：\n",
      "[[ForwardTrackData(sequence=tensor([[[-23.5000, -23.3750, -23.5000,  ..., -23.5000, -23.5000, -23.6250],\n",
      "           [-23.3750, -23.2500, -23.3750,  ..., -23.3750, -23.3750, -23.3750],\n",
      "           [-26.6250, -26.5000, -26.6250,  ..., -26.6250, -26.6250, -26.6250],\n",
      "           ...,\n",
      "           [-21.3750, -21.2500, -21.3750,  ..., -21.3750, -21.3750, -21.3750],\n",
      "           [-17.2500, -17.1250, -17.2500,  ..., -17.2500, -17.2500, -17.2500],\n",
      "           [-20.1250, -20.0000, -20.0000,  ..., -20.1250, -20.0000, -20.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.5000, -22.3750, -22.6250,  ..., -22.5000, -22.5000, -22.6250],\n",
      "           [-23.2500, -23.2500, -23.2500,  ..., -23.2500, -23.2500, -23.2500],\n",
      "           [-24.6250, -24.6250, -24.6250,  ..., -24.6250, -24.6250, -24.6250],\n",
      "           ...,\n",
      "           [-21.7500, -21.6250, -21.7500,  ..., -21.7500, -21.7500, -21.7500],\n",
      "           [-20.1250, -20.1250, -20.1250,  ..., -20.1250, -20.2500, -20.1250],\n",
      "           [-22.3750, -22.2500, -22.3750,  ..., -22.2500, -22.3750, -22.3750]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-23.5000, -23.3750, -23.5000,  ..., -23.5000, -23.5000, -23.6250],\n",
      "           [-27.2500, -27.0000, -27.1250,  ..., -27.1250, -27.2500, -27.1250],\n",
      "           [-27.5000, -27.3750, -27.5000,  ..., -27.3750, -27.5000, -27.5000],\n",
      "           ...,\n",
      "           [-28.2500, -28.1250, -28.2500,  ..., -28.1250, -28.2500, -28.2500],\n",
      "           [-23.2500, -23.1250, -23.2500,  ..., -23.1250, -23.2500, -23.2500],\n",
      "           [-23.6250, -23.5000, -23.6250,  ..., -23.5000, -23.6250, -23.6250]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-21.6250, -21.5000, -21.6250,  ..., -21.6250, -21.6250, -21.6250],\n",
      "           [-25.0000, -24.8750, -25.0000,  ..., -25.0000, -25.0000, -25.0000],\n",
      "           [-26.7500, -26.6250, -26.7500,  ..., -26.7500, -26.7500, -26.6250],\n",
      "           ...,\n",
      "           [-25.1250, -25.0000, -25.1250,  ..., -25.1250, -25.2500, -25.1250],\n",
      "           [-20.7500, -20.6250, -20.7500,  ..., -20.6250, -20.7500, -20.7500],\n",
      "           [-21.7500, -21.7500, -21.7500,  ..., -21.6250, -21.8750, -21.8750]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-23.0000, -22.8750, -23.0000,  ..., -22.8750, -23.0000, -23.0000],\n",
      "           [-27.6250, -27.5000, -27.6250,  ..., -27.5000, -27.6250, -27.6250],\n",
      "           [-27.3750, -27.1250, -27.3750,  ..., -27.2500, -27.3750, -27.2500],\n",
      "           ...,\n",
      "           [-27.6250, -27.5000, -27.6250,  ..., -27.5000, -27.6250, -27.5000],\n",
      "           [-24.1250, -24.0000, -24.1250,  ..., -24.0000, -24.1250, -24.1250],\n",
      "           [-22.6250, -22.3750, -22.5000,  ..., -22.5000, -22.6250, -22.5000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]]\n",
      "\n",
      "  📊 /content/drive/MyDrive/AFP_work/esmc_600_train_neg combined_embeddings.npy 的形状: (1200, 1152)\n",
      "  📄 /content/drive/MyDrive/AFP_work/esmc_600_train_neg combined_embeddings.npy 的前5行数据：\n",
      "[[-0.01458055  0.02122218 -0.02280448 ... -0.02389562 -0.01433752\n",
      "   0.01356946]\n",
      " [-0.00363107  0.0026253   0.00232742 ...  0.00368061  0.00893849\n",
      "   0.00313839]\n",
      " [ 0.00705204  0.03198364 -0.02578153 ...  0.0242763   0.0118436\n",
      "  -0.00296395]\n",
      " [-0.00482746  0.0018164   0.00409979 ... -0.00863889  0.0098847\n",
      "   0.00861327]\n",
      " [ 0.01053481  0.04374865 -0.02046082 ...  0.01394143 -0.00197555\n",
      "   0.00511242]]\n",
      "\n",
      "正在检查文件夹: /content/drive/MyDrive/AFP_work/esmc_600_test_pos\n",
      "  📊 combined_logits.npy 的形状: (308, 1)\n",
      "  📄 combined_logits.npy 的前5行数据：\n",
      "[[ForwardTrackData(sequence=tensor([[[-22.1250, -22.0000, -22.0000,  ..., -22.1250, -22.1250, -22.1250],\n",
      "           [-25.8750, -25.7500, -25.7500,  ..., -25.8750, -25.8750, -25.8750],\n",
      "           [-24.2500, -24.0000, -24.1250,  ..., -24.1250, -24.1250, -24.1250],\n",
      "           ...,\n",
      "           [-24.2500, -24.2500, -24.2500,  ..., -24.1250, -24.2500, -24.2500],\n",
      "           [-18.8750, -18.8750, -18.8750,  ..., -18.7500, -18.8750, -18.8750],\n",
      "           [-20.7500, -20.6250, -20.7500,  ..., -20.7500, -20.7500, -20.7500]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-21.5000, -21.3750, -21.5000,  ..., -21.5000, -21.6250, -21.5000],\n",
      "           [-27.3750, -27.2500, -27.3750,  ..., -27.2500, -27.3750, -27.3750],\n",
      "           [-22.7500, -22.6250, -22.7500,  ..., -22.7500, -22.7500, -22.7500],\n",
      "           ...,\n",
      "           [-26.2500, -26.1250, -26.1250,  ..., -26.1250, -26.2500, -26.1250],\n",
      "           [-21.7500, -21.6250, -21.6250,  ..., -21.6250, -21.6250, -21.6250],\n",
      "           [-19.5000, -19.3750, -19.5000,  ..., -19.5000, -19.5000, -19.5000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.5000, -22.2500, -22.3750,  ..., -22.3750, -22.5000, -22.3750],\n",
      "           [-24.1250, -23.8750, -23.7500,  ..., -23.8750, -24.0000, -24.0000],\n",
      "           [-25.6250, -25.6250, -25.6250,  ..., -25.6250, -25.6250, -25.6250],\n",
      "           ...,\n",
      "           [-26.6250, -26.5000, -26.6250,  ..., -26.6250, -26.7500, -26.5000],\n",
      "           [-17.0000, -16.8750, -17.0000,  ..., -16.8750, -17.0000, -16.8750],\n",
      "           [-19.2500, -19.1250, -19.2500,  ..., -19.1250, -19.2500, -19.2500]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.0000, -21.8750, -22.0000,  ..., -21.8750, -22.0000, -22.0000],\n",
      "           [-25.2500, -25.1250, -25.2500,  ..., -25.1250, -25.2500, -25.2500],\n",
      "           [-26.0000, -25.8750, -26.0000,  ..., -26.0000, -26.1250, -26.0000],\n",
      "           ...,\n",
      "           [-25.5000, -25.3750, -25.5000,  ..., -25.5000, -25.5000, -25.5000],\n",
      "           [-18.5000, -18.5000, -18.5000,  ..., -18.5000, -18.5000, -18.5000],\n",
      "           [-21.2500, -21.1250, -21.2500,  ..., -21.1250, -21.2500, -21.2500]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.0000, -21.7500, -22.0000,  15.8750,  17.0000,  19.3750,  21.0000,\n",
      "             17.3750,  17.7500,  17.1250,  15.2500,  14.7500,  16.0000,  17.0000,\n",
      "             18.8750,  17.3750,  11.8125,  14.3125,  17.0000,  13.8125,  15.6250,\n",
      "             13.5000,  17.1250,  16.3750,  17.0000,   0.4453,   0.5898, -12.0625,\n",
      "             -5.8438, -21.8750, -22.0000, -22.0000, -21.8750, -21.7500, -22.1250,\n",
      "            -21.8750, -22.0000, -22.0000, -21.8750, -21.8750, -21.8750, -21.7500,\n",
      "            -21.7500, -21.8750, -22.0000, -21.7500, -22.0000, -22.0000, -21.7500,\n",
      "            -22.0000, -21.8750, -21.8750, -21.6250, -21.8750, -21.7500, -22.0000,\n",
      "            -21.8750, -21.7500, -21.8750, -21.7500, -21.8750, -22.0000, -22.0000,\n",
      "            -22.0000],\n",
      "           [-22.7500, -22.6250, -22.6250,   3.0000,  16.1250,  13.6875,  11.4375,\n",
      "             14.7500,  12.2500,   7.3125,   8.5625,  10.2500,  16.7500,   6.8125,\n",
      "             11.5625,   8.8750,   8.1875,   8.9375,  22.7500,  13.3750,  14.1875,\n",
      "             10.5000,  13.0000,  11.0625,  11.6250, -18.7500, -20.0000, -21.1250,\n",
      "            -10.0625, -22.6250, -22.7500, -22.7500, -22.6250, -22.6250, -22.6250,\n",
      "            -22.5000, -22.6250, -22.5000, -22.5000, -22.6250, -22.6250, -22.6250,\n",
      "            -22.6250, -22.5000, -22.6250, -22.6250, -22.7500, -22.8750, -22.6250,\n",
      "            -22.6250, -22.5000, -22.7500, -22.6250, -22.7500, -22.6250, -22.6250,\n",
      "            -22.6250, -22.7500, -22.5000, -22.6250, -22.6250, -22.5000, -22.6250,\n",
      "            -22.7500],\n",
      "           [-21.0000, -21.0000, -21.1250,   9.6250,  27.8750,  20.0000,  16.7500,\n",
      "             22.7500,  16.5000,  11.8125,  15.7500,  16.1250,  25.5000,   9.6875,\n",
      "             17.5000,  15.0000,  14.5625,  12.8750,  24.3750,  17.0000,  21.3750,\n",
      "             15.5000,  19.3750,  13.1875,  18.8750, -24.0000, -26.6250, -22.0000,\n",
      "             -5.0000, -21.0000, -21.1250, -21.1250, -21.0000, -20.8750, -21.0000,\n",
      "            -21.0000, -21.1250, -21.0000, -21.0000, -21.0000, -21.0000, -21.0000,\n",
      "            -20.8750, -20.8750, -21.1250, -21.1250, -21.0000, -21.1250, -21.0000,\n",
      "            -21.0000, -21.0000, -21.0000, -21.0000, -21.1250, -21.2500, -21.1250,\n",
      "            -21.1250, -21.0000, -21.0000, -21.0000, -21.0000, -21.1250, -21.0000,\n",
      "            -21.1250],\n",
      "           [-19.7500, -19.6250, -19.7500,   9.6250,  15.0625,  18.0000,  21.2500,\n",
      "             12.0000,  20.8750,  13.5000,  15.8750,  16.7500,  11.1250,  14.5000,\n",
      "             24.7500,  16.0000,  17.0000,  17.6250,  12.5625,  11.9375,  12.1250,\n",
      "             16.8750,  10.7500,  10.9375,  16.6250, -20.5000, -14.8125, -19.6250,\n",
      "             -4.9375, -19.8750, -19.6250, -19.7500, -19.7500, -19.6250, -19.6250,\n",
      "            -19.6250, -19.6250, -19.7500, -19.6250, -19.7500, -19.7500, -19.6250,\n",
      "            -19.6250, -19.7500, -19.7500, -19.6250, -19.7500, -19.7500, -19.7500,\n",
      "            -19.7500, -19.7500, -19.7500, -19.5000, -19.7500, -19.7500, -19.7500,\n",
      "            -19.6250, -19.6250, -19.7500, -19.7500, -19.7500, -19.8750, -19.6250,\n",
      "            -19.6250],\n",
      "           [-18.2500, -18.1250, -18.2500,   4.8438,  20.8750,  23.1250,  17.8750,\n",
      "             21.7500,  18.8750,  14.3125,  14.2500,  20.1250,  21.5000,  12.9375,\n",
      "             14.2500,  15.3750,  16.7500,  14.9375,  18.5000,  16.0000,  20.8750,\n",
      "             17.5000,  11.9375,  10.1875,  16.7500, -27.7500, -23.5000, -25.7500,\n",
      "              0.1143, -18.2500, -18.2500, -18.2500, -18.2500, -18.0000, -18.2500,\n",
      "            -18.2500, -18.2500, -18.1250, -18.1250, -18.1250, -18.1250, -18.1250,\n",
      "            -18.1250, -18.1250, -18.1250, -18.2500, -18.2500, -18.2500, -18.1250,\n",
      "            -18.0000, -18.1250, -18.1250, -18.1250, -18.2500, -18.1250, -18.2500,\n",
      "            -18.1250, -18.1250, -18.2500, -18.2500, -18.3750, -18.2500, -18.2500,\n",
      "            -18.2500],\n",
      "           [-19.5000, -19.3750, -19.5000,   8.1250,  25.3750,  25.8750,  15.9375,\n",
      "             27.1250,  16.3750,  12.5000,  16.0000,  18.6250,  27.5000,   9.9375,\n",
      "             15.5625,  15.2500,  13.5000,  10.0625,  20.2500,  15.6250,  22.0000,\n",
      "             12.6875,  16.1250,  14.2500,  18.1250, -45.5000, -32.0000, -39.2500,\n",
      "             -0.7656, -19.5000, -19.5000, -19.3750, -19.3750, -19.2500, -19.3750,\n",
      "            -19.2500, -19.3750, -19.2500, -19.2500, -19.2500, -19.2500, -19.2500,\n",
      "            -19.3750, -19.3750, -19.2500, -19.3750, -19.3750, -19.3750, -19.2500,\n",
      "            -19.2500, -19.3750, -19.2500, -19.1250, -19.5000, -19.3750, -19.5000,\n",
      "            -19.3750, -19.3750, -19.2500, -19.3750, -19.5000, -19.2500, -19.3750,\n",
      "            -19.3750],\n",
      "           [-17.8750, -17.8750, -18.0000,   7.6250,  17.5000,  25.5000,  23.6250,\n",
      "             18.8750,  24.3750,  16.1250,  15.6875,  23.6250,  16.7500,  13.0625,\n",
      "             20.2500,  16.8750,  16.6250,  16.1250,  17.3750,  15.2500,  18.2500,\n",
      "             17.5000,  12.8750,  15.3750,  17.5000, -22.0000, -10.8750, -20.6250,\n",
      "              1.9062, -18.0000, -17.7500, -17.8750, -17.8750, -17.6250, -17.7500,\n",
      "            -17.8750, -17.8750, -17.8750, -17.8750, -17.8750, -17.7500, -17.7500,\n",
      "            -17.7500, -17.8750, -17.8750, -17.8750, -18.0000, -18.0000, -17.8750,\n",
      "            -17.8750, -17.8750, -17.8750, -17.8750, -18.0000, -17.7500, -17.8750,\n",
      "            -17.8750, -17.7500, -17.8750, -17.8750, -18.0000, -17.8750, -17.8750,\n",
      "            -17.8750],\n",
      "           [-23.7500, -23.6250, -23.7500,  13.8125,  15.1875,  16.8750,  21.3750,\n",
      "             13.4375,  22.8750,  22.5000,  24.0000,  20.5000,  14.6875,  21.2500,\n",
      "             17.5000,  29.6250,  25.0000,  25.6250,  12.8750,  15.7500,  16.8750,\n",
      "             22.6250,  14.3750,  14.3125,  21.3750, -10.1250, -15.1875, -12.4375,\n",
      "             -2.2656, -23.7500, -23.6250, -23.7500, -23.6250, -23.5000, -23.5000,\n",
      "            -23.6250, -23.6250, -23.5000, -23.6250, -23.8750, -23.7500, -23.6250,\n",
      "            -23.6250, -23.6250, -23.6250, -23.7500, -23.6250, -23.6250, -23.6250,\n",
      "            -23.7500, -23.7500, -23.6250, -23.5000, -23.7500, -23.6250, -23.5000,\n",
      "            -23.6250, -23.5000, -23.6250, -23.6250, -23.7500, -23.7500, -23.7500,\n",
      "            -23.6250],\n",
      "           [-20.3750, -20.2500, -20.5000,   6.5312,  27.7500,  23.3750,  16.7500,\n",
      "             25.0000,  19.1250,  15.4375,  17.5000,  22.1250,  23.7500,  10.1250,\n",
      "             15.1875,  18.8750,  17.7500,  14.2500,  23.2500,  20.8750,  23.8750,\n",
      "             19.8750,  17.2500,  15.8750,  21.3750, -22.6250, -12.3125, -23.2500,\n",
      "             -0.1982, -20.3750, -20.5000, -20.3750, -20.5000, -20.2500, -20.2500,\n",
      "            -20.3750, -20.3750, -20.3750, -20.2500, -20.2500, -20.2500, -20.2500,\n",
      "            -20.3750, -20.3750, -20.3750, -20.3750, -20.3750, -20.3750, -20.3750,\n",
      "            -20.1250, -20.1250, -20.2500, -20.2500, -20.3750, -20.3750, -20.3750,\n",
      "            -20.3750, -20.3750, -20.3750, -20.3750, -20.5000, -20.3750, -20.3750,\n",
      "            -20.2500],\n",
      "           [-20.0000, -19.8750, -20.0000,   7.5625,  28.3750,  22.3750,  16.0000,\n",
      "             24.5000,  16.3750,  10.1875,  17.3750,  16.7500,  25.0000,   9.4375,\n",
      "             16.6250,  15.4375,  13.0000,   9.8750,  23.5000,  14.6250,  21.7500,\n",
      "             15.0625,  14.3750,  16.5000,  21.7500, -20.0000, -12.7500, -21.6250,\n",
      "             -3.1250, -20.0000, -20.0000, -20.0000, -20.0000, -19.7500, -20.0000,\n",
      "            -19.8750, -19.8750, -19.8750, -19.8750, -19.8750, -19.8750, -19.8750,\n",
      "            -19.7500, -19.8750, -20.0000, -19.8750, -20.0000, -20.0000, -19.8750,\n",
      "            -19.7500, -19.7500, -19.8750, -19.7500, -20.0000, -20.0000, -20.0000,\n",
      "            -20.0000, -20.0000, -19.8750, -20.0000, -20.0000, -20.0000, -20.0000,\n",
      "            -20.0000],\n",
      "           [-19.1250, -19.1250, -19.2500,  11.5625,  12.8750,  18.2500,  24.7500,\n",
      "             15.2500,  22.3750,  17.5000,  16.2500,  19.6250,  12.8125,  17.3750,\n",
      "             17.8750,  17.1250,  16.8750,  17.7500,  13.0625,  11.8750,  13.1250,\n",
      "             16.7500,  10.4375,  14.9375,  15.9375, -13.1250,  -3.9531, -10.6875,\n",
      "             -0.9688, -19.2500, -19.1250, -19.2500, -19.1250, -19.0000, -19.1250,\n",
      "            -19.0000, -19.1250, -19.1250, -19.1250, -19.0000, -19.0000, -19.0000,\n",
      "            -19.1250, -19.1250, -19.1250, -19.0000, -19.1250, -19.1250, -19.0000,\n",
      "            -19.1250, -19.1250, -19.1250, -19.0000, -19.1250, -19.1250, -19.0000,\n",
      "            -19.0000, -19.1250, -19.1250, -19.1250, -19.1250, -19.1250, -19.2500,\n",
      "            -19.1250],\n",
      "           [-22.5000, -22.3750, -22.5000,  10.5000,  14.3750,  18.5000,  25.6250,\n",
      "             15.7500,  22.7500,  16.8750,  18.1250,  18.6250,  13.9375,  19.5000,\n",
      "             18.1250,  19.5000,  17.8750,  22.3750,  13.6875,  13.0625,  14.5000,\n",
      "             17.2500,  13.1875,  15.0000,  14.6250,  -9.9375, -11.1875,  -8.6250,\n",
      "             -2.9688, -22.5000, -22.3750, -22.3750, -22.3750, -22.2500, -22.2500,\n",
      "            -22.3750, -22.5000, -22.3750, -22.3750, -22.2500, -22.3750, -22.3750,\n",
      "            -22.3750, -22.3750, -22.5000, -22.3750, -22.3750, -22.5000, -22.3750,\n",
      "            -22.3750, -22.3750, -22.3750, -22.2500, -22.5000, -22.3750, -22.3750,\n",
      "            -22.3750, -22.3750, -22.3750, -22.3750, -22.5000, -22.3750, -22.5000,\n",
      "            -22.5000],\n",
      "           [-21.1250, -21.0000, -21.1250,   6.1250,  28.0000,  21.1250,  16.0000,\n",
      "             25.5000,  18.2500,  15.2500,  18.5000,  17.8750,  26.2500,  12.9375,\n",
      "             17.1250,  19.3750,  17.7500,  16.1250,  23.5000,  18.6250,  23.6250,\n",
      "             18.7500,  17.7500,  15.6875,  20.8750, -21.2500,  -8.4375, -20.1250,\n",
      "             -3.4062, -21.1250, -21.2500, -21.0000, -21.1250, -21.0000, -21.1250,\n",
      "            -21.1250, -21.0000, -20.8750, -20.8750, -21.0000, -21.0000, -20.8750,\n",
      "            -20.8750, -21.0000, -21.1250, -20.8750, -21.1250, -21.1250, -21.1250,\n",
      "            -20.8750, -20.8750, -21.0000, -21.0000, -21.1250, -21.1250, -21.1250,\n",
      "            -21.0000, -21.1250, -21.0000, -21.0000, -21.1250, -21.0000, -21.0000,\n",
      "            -21.0000],\n",
      "           [-15.0625, -15.0625, -15.0625,  14.0000,  25.0000,  18.0000,  14.8750,\n",
      "             22.6250,  16.7500,  12.0000,  16.2500,  19.2500,  23.0000,  11.2500,\n",
      "             15.6875,  17.0000,  15.1250,  14.1875,  22.7500,  14.7500,  21.8750,\n",
      "             15.7500,  14.8750,  14.6250,  18.2500, -19.7500,  -7.1250, -19.7500,\n",
      "              0.2910, -15.0625, -15.2500, -15.1250, -15.1250, -14.8750, -15.1875,\n",
      "            -15.1250, -14.9375, -15.0000, -15.0000, -14.9375, -15.0000, -15.0000,\n",
      "            -14.9375, -15.0000, -15.0625, -15.0000, -15.0000, -15.1875, -15.0625,\n",
      "            -14.9375, -15.0625, -15.0000, -15.0000, -15.1250, -15.1250, -15.1250,\n",
      "            -14.9375, -15.0625, -15.1250, -15.0000, -15.0000, -15.0000, -15.0625,\n",
      "            -15.0000],\n",
      "           [-13.3125, -13.1875, -13.5000,  14.6875,  11.6250,  18.6250,  26.3750,\n",
      "             15.5625,  20.7500,  22.0000,  17.1250,  18.7500,  10.6875,  22.6250,\n",
      "             23.2500,  19.3750,  17.3750,  21.6250,  13.8750,  12.8125,  12.2500,\n",
      "             18.0000,  12.5625,  17.8750,  20.2500, -15.6250, -10.5000, -22.6250,\n",
      "              3.0469, -13.5000, -13.3750, -13.4375, -13.6250, -13.2500, -13.1875,\n",
      "            -13.1875, -13.3125, -13.1875, -13.3750, -13.2500, -13.1875, -13.2500,\n",
      "            -13.2500, -13.2500, -13.3125, -13.3750, -13.3750, -13.4375, -13.2500,\n",
      "            -13.3750, -13.3750, -13.2500, -13.1250, -13.3750, -13.2500, -13.1875,\n",
      "            -13.2500, -13.1875, -13.4375, -13.1875, -13.1875, -13.3125, -13.3750,\n",
      "            -13.1875]]], device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]]\n",
      "\n",
      "  📊 /content/drive/MyDrive/AFP_work/esmc_600_test_pos combined_embeddings.npy 的形状: (308, 1152)\n",
      "  📄 /content/drive/MyDrive/AFP_work/esmc_600_test_pos combined_embeddings.npy 的前5行数据：\n",
      "[[ 0.00955147 -0.01634502 -0.0180667  ... -0.02730146 -0.01468285\n",
      "   0.00568047]\n",
      " [ 0.02001305 -0.01209932 -0.0231419  ...  0.00063677 -0.0013002\n",
      "  -0.01522294]\n",
      " [ 0.00369677 -0.00684411 -0.01874603 ... -0.01421224 -0.02489749\n",
      "   0.0071571 ]\n",
      " [-0.00890252  0.013904   -0.01096845 ... -0.00885079  0.0251286\n",
      "  -0.00218562]\n",
      " [ 0.01952402 -0.0061013   0.00099249 ... -0.01355654 -0.00697304\n",
      "  -0.01040544]]\n",
      "\n",
      "正在检查文件夹: /content/drive/MyDrive/AFP_work/esmc_600_test_neg\n",
      "  📊 combined_logits.npy 的形状: (308, 1)\n",
      "  📄 combined_logits.npy 的前5行数据：\n",
      "[[ForwardTrackData(sequence=tensor([[[-22.7500, -22.6250, -22.7500,  ..., -22.7500, -22.7500, -22.8750],\n",
      "           [-24.5000, -24.5000, -24.5000,  ..., -24.3750, -24.6250, -24.5000],\n",
      "           [-29.0000, -28.8750, -28.8750,  ..., -28.8750, -29.0000, -28.8750],\n",
      "           ...,\n",
      "           [-20.5000, -20.3750, -20.3750,  ..., -20.5000, -20.5000, -20.5000],\n",
      "           [-19.7500, -19.6250, -19.7500,  ..., -19.7500, -19.7500, -19.7500],\n",
      "           [-19.6250, -19.5000, -19.5000,  ..., -19.5000, -19.6250, -19.6250]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.5000, -22.3750, -22.5000,  ..., -22.5000, -22.5000, -22.5000],\n",
      "           [-24.3750, -24.3750, -24.3750,  ..., -24.3750, -24.3750, -24.3750],\n",
      "           [-25.2500, -25.1250, -25.1250,  ..., -25.2500, -25.2500, -25.2500],\n",
      "           ...,\n",
      "           [-23.1250, -23.1250, -23.2500,  ..., -23.1250, -23.2500, -23.2500],\n",
      "           [-18.2500, -18.1250, -18.2500,  ..., -18.2500, -18.2500, -18.2500],\n",
      "           [-19.3750, -19.2500, -19.3750,  ..., -19.3750, -19.3750, -19.3750]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.3750, -22.2500, -22.3750,  ..., -22.3750, -22.3750, -22.3750],\n",
      "           [-22.8750, -22.7500, -22.8750,  ..., -22.7500, -22.8750, -22.8750],\n",
      "           [-24.5000, -24.5000, -24.5000,  ..., -24.5000, -24.5000, -24.5000],\n",
      "           ...,\n",
      "           [-26.0000, -25.8750, -25.8750,  ..., -25.8750, -25.8750, -26.0000],\n",
      "           [-20.0000, -20.0000, -20.0000,  ..., -20.0000, -20.1250, -20.0000],\n",
      "           [-20.7500, -20.7500, -20.7500,  ..., -20.7500, -20.8750, -20.7500]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-23.0000, -22.8750, -23.0000,  ..., -23.0000, -23.0000, -23.1250],\n",
      "           [-21.7500, -21.6250, -21.6250,  ..., -21.6250, -21.7500, -21.7500],\n",
      "           [-21.7500, -21.7500, -21.8750,  ..., -21.7500, -21.7500, -21.8750],\n",
      "           ...,\n",
      "           [-26.0000, -25.8750, -26.0000,  ..., -25.8750, -26.0000, -26.0000],\n",
      "           [-19.5000, -19.5000, -19.6250,  ..., -19.5000, -19.6250, -19.6250],\n",
      "           [-22.0000, -22.0000, -22.0000,  ..., -22.0000, -22.0000, -22.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]\n",
      " [ForwardTrackData(sequence=tensor([[[-22.8750, -22.6250, -22.8750,  ..., -22.7500, -22.7500, -22.8750],\n",
      "           [-23.6250, -23.5000, -23.6250,  ..., -23.6250, -23.6250, -23.6250],\n",
      "           [-22.3750, -22.2500, -22.2500,  ..., -22.2500, -22.3750, -22.3750],\n",
      "           ...,\n",
      "           [-21.5000, -21.2500, -21.3750,  ..., -21.3750, -21.5000, -21.3750],\n",
      "           [-20.0000, -20.0000, -20.1250,  ..., -20.0000, -20.1250, -20.1250],\n",
      "           [-23.0000, -22.8750, -23.0000,  ..., -22.8750, -23.0000, -23.0000]]],\n",
      "         device='cuda:0', dtype=torch.bfloat16), structure=None, secondary_structure=None, sasa=None, function=None)]]\n",
      "\n",
      "  📊 /content/drive/MyDrive/AFP_work/esmc_600_test_neg combined_embeddings.npy 的形状: (308, 1152)\n",
      "  📄 /content/drive/MyDrive/AFP_work/esmc_600_test_neg combined_embeddings.npy 的前5行数据：\n",
      "[[-0.01633092  0.01812963 -0.00544356 ...  0.00477491  0.00082364\n",
      "   0.00811116]\n",
      " [ 0.00572116  0.02359726 -0.01338577 ...  0.00237886 -0.00929524\n",
      "  -0.00353848]\n",
      " [-0.00797354  0.01936996 -0.01320011 ... -0.00628619 -0.00710818\n",
      "  -0.00221466]\n",
      " [-0.02267375  0.0115626  -0.02234262 ... -0.02272365 -0.0095801\n",
      "   0.00937222]\n",
      " [ 0.00310926 -0.01185363 -0.01291295 ...  0.00137915  0.02103562\n",
      "   0.00094701]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 遍历每个特征文件夹，加载并打印 combined_logits.npy 和 combined_embeddings.npy 的维度\n",
    "def inspect_combined_files(feature_dirs):\n",
    "    \"\"\"\n",
    "    遍历每个特征文件夹，加载并打印 combined_logits.npy 和 combined_embeddings.npy 的维度。\n",
    "\n",
    "    参数:\n",
    "        feature_dirs (List[str]): 特征文件夹路径列表。\n",
    "    \"\"\"\n",
    "    for feature_dir in feature_dirs:\n",
    "        print(f\"正在检查文件夹: {feature_dir}\")\n",
    "\n",
    "        # 定义 combined_logits.npy 和 combined_embeddings.npy 的路径\n",
    "        combined_logits_path = os.path.join(feature_dir, 'combined_logits.npy')\n",
    "        combined_embeddings_path = os.path.join(feature_dir, 'combined_embeddings.npy')\n",
    "\n",
    "        # 检查并加载 combined_logits.npy\n",
    "        if os.path.isfile(combined_logits_path):\n",
    "            try:\n",
    "                combined_logits = np.load(combined_logits_path, allow_pickle=True)\n",
    "                print(f\"  {os.path.basename(combined_logits_path)} 的形状: {combined_logits.shape}\")\n",
    "                print(f\"  {os.path.basename(combined_logits_path)} 的前5行数据：\\n{combined_logits[:5]}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"  加载 {os.path.basename(combined_logits_path)} 时出错: {e}\")\n",
    "        else:\n",
    "            print(f\" {os.path.basename(combined_logits_path)} 不存在。\")\n",
    "\n",
    "        # 检查并加载 combined_embeddings.npy\n",
    "        if os.path.isfile(combined_embeddings_path):\n",
    "            try:\n",
    "                combined_embeddings = np.load(combined_embeddings_path, allow_pickle=True)\n",
    "                print(f\" {feature_dir} {os.path.basename(combined_embeddings_path)} 的形状: {combined_embeddings.shape}\")\n",
    "                print(f\" {feature_dir} {os.path.basename(combined_embeddings_path)} 的前5行数据：\\n{combined_embeddings[:5]}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"加载 {os.path.basename(combined_embeddings_path)} 时出错: {e}\\n\")\n",
    "        else:\n",
    "            print(f\"{os.path.basename(combined_embeddings_path)} 不存在。\\n\")\n",
    "            \n",
    "inspect_combined_files(feature_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pnBCTCqhdFH"
   },
   "source": [
    "## 处理结构信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1754200816480,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "HVvzEYr8hfQt",
    "outputId": "963779f9-1fe3-415b-805a-4dfa9b9c6033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: /content/drive/MyDrive/AFP_work/pdb/train_pos\n",
      "✅ 文件夹 'train_pos' 中共有 1140 个 .pdb 文件。\n",
      "\n",
      "正在处理文件夹: /content/drive/MyDrive/AFP_work/pdb/train_neg\n",
      "✅ 文件夹 'train_neg' 中共有 1200 个 .pdb 文件。\n",
      "\n",
      "正在处理文件夹: /content/drive/MyDrive/AFP_work/pdb/test_pos\n",
      "✅ 文件夹 'test_pos' 中共有 367 个 .pdb 文件。\n",
      "\n",
      "正在处理文件夹: /content/drive/MyDrive/AFP_work/pdb/test_neg\n",
      "✅ 文件夹 'test_neg' 中共有 308 个 .pdb 文件。\n",
      "\n",
      "📊 各文件夹中 .pdb 文件的数量统计：\n",
      "      folder                                           path  pdb_file_count\n",
      "0  train_pos  /content/drive/MyDrive/AFP_work/pdb/train_pos            1140\n",
      "1  train_neg  /content/drive/MyDrive/AFP_work/pdb/train_neg            1200\n",
      "2   test_pos   /content/drive/MyDrive/AFP_work/pdb/test_pos             367\n",
      "3   test_neg   /content/drive/MyDrive/AFP_work/pdb/test_neg             308\n",
      "\n",
      "✅ 统计结果已保存到 '/content/drive/MyDrive/AFP_work/pdb/pdb_file_counts.csv'。\n",
      "\n",
      "📄 统计结果 CSV 文件内容：\n",
      "      folder                                           path  pdb_file_count\n",
      "0  train_pos  /content/drive/MyDrive/AFP_work/pdb/train_pos            1140\n",
      "1  train_neg  /content/drive/MyDrive/AFP_work/pdb/train_neg            1200\n",
      "2   test_pos   /content/drive/MyDrive/AFP_work/pdb/test_pos             367\n",
      "3   test_neg   /content/drive/MyDrive/AFP_work/pdb/test_neg             308\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# 定义目标文件夹路径\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "\n",
    "# 将文件夹路径存储在一个字典中，便于遍历\n",
    "folder_paths = {\n",
    "    'train_pos': train_pos_folder_path,\n",
    "    'train_neg': train_neg_folder_path,\n",
    "    'test_pos': test_pos_folder_path,\n",
    "    'test_neg': test_neg_folder_path\n",
    "}\n",
    "\n",
    "# 初始化一个空列表，用于存储统计结果\n",
    "stats = []\n",
    "\n",
    "# 遍历每个文件夹，统计个数\n",
    "for folder_name, folder_path in folder_paths.items():\n",
    "    print(f\"正在处理文件夹: {folder_path}\")\n",
    "\n",
    "    # 检查文件夹是否存在\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"❌ 文件夹 '{folder_path}' 不存在。请检查路径是否正确。\\n\")\n",
    "        stats.append({\n",
    "            'folder': folder_name,\n",
    "            'path': folder_path,\n",
    "            'pdb_file_count': '文件夹不存在'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # 使用 glob 查找所有 .pdb 文件（不区分大小写）\n",
    "    pdb_files = glob.glob(os.path.join(folder_path, '*.pdb')) + glob.glob(os.path.join(folder_path, '*.PDB'))\n",
    "\n",
    "    # 统计 .pdb 文件的数量\n",
    "    pdb_count = len(pdb_files)\n",
    "\n",
    "    print(f\"✅ 文件夹 '{folder_name}' 中共有 {pdb_count} 个 .pdb 文件。\\n\")\n",
    "\n",
    "    # 将统计结果添加到列表中\n",
    "    stats.append({\n",
    "        'folder': folder_name,\n",
    "        'path': folder_path,\n",
    "        'pdb_file_count': pdb_count\n",
    "    })\n",
    "\n",
    "# 创建 DataFrame\n",
    "df_stats = pd.DataFrame(stats)\n",
    "\n",
    "# 显示统计结果\n",
    "print(\"📊 各文件夹中 .pdb 文件的数量统计：\")\n",
    "print(df_stats)\n",
    "\n",
    "# 保存统计结果为 CSV 文件\n",
    "csv_output_path = '/content/drive/MyDrive/AFP_work/pdb/pdb_file_counts.csv'\n",
    "df_stats.to_csv(csv_output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n✅ 统计结果已保存到 '{csv_output_path}'。\")\n",
    "\n",
    "# 读取并显示保存的 CSV 文件内容\n",
    "df_loaded_stats = pd.read_csv(csv_output_path)\n",
    "print(\"\\n📄 统计结果 CSV 文件内容：\")\n",
    "print(df_loaded_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNGVugPJmcfN",
    "outputId": "16c0fc58-6fa9-4739-d59e-aa4deb9227b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有 .pdb 文件名已成功保存到 '/content/drive/MyDrive/pdb/test_pos_pdb_filenames.csv'。\n",
      "总共保存了 308 个文件名。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# 定义目标文件夹路径\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "\n",
    "# 定义输出 CSV 文件的保存路径\n",
    "output_csv_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos_pdb_filenames.csv'\n",
    "\n",
    "# 检查文件夹是否存在\n",
    "if not os.path.isdir(train_pos_folder_path):\n",
    "    print(f\"❌ 文件夹 '{train_pos_folder_path}' 不存在。请检查路径是否正确。\")\n",
    "else:\n",
    "    # 使用 glob 查找所有 .pdb 文件（不区分大小写）\n",
    "    pdb_files_lower = glob.glob(os.path.join(train_pos_folder_path, '*.pdb'))\n",
    "    pdb_files_upper = glob.glob(os.path.join(train_pos_folder_path, '*.PDB'))\n",
    "    pdb_files = pdb_files_lower + pdb_files_upper\n",
    "\n",
    "    # 提取文件名\n",
    "    pdb_filenames = [os.path.basename(f) for f in pdb_files]\n",
    "\n",
    "    # 检查是否找到任何 .pdb 文件\n",
    "    if not pdb_filenames:\n",
    "        print(f\"❌ 在文件夹 '{train_pos_folder_path}' 中未找到任何 .pdb 文件。\")\n",
    "    else:\n",
    "        # 创建一个 DataFrame\n",
    "        df = pd.DataFrame({'filename': pdb_filenames})\n",
    "\n",
    "        # 保存为 CSV 文件\n",
    "        try:\n",
    "            df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"✅ 所有 .pdb 文件名已成功保存到 '{output_csv_path}'。\")\n",
    "            print(f\"总共保存了 {len(pdb_filenames)} 个文件名。\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 保存 CSV 文件时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFQPsDFXnMF6",
    "outputId": "b30531eb-18e9-426f-97bd-4d92bd3b0ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 提取的数字已成功保存到 '/content/drive/MyDrive/pdb/test_pos_pdb_filenames_extracted.csv'。\n",
      "总共保存了 308 个数字。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def save_extracted_numbers_to_csv(folder_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    从指定文件夹中提取 .pdb 文件名中 '_relaxed_rank_001' 之前的数字，并保存到 CSV 文件中。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): 目标文件夹路径。\n",
    "        output_csv_path (str): 输出 CSV 文件的路径。\n",
    "    \"\"\"\n",
    "    # 检查文件夹是否存在\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"❌ 文件夹 '{folder_path}' 不存在。请检查路径是否正确。\")\n",
    "        return\n",
    "\n",
    "    # 使用 glob 查找所有 .pdb 文件（不区分大小写）\n",
    "    pdb_files_lower = glob.glob(os.path.join(folder_path, '*.pdb'))\n",
    "    pdb_files_upper = glob.glob(os.path.join(folder_path, '*.PDB'))\n",
    "    pdb_files = pdb_files_lower + pdb_files_upper\n",
    "\n",
    "    # 提取文件名\n",
    "    pdb_filenames = [os.path.basename(f) for f in pdb_files]\n",
    "\n",
    "    # 定义正则表达式模式\n",
    "    # 文件名格式：数字_relaxed_rank_001_其他信息.pdb，例如：2033_relaxed_rank_001_alphafold2_ptm_model_4_seed_000.pdb\n",
    "    pattern = re.compile(r'^(\\d+)_relaxed_rank_001.*\\.pdb$', re.IGNORECASE)\n",
    "\n",
    "    # 初始化列表存储提取的数字\n",
    "    extracted_numbers = []\n",
    "\n",
    "    # 遍历文件名并提取数字\n",
    "    for filename in pdb_filenames:\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            extracted_numbers.append(int(number))  # 转换为整数类型\n",
    "        else:\n",
    "            print(f\"⚠️ 文件名不符合预期模式，无法提取数字: {filename}\")\n",
    "\n",
    "    # 检查是否提取到了任何数字\n",
    "    if not extracted_numbers:\n",
    "        print(f\"❌ 在文件夹 '{folder_path}' 中未找到符合模式的 .pdb 文件。\")\n",
    "    else:\n",
    "        # 创建一个 DataFrame\n",
    "        df = pd.DataFrame({'extracted_number': extracted_numbers})\n",
    "\n",
    "        # 保存为 CSV 文件\n",
    "        try:\n",
    "            df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"✅ 提取的数字已成功保存到 '{output_csv_path}'。\")\n",
    "            print(f\"总共保存了 {len(extracted_numbers)} 个数字。\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 保存 CSV 文件时出错: {e}\")\n",
    "\n",
    "# 定义目标文件夹路径\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "\n",
    "# 定义输出 CSV 文件的保存路径\n",
    "output_csv_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos_pdb_filenames_extracted.csv'\n",
    "\n",
    "# 调用函数\n",
    "save_extracted_numbers_to_csv(train_pos_folder_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "-zMy7WkUuhOK",
    "outputId": "373a9827-4b39-4f85-bbfd-6996a1312446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理文件夹: /content/drive/MyDrive/pdb/train_pos\n",
      "文件数量: 1200\n",
      "✅ 已处理 100 个文件。\n",
      "✅ 已处理 200 个文件。\n",
      "✅ 已处理 300 个文件。\n",
      "✅ 已处理 400 个文件。\n",
      "✅ 已处理 500 个文件。\n",
      "✅ 已处理 600 个文件。\n",
      "✅ 已处理 700 个文件。\n",
      "✅ 已处理 800 个文件。\n",
      "✅ 已处理 900 个文件。\n",
      "✅ 已处理 1000 个文件。\n",
      "✅ 已处理 1100 个文件。\n",
      "✅ 已处理 1200 个文件。\n",
      "✅ 完成处理 1200 个文件。\n",
      "处理文件夹: /content/drive/MyDrive/pdb/train_neg\n",
      "文件数量: 1200\n",
      "✅ 已处理 100 个文件。\n",
      "✅ 已处理 200 个文件。\n",
      "✅ 已处理 300 个文件。\n",
      "✅ 已处理 400 个文件。\n",
      "✅ 已处理 500 个文件。\n",
      "✅ 已处理 600 个文件。\n",
      "✅ 已处理 700 个文件。\n",
      "✅ 已处理 800 个文件。\n",
      "✅ 已处理 900 个文件。\n",
      "✅ 已处理 1000 个文件。\n",
      "✅ 已处理 1100 个文件。\n",
      "✅ 已处理 1200 个文件。\n",
      "✅ 完成处理 1200 个文件。\n",
      "处理文件夹: /content/drive/MyDrive/pdb/test_pos\n",
      "文件数量: 308\n",
      "✅ 已处理 100 个文件。\n",
      "✅ 已处理 200 个文件。\n",
      "✅ 已处理 300 个文件。\n",
      "✅ 完成处理 308 个文件。\n",
      "处理文件夹: /content/drive/MyDrive/pdb/test_neg\n",
      "文件数量: 308\n",
      "✅ 已处理 100 个文件。\n",
      "✅ 已处理 200 个文件。\n",
      "✅ 已处理 300 个文件。\n",
      "✅ 完成处理 308 个文件。\n",
      "总处理时间: 947.43 秒\n",
      "\n",
      "保存的 JSON 文件数量:\n",
      "文件夹 \"/content/drive/MyDrive/pdb_features/train_pos\" 中保存的 JSON 文件数量: 1200\n",
      "文件夹 \"/content/drive/MyDrive/pdb_features/train_neg\" 中保存的 JSON 文件数量: 1200\n",
      "文件夹 \"/content/drive/MyDrive/pdb_features/test_pos\" 中保存的 JSON 文件数量: 308\n",
      "文件夹 \"/content/drive/MyDrive/pdb_features/test_neg\" 中保存的 JSON 文件数量: 308\n",
      "\n",
      "检查部分 JSON 文件的维度信息:\n",
      "文件: 402_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "节点特征数量: 13, 位置特征维度: 3\n",
      "边特征数量: 43, 方向特征数量: 43, 旋转特征数量: 43\n",
      "\n",
      "文件: 489_relaxed_rank_001_alphafold2_ptm_model_2_seed_000_features.json\n",
      "节点特征数量: 13, 位置特征维度: 3\n",
      "边特征数量: 33, 方向特征数量: 33, 旋转特征数量: 33\n",
      "\n",
      "文件: 524_relaxed_rank_001_alphafold2_ptm_model_5_seed_000_features.json\n",
      "节点特征数量: 13, 位置特征维度: 3\n",
      "边特征数量: 50, 方向特征数量: 50, 旋转特征数量: 50\n",
      "\n",
      "文件: 596_relaxed_rank_001_alphafold2_ptm_model_1_seed_000_features.json\n",
      "节点特征数量: 13, 位置特征维度: 3\n",
      "边特征数量: 50, 方向特征数量: 50, 旋转特征数量: 50\n",
      "\n",
      "文件: 703_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "节点特征数量: 13, 位置特征维度: 3\n",
      "边特征数量: 51, 方向特征数量: 51, 旋转特征数量: 51\n",
      "\n",
      "文件: 414_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "节点特征数量: 78, 位置特征维度: 3\n",
      "边特征数量: 392, 方向特征数量: 392, 旋转特征数量: 392\n",
      "\n",
      "文件: 509_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "节点特征数量: 78, 位置特征维度: 3\n",
      "边特征数量: 615, 方向特征数量: 615, 旋转特征数量: 615\n",
      "\n",
      "文件: 578_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "节点特征数量: 78, 位置特征维度: 3\n",
      "边特征数量: 474, 方向特征数量: 474, 旋转特征数量: 474\n",
      "\n",
      "文件: 1089_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "节点特征数量: 78, 位置特征维度: 3\n",
      "边特征数量: 485, 方向特征数量: 485, 旋转特征数量: 485\n",
      "\n",
      "文件: 1116_relaxed_rank_001_alphafold2_ptm_model_1_seed_000_features.json\n",
      "节点特征数量: 78, 位置特征维度: 3\n",
      "边特征数量: 511, 方向特征数量: 511, 旋转特征数量: 511\n",
      "\n",
      "文件: 398_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "节点特征数量: 11, 位置特征维度: 3\n",
      "边特征数量: 42, 方向特征数量: 42, 旋转特征数量: 42\n",
      "\n",
      "文件: 297_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "节点特征数量: 12, 位置特征维度: 3\n",
      "边特征数量: 45, 方向特征数量: 45, 旋转特征数量: 45\n",
      "\n",
      "文件: 402_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "节点特征数量: 12, 位置特征维度: 3\n",
      "边特征数量: 33, 方向特征数量: 33, 旋转特征数量: 33\n",
      "\n",
      "文件: 436_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "节点特征数量: 12, 位置特征维度: 3\n",
      "边特征数量: 44, 方向特征数量: 44, 旋转特征数量: 44\n",
      "\n",
      "文件: 18_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "节点特征数量: 13, 位置特征维度: 3\n",
      "边特征数量: 52, 方向特征数量: 52, 旋转特征数量: 52\n",
      "\n",
      "文件: 2_relaxed_rank_001_alphafold2_ptm_model_5_seed_000_features.json\n",
      "节点特征数量: 35, 位置特征维度: 3\n",
      "边特征数量: 215, 方向特征数量: 215, 旋转特征数量: 215\n",
      "\n",
      "文件: 65_relaxed_rank_001_alphafold2_ptm_model_2_seed_000_features.json\n",
      "节点特征数量: 11, 位置特征维度: 3\n",
      "边特征数量: 41, 方向特征数量: 41, 旋转特征数量: 41\n",
      "\n",
      "文件: 125_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "节点特征数量: 12, 位置特征维度: 3\n",
      "边特征数量: 45, 方向特征数量: 45, 旋转特征数量: 45\n",
      "\n",
      "文件: 241_relaxed_rank_001_alphafold2_ptm_model_3_seed_000_features.json\n",
      "节点特征数量: 11, 位置特征维度: 3\n",
      "边特征数量: 40, 方向特征数量: 40, 旋转特征数量: 40\n",
      "\n",
      "文件: 546_relaxed_rank_001_alphafold2_ptm_model_4_seed_000_features.json\n",
      "节点特征数量: 12, 位置特征维度: 3\n",
      "边特征数量: 45, 方向特征数量: 45, 旋转特征数量: 45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "\n",
    "# 训练集和测试集的 PDB 文件夹路径\n",
    "train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_pos'\n",
    "train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/train_neg'\n",
    "test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_pos'\n",
    "test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb/test_neg'\n",
    "\n",
    "# 输出文件夹路径\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "# 创建输出文件夹（如果不存在）\n",
    "os.makedirs(output_train_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_train_neg_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_pos_folder_path, exist_ok=True)\n",
    "os.makedirs(output_test_neg_folder_path, exist_ok=True)\n",
    "\n",
    "# 提取训练集和测试集中的所有 PDB 文件\n",
    "train_pos_pdb_files = [f for f in os.listdir(train_pos_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "train_neg_pdb_files = [f for f in os.listdir(train_neg_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "test_pos_pdb_files = [f for f in os.listdir(test_pos_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "test_neg_pdb_files = [f for f in os.listdir(test_neg_folder_path) if f.endswith('.pdb') or f.endswith('.PDB')]\n",
    "\n",
    "# 初始化 PDBParser\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 优化 PDB 文件处理函数\n",
    "def process_pdb_file(pdb_path):\n",
    "    try:\n",
    "        structure = parser.get_structure('', pdb_path)\n",
    "        residues = [residue for residue in structure.get_residues() if 'CA' in residue]\n",
    "        num_residues = len(residues)\n",
    "\n",
    "        if num_residues == 0:\n",
    "            print(f\"⚠️ 文件 '{pdb_path}' 中没有找到 CA 原子。\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        # 提取位置特征、方向特征和旋转特征\n",
    "        positions = np.array([residue['CA'].get_coord() for residue in residues], dtype=np.float64)\n",
    "        edges = []\n",
    "        directions = []\n",
    "        rotations = []\n",
    "\n",
    "        # 计算接触图和附加特征\n",
    "        for i in range(num_residues):\n",
    "            for j in range(i + 1, num_residues):\n",
    "                distance = np.linalg.norm(positions[i] - positions[j])\n",
    "                if distance < 10.0:  # 阈值为10Å来定义接触\n",
    "                    edges.append([i, j])\n",
    "                    direction = positions[j] - positions[i]\n",
    "                    norm = np.linalg.norm(direction)\n",
    "                    if norm != 0:\n",
    "                        directions.append(direction / norm)\n",
    "                        rotations.append(float(np.arctan2(direction[1], direction[0])))\n",
    "\n",
    "        return positions, edges, directions, rotations\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 处理文件 '{pdb_path}' 时出错: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# 处理 PDB 文件并保存特征\n",
    "def process_and_save(pdb_files, folder_path, output_folder_path, label):\n",
    "    print(f'处理文件夹: {folder_path}')\n",
    "    print(f'文件数量: {len(pdb_files)}')\n",
    "\n",
    "    processed_count = 0\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_path = os.path.join(folder_path, pdb_file)\n",
    "        positions, edges, directions, rotations = process_pdb_file(pdb_path)\n",
    "\n",
    "        if positions is None:\n",
    "            continue  # 跳过处理出错的文件\n",
    "\n",
    "        features = {\n",
    "            \"node_features\": positions.tolist(),\n",
    "            \"edge_features\": {\n",
    "                \"edges\": edges,\n",
    "                \"directions\": [d.tolist() for d in directions],\n",
    "                \"rotations\": [float(rot) for rot in rotations]\n",
    "            },\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "        # 定义输出文件路径\n",
    "        output_file_path = os.path.join(output_folder_path, f'{os.path.splitext(pdb_file)[0]}_features.json')\n",
    "\n",
    "        # 保存特征到文件\n",
    "        try:\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                json.dump(features, output_file)\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 保存文件 '{output_file_path}' 时出错: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 每处理100个文件，打印一次进度\n",
    "        if processed_count % 100 == 0:\n",
    "            print(f'✅ 已处理 {processed_count} 个文件。')\n",
    "\n",
    "    print(f'✅ 完成处理 {processed_count} 个文件。')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 处理训练集中的阳性 PDB 文件\n",
    "process_and_save(train_pos_pdb_files, train_pos_folder_path, output_train_pos_folder_path, label=1)\n",
    "# 处理训练集中的阴性 PDB 文件\n",
    "process_and_save(train_neg_pdb_files, train_neg_folder_path, output_train_neg_folder_path, label=0)\n",
    "# 处理测试集中的阳性 PDB 文件\n",
    "process_and_save(test_pos_pdb_files, test_pos_folder_path, output_test_pos_folder_path, label=1)\n",
    "# 处理测试集中的阴性 PDB 文件\n",
    "process_and_save(test_neg_pdb_files, test_neg_folder_path, output_test_neg_folder_path, label=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"总处理时间: {end_time - start_time:.2f} 秒\")\n",
    "\n",
    "\n",
    "\n",
    "# 输出保存的 JSON 文件数量\n",
    "def count_json_files(output_folder_path):\n",
    "    json_files = [f for f in os.listdir(output_folder_path) if f.endswith('.json')]\n",
    "    print(f'文件夹 \"{output_folder_path}\" 中保存的 JSON 文件数量: {len(json_files)}')\n",
    "    return json_files\n",
    "\n",
    "print(\"\\n保存的 JSON 文件数量:\")\n",
    "count_json_files(output_train_pos_folder_path)\n",
    "count_json_files(output_train_neg_folder_path)\n",
    "count_json_files(output_test_pos_folder_path)\n",
    "count_json_files(output_test_neg_folder_path)\n",
    "\n",
    "# 检查每个保存的 JSON 文件中的维度信息\n",
    "def check_json_dimensions(output_folder_path):\n",
    "    json_files = [f for f in os.listdir(output_folder_path) if f.endswith('.json')]\n",
    "    for json_file in json_files[:5]:  # 仅检查前5个文件\n",
    "        json_path = os.path.join(output_folder_path, json_file)\n",
    "        with open(json_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            node_features = data.get(\"node_features\", [])\n",
    "            edge_features = data.get(\"edge_features\", {})\n",
    "            edges = edge_features.get(\"edges\", [])\n",
    "            directions = edge_features.get(\"directions\", [])\n",
    "            rotations = edge_features.get(\"rotations\", [])\n",
    "            print(f'文件: {json_file}')\n",
    "            print(f'节点特征数量: {len(node_features)}, 位置特征维度: {len(node_features[0]) if node_features else 0}')\n",
    "            print(f'边特征数量: {len(edges)}, 方向特征数量: {len(directions)}, 旋转特征数量: {len(rotations)}\\n')\n",
    "\n",
    "print(\"\\n检查部分 JSON 文件的维度信息:\")\n",
    "check_json_dimensions(output_train_pos_folder_path)\n",
    "check_json_dimensions(output_train_neg_folder_path)\n",
    "check_json_dimensions(output_test_pos_folder_path)\n",
    "check_json_dimensions(output_test_neg_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maxpswcORjOn",
    "outputId": "411069fe-9c71-4fbf-857c-ad3961235a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 'train_pos' 中的 JSON 文件数量: 1200\n",
      "文件夹 'train_neg' 中的 JSON 文件数量: 1200\n",
      "文件夹 'test_pos' 中的 JSON 文件数量: 308\n",
      "文件夹 'test_neg' 中的 JSON 文件数量: 308\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# 定义输出文件夹路径\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "# 将文件夹路径存储在一个字典中，便于遍历\n",
    "folders = {\n",
    "    'train_pos': output_train_pos_folder_path,\n",
    "    'train_neg': output_train_neg_folder_path,\n",
    "    'test_pos': output_test_pos_folder_path,\n",
    "    'test_neg': output_test_neg_folder_path\n",
    "}\n",
    "\n",
    "# 遍历每个文件夹并统计 JSON 文件数量\n",
    "for folder_name, folder_path in folders.items():\n",
    "    if os.path.isdir(folder_path):\n",
    "        # 使用 glob 查找所有 .json 文件（不区分大小写）\n",
    "        json_files = glob.glob(os.path.join(folder_path, '*.json')) + glob.glob(os.path.join(folder_path, '*.JSON'))\n",
    "        count = len(json_files)\n",
    "        print(f\"文件夹 '{folder_name}' 中的 JSON 文件数量: {count}\")\n",
    "    else:\n",
    "        print(f\"❌ 文件夹 '{folder_name}' 不存在。请检查路径是否正确。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2hWiwqzc-V-1",
    "outputId": "e321fe47-78cd-44db-b20f-2ea35524cde6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(filename='/content/drive/MyDrive/AFP_work/pdb_features/processing.log',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# 定义输出文件夹路径\n",
    "output_train_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_pos'\n",
    "output_train_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/train_neg'\n",
    "output_test_pos_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_pos'\n",
    "output_test_neg_folder_path = '/content/drive/MyDrive/AFP_work/pdb_features/test_neg'\n",
    "\n",
    "output_folders = {\n",
    "    'train_pos': output_train_pos_folder_path,\n",
    "    'train_neg': output_train_neg_folder_path,\n",
    "    'test_pos': output_test_pos_folder_path,\n",
    "    'test_neg': output_test_neg_folder_path\n",
    "}\n",
    "\n",
    "# 创建输出文件夹（如果不存在）\n",
    "for folder in output_folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 定义函数加载单个 JSON 文件\n",
    "def load_single_json(json_file):\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            sample = json.load(f)\n",
    "        logging.info(f\"成功加载文件: {json_file}\")\n",
    "        return sample\n",
    "    except Exception as e:\n",
    "        logging.error(f\"加载文件 '{json_file}' 时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "# 定义函数并行加载 JSON 文件\n",
    "def load_json_files_parallel(folder_path, max_workers=8):\n",
    "    \"\"\"\n",
    "    并行加载指定文件夹中的所有 JSON 文件。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): JSON 文件所在的文件夹路径。\n",
    "        max_workers (int): 并行工作的最大线程数。\n",
    "\n",
    "    返回:\n",
    "        list: 包含所有成功加载的样本数据的列表。\n",
    "    \"\"\"\n",
    "    json_files = glob.glob(os.path.join(folder_path, '*.json'))\n",
    "    data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(load_single_json, f): f for f in json_files}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=f'Loading {os.path.basename(folder_path)}'):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                data.append(result)\n",
    "    return data\n",
    "\n",
    "# 初始化训练集和测试集\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# 加载训练集数据\n",
    "train_pos_data = load_json_files_parallel(output_folders['train_pos'])\n",
    "train_neg_data = load_json_files_parallel(output_folders['train_neg'])\n",
    "train_data = train_pos_data + train_neg_data\n",
    "\n",
    "# 加载测试集数据\n",
    "test_pos_data = load_json_files_parallel(output_folders['test_pos'])\n",
    "test_neg_data = load_json_files_parallel(output_folders['test_neg'])\n",
    "test_data = test_pos_data + test_neg_data\n",
    "\n",
    "print(f\"✅ 训练集总样本数: {len(train_data)}\")\n",
    "print(f\"✅ 测试集总样本数: {len(test_data)}\")\n",
    "\n",
    "# 定义输出汇总文件的路径\n",
    "aggregated_output_folder = '/content/drive/MyDrive/AFP_work/pdb_features/aggregated'\n",
    "os.makedirs(aggregated_output_folder, exist_ok=True)\n",
    "\n",
    "train_output_path = os.path.join(aggregated_output_folder, 'train_dataset.json')\n",
    "test_output_path = os.path.join(aggregated_output_folder, 'test_dataset.json')\n",
    "\n",
    "# 保存训练集\n",
    "try:\n",
    "    with open(train_output_path, 'w') as f:\n",
    "        json.dump(train_data, f)\n",
    "    print(f\"✅ 训练集已保存到 '{train_output_path}'。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 保存训练集时出错: {e}\")\n",
    "\n",
    "# 保存测试集\n",
    "try:\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_data, f)\n",
    "    print(f\"✅ 测试集已保存到 '{test_output_path}'。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 保存测试集时出错: {e}\")\n",
    "\n",
    "# 验证汇总结果\n",
    "def load_aggregated_data(file_path):\n",
    "    \"\"\"\n",
    "    从指定的 JSON 文件中加载汇总数据。\n",
    "\n",
    "    参数:\n",
    "        file_path (str): 汇总数据的 JSON 文件路径。\n",
    "\n",
    "    返回:\n",
    "        list: 包含所有样本数据的列表。\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"❌ 文件 '{file_path}' 不存在。\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"✅ 成功加载 '{file_path}'，样本数: {len(data)}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 加载文件 '{file_path}' 时出错: {e}\")\n",
    "        return []\n",
    "\n",
    "# 加载并查看训练集\n",
    "train_dataset = load_aggregated_data(train_output_path)\n",
    "if train_dataset:\n",
    "    print(f\"训练集第一个样本内容:\")\n",
    "    print(json.dumps(train_dataset[0], indent=2))\n",
    "\n",
    "# 加载并查看测试集\n",
    "test_dataset = load_aggregated_data(test_output_path)\n",
    "if test_dataset:\n",
    "    print(f\"测试集第一个样本内容:\")\n",
    "    print(json.dumps(test_dataset[0], indent=2))\n",
    "\n",
    "# 转换为 Pandas DataFrame（可选）\n",
    "def json_to_dataframe(data):\n",
    "    \"\"\"\n",
    "    将 JSON 数据转换为 Pandas DataFrame。\n",
    "\n",
    "    参数:\n",
    "        data (list): 包含所有样本数据的列表。\n",
    "\n",
    "    返回:\n",
    "        pd.DataFrame: 包含标签和特征的 DataFrame。\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for sample in data:\n",
    "        record = {}\n",
    "        record['label'] = sample['label']\n",
    "        # 示例：计算节点特征的均值和标准差作为简单特征\n",
    "        node_features = np.array(sample['node_features'])\n",
    "        record['node_mean_x'] = node_features[:, 0].mean()\n",
    "        record['node_mean_y'] = node_features[:, 1].mean()\n",
    "        record['node_mean_z'] = node_features[:, 2].mean()\n",
    "        record['node_std_x'] = node_features[:, 0].std()\n",
    "        record['node_std_y'] = node_features[:, 1].std()\n",
    "        record['node_std_z'] = node_features[:, 2].std()\n",
    "        # 可以根据需要添加更多特征\n",
    "        records.append(record)\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "# 转换训练集和测试集为 DataFrame\n",
    "train_df = json_to_dataframe(train_dataset)\n",
    "test_df = json_to_dataframe(test_dataset)\n",
    "\n",
    "print(\"训练集 DataFrame 预览:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n测试集 DataFrame 预览:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# 保存为 CSV 文件（可选）\n",
    "train_csv_path = os.path.join(aggregated_output_folder, 'train_dataset_dataframe.csv')\n",
    "test_csv_path = os.path.join(aggregated_output_folder, 'test_dataset_dataframe.csv')\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False, encoding='utf-8-sig')\n",
    "test_df.to_csv(test_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ 训练集 DataFrame 已保存到 '{train_csv_path}'。\")\n",
    "print(f\"✅ 测试集 DataFrame 已保存到 '{test_csv_path}'。\")\n",
    "\n",
    "# 定义 PyTorch Geometric 数据集类（可选）\n",
    "class PDBDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(PDBDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        sample = self.data_list[idx]\n",
    "        node_features = torch.tensor(sample['node_features'], dtype=torch.float)\n",
    "        edge_index = torch.tensor(sample['edge_features']['edges'], dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(sample['edge_features']['directions'], dtype=torch.float)\n",
    "        rotations = torch.tensor(sample['edge_features']['rotations'], dtype=torch.float).unsqueeze(1)\n",
    "        edge_features = torch.cat([edge_attr, rotations], dim=1)  # 合并方向和旋转特征\n",
    "        label = torch.tensor(sample['label'], dtype=torch.long)\n",
    "\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features, y=label)\n",
    "        return data\n",
    "\n",
    "# 创建 PyTorch Geometric 数据集（可选）\n",
    "train_pyg_dataset = PDBDataset(train_dataset)\n",
    "test_pyg_dataset = PDBDataset(test_dataset)\n",
    "\n",
    "# 创建 DataLoader（可选）\n",
    "train_loader = DataLoader(train_pyg_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_pyg_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"✅ PyTorch Geometric 训练集数据量: {len(train_pyg_dataset)}\")\n",
    "print(f\"✅ PyTorch Geometric 测试集数据量: {len(test_pyg_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8orO_EtEEoLh"
   },
   "source": [
    "# 结合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 195262,
     "status": "ok",
     "timestamp": 1759391305851,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "INRwfY49Ep95",
    "outputId": "36a55a84-f21f-4d78-d68d-7afdb3840041"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "\n",
    "from torch_geometric.nn import (\n",
    "    GATConv, SAGEConv, GINConv, Set2Set, global_mean_pool\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, precision_recall_fscore_support,\n",
    "    matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import copy\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "esmc_folders = {\n",
    "    'train_pos': '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    'train_neg': '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    'test_pos': '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',\n",
    "    'test_neg': '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "}\n",
    "\n",
    "struct_folders = {\n",
    "    'train': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/train_dataset.json',\n",
    "    'test': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/test_dataset.json'\n",
    "}\n",
    "\n",
    "aggregated_output_folder = '/content/drive/MyDrive/AFP_work/esmc_struct_aggregated'\n",
    "os.makedirs(aggregated_output_folder, exist_ok=True)\n",
    "\n",
    "#==============================数据加载和预处理==============================\n",
    "#***************************加载 ESM-C 特征***************************\n",
    "def load_esmc_features(esmc_folder):\n",
    "    logits_path = os.path.join(esmc_folder, 'combined_logits.npy')\n",
    "    embeddings_path = os.path.join(esmc_folder, 'combined_embeddings.npy')\n",
    "    logits = np.load(logits_path, allow_pickle=True)\n",
    "    embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # 调试：检查 logits 的结构\n",
    "    print(f\"Logits[0] 类型: {type(logits[0])}, 值: {logits[0]}\")  #  类型 <class 'numpy.ndarray'>\n",
    "    # 打印每个样本的 logits 和 embeddings\n",
    "    print(\"Logits sample:\", logits[0])  # 打印第一个样本的 logits\n",
    "    print(\"Embeddings sample:\", embeddings[0])  # 打印第一个样本的 embeddings\n",
    "\n",
    "    # 保存特征\n",
    "    # if save_path:\n",
    "    #     np.savetxt(os.path.join(save_path, 'logits.csv'), logits, delimiter=\",\")\n",
    "    #     np.savetxt(os.path.join(save_path, 'embeddings.csv'), embeddings, delimiter=\",\")\n",
    "\n",
    "    # 从 ForwardTrackData 中提取 sequence 张量并池化\n",
    "    logits_values = []\n",
    "    for l in logits:\n",
    "        # 假设 l 是一个包含 ForwardTrackData 的数组，取第一个元素\n",
    "        forward_data = l[0] if isinstance(l, np.ndarray) else l\n",
    "        sequence_tensor = forward_data.sequence  # 获取张量\n",
    "        # 将张量移到 CPU 并转换为 float32\n",
    "        sequence_tensor = sequence_tensor.to(device='cpu', dtype=torch.float32)\n",
    "        # 对所有维度取均值，确保标量\n",
    "        pooled_value = sequence_tensor.mean(dim=[0, 1, 2]).item()  # 池化为标量\n",
    "        logits_values.append(pooled_value)\n",
    "\n",
    "    logits_values = np.array(logits_values, dtype=np.float32).reshape(-1, 1)\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    label = 1 if 'pos' in esmc_folder else 0\n",
    "    labels = np.full((logits_values.shape[0],), label)\n",
    "    return logits_values, embeddings, labels\n",
    "# 加载训练集和测试集的 ESM-C 特征\n",
    "train_pos_logits, train_pos_embeddings, train_pos_labels = load_esmc_features(esmc_folders['train_pos'])\n",
    "train_neg_logits, train_neg_embeddings, train_neg_labels = load_esmc_features(esmc_folders['train_neg'])\n",
    "test_pos_logits, test_pos_embeddings, test_pos_labels = load_esmc_features(esmc_folders['test_pos'])\n",
    "test_neg_logits, test_neg_embeddings, test_neg_labels = load_esmc_features(esmc_folders['test_neg'])\n",
    "# 合并训练集和测试集特征\n",
    "train_logits = np.vstack((train_pos_logits, train_neg_logits))\n",
    "train_embeddings = np.vstack((train_pos_embeddings, train_neg_embeddings))\n",
    "train_labels = np.hstack((train_pos_labels, train_neg_labels))\n",
    "\n",
    "test_logits = np.vstack((test_pos_logits, test_neg_logits))\n",
    "test_embeddings = np.vstack((test_pos_embeddings, test_neg_embeddings))\n",
    "test_labels = np.hstack((test_pos_labels, test_neg_labels))\n",
    "\n",
    "\n",
    "#***************************2、加载结构特征***************************\n",
    "def load_struct_features(json_path, sample_limit=5):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    data_list = []\n",
    "    for idx, sample in enumerate(tqdm(json_data, desc=f'加载结构特征 from {json_path}')):\n",
    "        required_keys = ['node_features', 'edge_features', 'label']\n",
    "        if not all(key in sample for key in required_keys):\n",
    "            print(f\"[ERROR] 样本缺少必要的键: {sample}\")\n",
    "            continue\n",
    "        node_features = sample['node_features']\n",
    "        edge_features = sample['edge_features']\n",
    "        label = sample['label']\n",
    "        edges = edge_features.get('edges', [])\n",
    "        directions = edge_features.get('directions', [])\n",
    "        rotations = edge_features.get('rotations', [])\n",
    "        num_edges = len(edges)\n",
    "        if not (len(directions) == num_edges and len(rotations) == num_edges):\n",
    "            print(f\"[ERROR] 边的数量与方向或旋转数量不匹配: {sample}\")\n",
    "            continue\n",
    "        if num_edges > 0:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            directions = torch.tensor(directions, dtype=torch.float)\n",
    "            rotations = torch.tensor(rotations, dtype=torch.float).unsqueeze(1)\n",
    "            edge_attr = torch.cat([directions, rotations], dim=1)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.empty((0, 4), dtype=torch.float)\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=label)\n",
    "        data_list.append(data)\n",
    "        if idx < sample_limit:\n",
    "            num_nodes = node_features.shape[0]\n",
    "            node_feature_dim = node_features.shape[1]\n",
    "            print(f\"样本 {idx+1}: 节点数量: {num_nodes}, 节点特征维度: {node_feature_dim}, 边数量: {num_edges}\")\n",
    "            if num_edges > 0:\n",
    "                print(f\"  边特征维度: {edge_attr.shape[1]}\")\n",
    "            print(\"-\" * 50)\n",
    "    unique_node_feature_dims = set([data.x.shape[1] for data in data_list])\n",
    "    unique_edge_feature_dims = set([data.edge_attr.shape[1] for data in data_list if data.edge_attr.shape[0] > 0])\n",
    "    print(f\"\\n所有样本中唯一的节点特征维度: {unique_node_feature_dims}\")  # 3\n",
    "    print(f\"所有样本中唯一的边特征维度: {unique_edge_feature_dims}\")  # 4\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = load_struct_features(struct_folders['train'])\n",
    "test_struct_data = load_struct_features(struct_folders['test'])\n",
    "\n",
    "for i in range(min(3, len(train_struct_data))):\n",
    "    data = train_struct_data[i]\n",
    "    print(f\"样本 {i+1} - 节点特征: {data.x.shape}, 边特征: {data.edge_attr.shape}\")\n",
    "\n",
    "##*************************** 特征标准化 ***************************\n",
    "def normalize_features(train_data_list, test_data_list=None):\n",
    "    node_scaler = StandardScaler()\n",
    "    edge_scaler = StandardScaler()\n",
    "    all_node_features = np.concatenate([data.x.numpy() for data in train_data_list], axis=0)\n",
    "    all_edge_features = np.concatenate([data.edge_attr.numpy() for data in train_data_list if data.edge_attr.shape[0] > 0], axis=0)\n",
    "    node_scaler.fit(all_node_features)\n",
    "    if all_edge_features.size > 0:\n",
    "        edge_scaler.fit(all_edge_features)\n",
    "    for data in train_data_list:\n",
    "        data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "        if data.edge_attr.shape[0] > 0:\n",
    "            data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    if test_data_list:\n",
    "        for data in test_data_list:\n",
    "            data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "            if data.edge_attr.shape[0] > 0:\n",
    "                data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    return train_data_list, test_data_list\n",
    "\n",
    "train_struct_data, test_struct_data = normalize_features(train_struct_data, test_struct_data)\n",
    "\n",
    "# #*************************** 整合 ESM-C 特征 ***************************\n",
    "def integrate_features(data_list, embeddings, logits):\n",
    "    if len(data_list) != len(embeddings) or len(data_list) != len(logits):\n",
    "        raise ValueError(f\"data_list, embeddings 和 logits 长度不匹配: {len(data_list)} vs {len(embeddings)} vs {len(logits)}\")\n",
    "    for i, data in enumerate(tqdm(data_list, desc='整合 ESM-C embeddings 和 logits')):\n",
    "        embedding = torch.tensor(embeddings[i], dtype=torch.float)  # [1152]\n",
    "        logit = torch.tensor(logits[i], dtype=torch.float).squeeze()  # [1] -> 标量\n",
    "        combined_feature = torch.cat([embedding, logit.unsqueeze(0)], dim=0)  # [1152 + 1 = 1153]\n",
    "        num_nodes = data.x.shape[0]\n",
    "        combined_expanded = combined_feature.unsqueeze(0).repeat(num_nodes, 1)  # [num_nodes, 1153]\n",
    "        data.x = torch.cat([data.x, combined_expanded], dim=1)  # [num_nodes, 3 + 1153 = 1156]\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = integrate_features(train_struct_data, train_embeddings, train_logits)\n",
    "test_struct_data = integrate_features(test_struct_data, test_embeddings, test_logits)\n",
    "\n",
    "print(f\"训练集第一个样本的节点特征维度（整合后）: {train_struct_data[0].x.shape[1]}\")  # 1156\n",
    "print(f\"测试集第一个样本的节点特征维度（整合后）: {test_struct_data[0].x.shape[1]}\") # 1156\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(ProteinDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_dataset = ProteinDataset(train_struct_data)\n",
    "test_dataset = ProteinDataset(test_struct_data)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5rxB9opFQZv"
   },
   "outputs": [],
   "source": [
    "class DeepGATModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, hidden_dim, out_dim, num_heads=4, dropout=0.3, num_layers=3):\n",
    "        super(DeepGATModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 边特征预处理层\n",
    "        self.edge_preprocess = nn.Sequential(\n",
    "            nn.Linear(edge_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # 堆叠 GAT 层\n",
    "        for layer in range(num_layers):\n",
    "            in_dim = node_feature_dim if layer == 0 else hidden_dim * num_heads\n",
    "            self.convs.append(GATConv(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                edge_dim=hidden_dim,  # 调整为预处理后的边特征维度\n",
    "                add_self_loops=True  # 添加自环，增强稳定性。能提升稳定性（每个节点至少保留自身信息）\n",
    "            ))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n",
    "\n",
    "        # 替换 Set2Set 为更简单的池化\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_heads, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        # 预处理边特征\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, out_dim, num_layers=3, dropout=0.5):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.convs.append(SAGEConv(node_feature_dim, hidden_dim))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class GINModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, out_dim, num_layers=3, dropout=0.5):\n",
    "        super(GINModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                nn_lin = nn.Sequential(nn.Linear(node_feature_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "            else:\n",
    "                nn_lin = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.convs.append(GINConv(nn_lin))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# 交叉注意力融合模块\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads=4, dropout=0.1):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        # 交叉注意力机制\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "        self.fc = nn.Linear(feature_dim, 2)  # 最终分类层，输出 2 类\n",
    "\n",
    "    def forward(self, features_list):\n",
    "        # features_list: [model1_features, model2_features, model3_features], 每个形状为 [batch_size, feature_dim]\n",
    "        # 堆叠特征为 [num_models, batch_size, feature_dim]\n",
    "        feats = torch.stack(features_list, dim=0)   # features_list = [feat_gat, feat_sage, feat_gin]\n",
    "        # 应用交叉注意力\n",
    "        attn_output, _ = self.attention(feats, feats, feats)\n",
    "        # 融合特征：取平均值\n",
    "        fused_feats = attn_output.mean(dim=0)  # [batch_size, feature_dim]\n",
    "        fused_feats = self.norm(fused_feats)\n",
    "        # 分类\n",
    "        out = self.fc(fused_feats)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=10, model_save_path='best_model.pth'):\n",
    "    best_test_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in tqdm(train_loader, desc=f'训练 Epoch {epoch}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "        train_acc, _, _ = test(model, train_loader, device)\n",
    "        test_acc, test_trues, test_preds = test(model, test_loader, device)\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"早停：在第 {epoch} 轮训练后，无提升，停止训练。\")\n",
    "                break\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return best_test_acc, best_model_wts\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='评估'):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            trues.extend(data.y.cpu().numpy())\n",
    "            correct += (pred == data.y).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy, trues, preds\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "\n",
    "def detailed_test(model, loader, device, models=None):\n",
    "    model.eval()\n",
    "    preds, trues, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='详细评估'):\n",
    "            data = data.to(device)\n",
    "            if isinstance(model, CrossAttentionFusion):  # 检查是否为融合模型\n",
    "                if models is None:\n",
    "                    raise ValueError(\"models dictionary required for CrossAttentionFusion evaluation\")\n",
    "                # 提取特征列表\n",
    "                feat_gat = models['DeepGATModel'].get_last_layer_features(data)\n",
    "                feat_sage = models['GraphSAGEModel'].get_last_layer_features(data)\n",
    "                feat_gin = models['GINModel'].get_last_layer_features(data)\n",
    "                features_list = [feat_gat, feat_sage, feat_gin]\n",
    "                out = model(features_list)\n",
    "            else:\n",
    "                out = model(data)  # 普通模型直接处理 DataBatch\n",
    "            prob = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "            true = data.y.cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            trues.extend(true)\n",
    "            probs.extend(prob)\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(trues, preds, average='binary')\n",
    "    mcc = matthews_corrcoef(trues, preds)\n",
    "    auc = roc_auc_score(trues, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(trues, preds).ravel()\n",
    "    sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics = {'acc': acc, 'mcc': mcc, 'auc': auc, 'sn': sn, 'sp': sp, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    return metrics\n",
    "\n",
    "def optimize_model(model_class, train_loader, test_loader, device, model_params, n_trials=5):\n",
    "    def objective(trial):\n",
    "        # 定义超参数搜索空间\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 64, 512)\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "\n",
    "        # 根据模型类初始化模型\n",
    "        if model_class == DeepGATModel:\n",
    "            num_heads = trial.suggest_int('num_heads', 2, 16)\n",
    "            model = DeepGATModel(\n",
    "                node_feature_dim=model_params['node_feature_dim'],\n",
    "                edge_feature_dim=model_params['edge_feature_dim'],\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=model_params['out_dim'],\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                num_layers=num_layers\n",
    "            ).to(device)\n",
    "        elif model_class == GraphSAGEModel:\n",
    "            model = GraphSAGEModel(\n",
    "                node_feature_dim=model_params['node_feature_dim'],\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=model_params['out_dim'],\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "        elif model_class == GINModel:\n",
    "            model = GINModel(\n",
    "                node_feature_dim=model_params['node_feature_dim'],\n",
    "                hidden_dim=hidden_dim,\n",
    "                out_dim=model_params['out_dim'],\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        best_acc, _ = train_model(\n",
    "            model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
    "            device, num_epochs=50, patience=10, model_save_path=f\"best_{model_class.__name__}.pth\"\n",
    "        )\n",
    "        return best_acc\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=5)\n",
    "    print(f\"总试验次数: {len(study.trials)}\")\n",
    "    print(f\"{model_class.__name__} 最佳超参数: {study.best_params}\")\n",
    "    for trial in study.trials:\n",
    "      print(f\"Trial {trial.number}: State={trial.state}, Value={trial.value}\")\n",
    "    return study.best_params\n",
    "\n",
    "def explain_features(model, test_loader, device, output_folder):\n",
    "    # ESM-C 特征的 SHAP 分析\n",
    "    print(\"正在进行 ESM-C 特征的 SHAP 分析...\")\n",
    "    esmc_features = np.hstack([train_embeddings, train_logits])\n",
    "    labels = train_labels\n",
    "    proxy_model = XGBClassifier()\n",
    "    proxy_model.fit(esmc_features, labels)\n",
    "    explainer = shap.Explainer(proxy_model)\n",
    "    shap_values = explainer(esmc_features)\n",
    "    shap.summary_plot(shap_values, esmc_features, plot_type=\"bar\", show=False)\n",
    "    plt.title(\"ESM-C 特征重要性 (SHAP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, \"shap_esmc_features.png\"))\n",
    "    plt.close()\n",
    "    print(\"SHAP 分析完成，结果已保存至 shap_esmc_features.png\")\n",
    "\n",
    "    #GNNExplainer 分析（以 DeepGATModel 为例）\n",
    "    print(\"正在进行 GNNExplainer 分析...\")\n",
    "    trained_model = models['DeepGATModel']\n",
    "    explainer = GNNExplainer(trained_model, epochs=200, lr=0.01)\n",
    "    for sample_idx in range(min(5, len(test_struct_data))):\n",
    "        data = test_struct_data[sample_idx].to(device)\n",
    "        node_idx = 0  # 分析第一个节点\n",
    "        node_feat_mask, edge_mask = explainer.explain_node(node_idx, data.x, data.edge_index, data.edge_attr)\n",
    "        print(f\"样本 {sample_idx+1} | 节点 0 特征重要性（前5个）: {node_feat_mask[:5]} | 边重要性（前5个）: {edge_mask[:5]}\")\n",
    "    print(\"GNNExplainer 分析完成\")\n",
    "\n",
    "    # t-SNE 可视化\n",
    "    print(\"正在进行 t-SNE 可视化...\")\n",
    "    def get_last_layer_features(model, loader, device):\n",
    "        model.eval()\n",
    "        features = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                feat = model.get_last_layer_features(data)\n",
    "                features.append(feat.cpu().numpy())\n",
    "                labels.append(data.y.cpu().numpy())\n",
    "        return np.vstack(features), np.hstack(labels)\n",
    "\n",
    "    features, labels = get_last_layer_features(trained_model, test_loader, device)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels, cmap='coolwarm', alpha=0.6)\n",
    "    plt.title(\"t-SNE of Last Layer Features (DeepGATModel)\")\n",
    "    plt.colorbar(label='Class')\n",
    "    plt.savefig(os.path.join(output_folder, \"tsne_last_layer.png\"))\n",
    "    plt.close()\n",
    "    print(\"t-SNE 可视化完成，结果已保存至 tsne_last_layer.png\")\n",
    "\n",
    "    return results, models\n",
    "\n",
    "def train_and_evaluate_models(train_loader, test_loader, device, output_folder):\n",
    "    model_params = {\"node_feature_dim\": 1156, \"edge_feature_dim\": 4, \"out_dim\": 2}\n",
    "    best_params = {}\n",
    "\n",
    "    # 优化并训练三个基础模型\n",
    "    for model_class in [DeepGATModel, GraphSAGEModel, GINModel]:\n",
    "        print(f\"优化 {model_class.__name__}...\")\n",
    "        best_params[model_class.__name__] = optimize_model(model_class, train_loader, test_loader, device, model_params, n_trials=10)\n",
    "\n",
    "    # 初始化模型\n",
    "    models = {\n",
    "        \"DeepGATModel\": DeepGATModel(\n",
    "            node_feature_dim=1156, edge_feature_dim=4, out_dim=2,\n",
    "            hidden_dim=best_params[\"DeepGATModel\"][\"hidden_dim\"],\n",
    "            num_layers=best_params[\"DeepGATModel\"][\"num_layers\"],\n",
    "            dropout=best_params[\"DeepGATModel\"][\"dropout\"],\n",
    "            num_heads=best_params[\"DeepGATModel\"][\"num_heads\"]\n",
    "        ).to(device),\n",
    "        \"GraphSAGEModel\": GraphSAGEModel(\n",
    "            node_feature_dim=1156, out_dim=2,\n",
    "            hidden_dim=best_params[\"GraphSAGEModel\"][\"hidden_dim\"],\n",
    "            num_layers=best_params[\"GraphSAGEModel\"][\"num_layers\"],\n",
    "            dropout=best_params[\"GraphSAGEModel\"][\"dropout\"]\n",
    "        ).to(device),\n",
    "        \"GINModel\": GINModel(\n",
    "            node_feature_dim=1156, out_dim=2,\n",
    "            hidden_dim=best_params[\"GINModel\"][\"hidden_dim\"],\n",
    "            num_layers=best_params[\"GINModel\"][\"num_layers\"],\n",
    "            dropout=best_params[\"GINModel\"][\"dropout\"]\n",
    "        ).to(device)\n",
    "    }\n",
    "\n",
    "    # 训练并评估每个模型\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=best_params[name][\"lr\"], weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        save_path = os.path.join(output_folder, f\"best_{name}.pth\")\n",
    "        best_acc, _ = train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=10, model_save_path=save_path)\n",
    "        metrics = detailed_test(model, test_loader, device)\n",
    "        results[name] = metrics\n",
    "        print(f\"{name} - Acc: {metrics['acc']:.4f}, MCC: {metrics['mcc']:.4f}, AUC: {metrics['auc']:.4f}\")\n",
    "\n",
    "    # 性能对比\n",
    "    print(\"\\n### 三个模型性能对比 ###\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"{name}: Acc: {metrics['acc']:.4f}, MCC: {metrics['mcc']:.4f}, AUC: {metrics['auc']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "    # 加载最佳模型\n",
    "    for name, model in models.items():\n",
    "        model.load_state_dict(torch.load(os.path.join(output_folder, f\"best_{name}.pth\")))\n",
    "        model.eval()\n",
    "\n",
    "    return results, models\n",
    "\n",
    "# 交叉注意力融合训练\n",
    "# 在交叉注意力融合训练函数中修改\n",
    "def train_cross_attention_fusion(models, train_loader, test_loader, device, output_folder, num_epochs=50, patience=10):\n",
    "    fusion_module = CrossAttentionFusion(feature_dim=256, num_heads=4, dropout=0.1).to(device)\n",
    "    optimizer_fusion = torch.optim.Adam(fusion_module.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler_fusion = torch.optim.lr_scheduler.StepLR(optimizer_fusion, step_size=10, gamma=0.1)\n",
    "\n",
    "    print(\"\\n### 训练交叉注意力融合模块 ###\")\n",
    "    best_fusion_acc = 0\n",
    "    best_fusion_wts = copy.deepcopy(fusion_module.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        fusion_module.train()\n",
    "        total_loss = 0\n",
    "        for data in tqdm(train_loader, desc=f'融合训练 Epoch {epoch}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                # 确保获取的是纯张量\n",
    "                feat_gat = models['DeepGATModel'].get_last_layer_features(data)\n",
    "                feat_sage = models['GraphSAGEModel'].get_last_layer_features(data)\n",
    "                feat_gin = models['GINModel'].get_last_layer_features(data)\n",
    "                # 调试：打印特征形状和类型\n",
    "                print(f\"feat_gat shape: {feat_gat.shape}, type: {type(feat_gat)}\")\n",
    "                print(f\"feat_sage shape: {feat_sage.shape}, type: {type(feat_sage)}\")\n",
    "                print(f\"feat_gin shape: {feat_gin.shape}, type: {type(feat_gin)}\")\n",
    "                # 如果返回的是 DataBatch，提取特征张量\n",
    "                if isinstance(feat_gat, torch.Tensor) and feat_gat.dim() == 2:  # 确保是 [batch_size, feature_dim]\n",
    "                    features_list = [feat_gat, feat_sage, feat_gin]\n",
    "                else:\n",
    "                    raise ValueError(\"Expected pure tensors from get_last_layer_features, got unexpected type or shape\")\n",
    "            out = fusion_module(features_list)\n",
    "            loss = criterion(out, data.y)\n",
    "            optimizer_fusion.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_fusion.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        scheduler_fusion.step()\n",
    "\n",
    "        fusion_module.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                data = data.to(device)\n",
    "                feat_gat = models['DeepGATModel'].get_last_layer_features(data)\n",
    "                feat_sage = models['GraphSAGEModel'].get_last_layer_features(data)\n",
    "                feat_gin = models['GINModel'].get_last_layer_features(data)\n",
    "                # 同样的检查和处理\n",
    "                if isinstance(feat_gat, torch.Tensor) and feat_gat.dim() == 2:\n",
    "                    features_list = [feat_gat, feat_sage, feat_gin]\n",
    "                else:\n",
    "                    raise ValueError(\"Expected pure tensors from get_last_layer_features in test loop\")\n",
    "                out = fusion_module(features_list)\n",
    "                pred = out.argmax(dim=1)\n",
    "                preds.extend(pred.cpu().numpy())\n",
    "                trues.extend(data.y.cpu().numpy())\n",
    "        test_acc = accuracy_score(trues, preds)\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {avg_loss:.4f}, Fusion Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        if test_acc > best_fusion_acc:\n",
    "            best_fusion_acc = test_acc\n",
    "            best_fusion_wts = copy.deepcopy(fusion_module.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(fusion_module.state_dict(), os.path.join(output_folder, \"best_cross_attention_fusion.pth\"))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"早停：在第 {epoch} 轮训练后，无提升，停止训练。\")\n",
    "                break\n",
    "\n",
    "    fusion_module.load_state_dict(best_fusion_wts)\n",
    "    fusion_metrics = detailed_test(fusion_module, test_loader, device, models=models)  # 传递 models 参数\n",
    "    return fusion_module, fusion_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3797213,
     "status": "ok",
     "timestamp": 1759399696297,
     "user": {
      "displayName": "Linnaea",
      "userId": "04086206744193458339"
     },
     "user_tz": -480
    },
    "id": "hj-siF4M54al",
    "outputId": "0da1ee9e-15d0-4f14-fb2f-d99737acc934"
   },
   "outputs": [],
   "source": [
    "# ### 运行实验\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    results, models = train_and_evaluate_models(train_loader, test_loader, device, aggregated_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "v-TWJS8_bYqw",
    "outputId": "c0f3eceb-1170-4cf6-ecf7-5ceae06340e0"
   },
   "outputs": [],
   "source": [
    "fusion_module, fusion_metrics = train_cross_attention_fusion(models, train_loader, test_loader, device, aggregated_output_folder)\n",
    "results[\"CrossAttentionFusion\"] = fusion_metrics\n",
    "print(f\"CrossAttentionFusion - Acc: {fusion_metrics['acc']:.4f}, MCC: {fusion_metrics['mcc']:.4f}, AUC: {fusion_metrics['auc']:.4f}, Precision: {fusion_metrics['precision']:.4f}, Recall: {fusion_metrics['recall']:.4f}, F1: {fusion_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIlrqhJhTvaP"
   },
   "source": [
    "# 单调GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5BI2ntGwZFJ8",
    "outputId": "46977f40-9e3b-40ca-afc8-250b22c50712"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, precision_recall_fscore_support,\n",
    "    matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import copy\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置设备（GPU/CPU）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 路径配置\n",
    "esmc_folders = {\n",
    "    'train_pos': '/content/drive/MyDrive/AFP_work/esmc_600_train_pos',\n",
    "    'train_neg': '/content/drive/MyDrive/AFP_work/esmc_600_train_neg',\n",
    "    'test_pos': '/content/drive/MyDrive/AFP_work/esmc_600_test_pos',\n",
    "    'test_neg': '/content/drive/MyDrive/AFP_work/esmc_600_test_neg'\n",
    "}\n",
    "\n",
    "struct_folders = {\n",
    "    'train': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/train_dataset.json',\n",
    "    'test': '/content/drive/MyDrive/AFP_work/pdb_features/aggregated/test_dataset.json'\n",
    "}\n",
    "\n",
    "output_folder = '/content/drive/MyDrive/AFP_work/deepgat_explain_results'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ### 数据加载和预处理\n",
    "\n",
    "# **加载 ESM-C 特征**\n",
    "def load_esmc_features(esmc_folder, save_path=None):\n",
    "    logits_path = os.path.join(esmc_folder, 'combined_logits.npy')\n",
    "    embeddings_path = os.path.join(esmc_folder, 'combined_embeddings.npy')\n",
    "    logits = np.load(logits_path, allow_pickle=True)\n",
    "    embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # 调试：检查 logits 和 embeddings 的结构\n",
    "    print(f\"Logits[0] 类型: {type(logits[0])}, 值: {logits[0]}\")\n",
    "    print(f\"Embeddings[0] 形状: {embeddings[0].shape}, 样本: {embeddings[0][:5]}\")\n",
    "\n",
    "    # 从 ForwardTrackData 中提取 sequence 张量并池化\n",
    "    logits_values = []\n",
    "    for l in logits:\n",
    "        forward_data = l[0] if isinstance(l, np.ndarray) else l\n",
    "        sequence_tensor = forward_data.sequence  # 获取张量\n",
    "        sequence_tensor = sequence_tensor.to(device='cpu', dtype=torch.float32)\n",
    "        pooled_value = sequence_tensor.mean(dim=[0, 1, 2]).item()  # 池化为标量\n",
    "        logits_values.append(pooled_value)\n",
    "\n",
    "    logits_values = np.array(logits_values, dtype=np.float32).reshape(-1, 1)\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    label = 1 if 'pos' in esmc_folder else 0\n",
    "    labels = np.full((logits_values.shape[0],), label)\n",
    "    return logits_values, embeddings, labels\n",
    "\n",
    "# 加载训练集和测试集的 ESM-C 特征\n",
    "train_pos_logits, train_pos_embeddings, train_pos_labels = load_esmc_features(esmc_folders['train_pos'])\n",
    "train_neg_logits, train_neg_embeddings, train_neg_labels = load_esmc_features(esmc_folders['train_neg'])\n",
    "test_pos_logits, test_pos_embeddings, test_pos_labels = load_esmc_features(esmc_folders['test_pos'])\n",
    "test_neg_logits, test_neg_embeddings, test_neg_labels = load_esmc_features(esmc_folders['test_neg'])\n",
    "\n",
    "# 合并训练集和测试集特征\n",
    "train_logits = np.vstack((train_pos_logits, train_neg_logits))\n",
    "train_embeddings = np.vstack((train_pos_embeddings, train_neg_embeddings))\n",
    "train_labels = np.hstack((train_pos_labels, train_neg_labels))\n",
    "\n",
    "test_logits = np.vstack((test_pos_logits, test_neg_logits))\n",
    "test_embeddings = np.vstack((test_pos_embeddings, test_neg_embeddings))\n",
    "test_labels = np.hstack((test_pos_labels, test_neg_labels))\n",
    "\n",
    "print(f\"训练集 logits 形状: {train_logits.shape}\")\n",
    "print(f\"训练集 embeddings 形状: {train_embeddings.shape}\")\n",
    "print(f\"训练集 labels 形状: {train_labels.shape}\")\n",
    "print(f\"测试集 logits 形状: {test_logits.shape}\")\n",
    "print(f\"测试集 embeddings 形状: {test_embeddings.shape}\")\n",
    "print(f\"测试集 labels 形状: {test_labels.shape}\")\n",
    "\n",
    "# **加载结构特征**\n",
    "def load_struct_features(json_path, sample_limit=5):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    data_list = []\n",
    "    for idx, sample in enumerate(tqdm(json_data, desc=f'加载结构特征 from {json_path}')):\n",
    "        required_keys = ['node_features', 'edge_features', 'label']\n",
    "        if not all(key in sample for key in required_keys):\n",
    "            print(f\" 样本缺少必要的键: {sample}\")\n",
    "            continue\n",
    "        node_features = sample['node_features']\n",
    "        edge_features = sample['edge_features']\n",
    "        label = sample['label']\n",
    "        edges = edge_features.get('edges', [])\n",
    "        directions = edge_features.get('directions', [])\n",
    "        rotations = edge_features.get('rotations', [])\n",
    "        num_edges = len(edges)\n",
    "        if not (len(directions) == num_edges and len(rotations) == num_edges):\n",
    "            print(f\" 边的数量与方向或旋转数量不匹配: {sample}\")\n",
    "            continue\n",
    "        if num_edges > 0:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            directions = torch.tensor(directions, dtype=torch.float)\n",
    "            rotations = torch.tensor(rotations, dtype=torch.float).unsqueeze(1)\n",
    "            edge_attr = torch.cat([directions, rotations], dim=1)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.empty((0, 4), dtype=torch.float)\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=label)\n",
    "        data_list.append(data)\n",
    "        if idx < sample_limit:\n",
    "            num_nodes = node_features.shape[0]\n",
    "            node_feature_dim = node_features.shape[1]\n",
    "            print(f\"样本 {idx+1}: 节点数量: {num_nodes}, 节点特征维度: {node_feature_dim}, 边数量: {num_edges}\")\n",
    "            if num_edges > 0:\n",
    "                print(f\"  边特征维度: {edge_attr.shape[1]}\")\n",
    "            print(\"-\" * 50)\n",
    "    print(f\"\\n所有样本中唯一的节点特征维度: {set([data.x.shape[1] for data in data_list])}\")\n",
    "    print(f\"所有样本中唯一的边特征维度: {set([data.edge_attr.shape[1] for data in data_list if data.edge_attr.shape[0] > 0])}\")\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = load_struct_features(struct_folders['train'])\n",
    "test_struct_data = load_struct_features(struct_folders['test'])\n",
    "\n",
    "# **特征标准化**\n",
    "def normalize_features(train_data_list, test_data_list=None):\n",
    "    node_scaler = StandardScaler()\n",
    "    edge_scaler = StandardScaler()\n",
    "    all_node_features = np.concatenate([data.x.numpy() for data in train_data_list], axis=0)\n",
    "    all_edge_features = np.concatenate([data.edge_attr.numpy() for data in train_data_list if data.edge_attr.shape[0] > 0], axis=0)\n",
    "    node_scaler.fit(all_node_features)\n",
    "    if all_edge_features.size > 0:\n",
    "        edge_scaler.fit(all_edge_features)\n",
    "    for data in train_data_list:\n",
    "        data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "        if data.edge_attr.shape[0] > 0:\n",
    "            data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    if test_data_list:\n",
    "        for data in test_data_list:\n",
    "            data.x = torch.tensor(node_scaler.transform(data.x.numpy()), dtype=torch.float)\n",
    "            if data.edge_attr.shape[0] > 0:\n",
    "                data.edge_attr = torch.tensor(edge_scaler.transform(data.edge_attr.numpy()), dtype=torch.float)\n",
    "    return train_data_list, test_data_list\n",
    "\n",
    "train_struct_data, test_struct_data = normalize_features(train_struct_data, test_struct_data)\n",
    "\n",
    "# **整合 ESM-C 特征**\n",
    "def integrate_features(data_list, embeddings, logits):\n",
    "    if len(data_list) != len(embeddings) or len(data_list) != len(logits):\n",
    "        raise ValueError(f\"data_list, embeddings 和 logits 长度不匹配: {len(data_list)} vs {len(embeddings)} vs {len(logits)}\")\n",
    "    for i, data in enumerate(tqdm(data_list, desc='整合 ESM-C embeddings 和 logits')):\n",
    "        embedding = torch.tensor(embeddings[i], dtype=torch.float)  # [1152]\n",
    "        logit = torch.tensor(logits[i], dtype=torch.float).squeeze()  # [1] -> 标量\n",
    "        combined_feature = torch.cat([embedding, logit.unsqueeze(0)], dim=0)  # [1153]\n",
    "        num_nodes = data.x.shape[0]\n",
    "        combined_expanded = combined_feature.unsqueeze(0).repeat(num_nodes, 1)  # [num_nodes, 1153]\n",
    "        data.x = torch.cat([data.x, combined_expanded], dim=1)  # [num_nodes, 1156]\n",
    "    return data_list\n",
    "\n",
    "train_struct_data = integrate_features(train_struct_data, train_embeddings, train_logits)\n",
    "test_struct_data = integrate_features(test_struct_data, test_embeddings, test_logits)\n",
    "\n",
    "print(f\"训练集第一个样本的节点特征维度（整合后）: {train_struct_data[0].x.shape[1]}\")\n",
    "print(f\"测试集第一个样本的节点特征维度（整合后）: {test_struct_data[0].x.shape[1]}\")\n",
    "\n",
    "# **创建数据集和数据加载器**\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(ProteinDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_dataset = ProteinDataset(train_struct_data)\n",
    "test_dataset = ProteinDataset(test_struct_data)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ### 调试 DeepGATModel\n",
    "class DeepGATModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, hidden_dim, out_dim, num_heads=4, dropout=0.3, num_layers=3):\n",
    "        super(DeepGATModel, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 边特征预处理层\n",
    "        self.edge_preprocess = nn.Sequential(\n",
    "            nn.Linear(edge_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # 初始化GAT层\n",
    "        for layer in range(num_layers):\n",
    "            in_dim = node_feature_dim if layer == 0 else hidden_dim * num_heads\n",
    "            self.convs.append(GATConv(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                edge_dim=hidden_dim,  # 预处理后的边特征维度\n",
    "                add_self_loops=True  # 添加自环，增强稳定性\n",
    "            ))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n",
    "\n",
    "        # 池化和全连接层\n",
    "        self.readout = global_mean_pool\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_heads, 256)\n",
    "        self.fc2 = nn.Linear(256, out_dim)\n",
    "\n",
    "    def forward(self, data, print_shapes=False):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # 仅在第一次批次打印形状（每轮试验一次）\n",
    "        if print_shapes:\n",
    "            print(f\"输入 - 节点特征形状: {x.shape}, 边特征形状: {edge_attr.shape if edge_attr is not None else '无'}, 边索引形状: {edge_index.shape}\")\n",
    "\n",
    "        # 预处理边特征\n",
    "        if edge_attr is not None and edge_attr.shape[0] > 0:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "            if print_shapes:\n",
    "                print(f\"预处理后边特征形状: {edge_attr.shape}\")\n",
    "        else:\n",
    "            edge_attr = None\n",
    "            if print_shapes:\n",
    "                print(\"无边特征，使用默认边处理\")\n",
    "\n",
    "        # 逐层处理\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.batch_norms)):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            if print_shapes:\n",
    "                print(f\"GATConv层 {i+1} 输出形状: {x.shape}\")\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 池化\n",
    "        x = self.readout(x, batch)\n",
    "        if print_shapes:\n",
    "            print(f\"池化后特征形状: {x.shape}\")\n",
    "\n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        if print_shapes:\n",
    "            print(f\"FC1输出形状: {x.shape}\")\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        if print_shapes:\n",
    "            print(f\"FC2输出形状: {x.shape}\")  # 应为 [batch_size, 2]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_last_layer_features(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        if edge_attr is not None and edge_attr.shape[0] > 0:\n",
    "            edge_attr = self.edge_preprocess(edge_attr)\n",
    "        else:\n",
    "            edge_attr = None\n",
    "\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = bn(x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.readout(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# ### 训练和调试 DeepGATModel\n",
    "def train_deepgat(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=10, model_save_path='best_deepgat.pth'):\n",
    "    best_test_acc = 0\n",
    "    best_model_wts = None  # 初始值设置为None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        # 仅在第一个批次打印形状（每轮试验一次）\n",
    "        print_shapes = (epoch == 1)  # 仅第一轮打印\n",
    "        for data in tqdm(train_loader, desc=f'训练 DeepGAT Epoch {epoch}/{num_epochs}'):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data, print_shapes=print_shapes)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            if print_shapes:\n",
    "                print(f\"批次损失: {loss.item():.4f}\")\n",
    "                print_shapes = False  # 仅打印一次\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "        train_acc, train_trues, train_preds = test(model, train_loader, device)\n",
    "        test_acc, test_trues, test_preds = test(model, test_loader, device)\n",
    "        print(f\"Epoch: {epoch:02d}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())  # 更新最佳权重\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(best_model_wts, model_save_path)  # 保存最佳模型\n",
    "            print(f\"保存最佳模型，测试准确率: {test_acc:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"早停：在第 {epoch} 轮训练后，无提升，停止训练。\")\n",
    "                break\n",
    "\n",
    "    # 加载最佳权重\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    else:\n",
    "        print(\"警告：未找到最佳权重，使用当前模型状态。\")\n",
    "    return best_test_acc, best_model_wts\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='评估 DeepGAT'):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            trues.extend(data.y.cpu().numpy())\n",
    "            correct += (pred == data.y).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy, trues, preds\n",
    "\n",
    "def detailed_test(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, trues, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc='详细评估 DeepGAT'):\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            prob = F.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "            true = data.y.cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            trues.extend(true)\n",
    "            probs.extend(prob)\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(trues, preds, average='binary')\n",
    "    mcc = matthews_corrcoef(trues, preds)\n",
    "    auc = roc_auc_score(trues, probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(trues, preds).ravel()\n",
    "    sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics = {'acc': acc, 'mcc': mcc, 'auc': auc, 'sn': sn, 'sp': sp, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    return metrics\n",
    "\n",
    "# ### 超参数优化 (Optuna)\n",
    "\n",
    "def optimize_deepgat(train_loader, test_loader, device, n_trials=10):\n",
    "    def objective(trial):\n",
    "        # 定义超参数搜索空间\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 64, 512)\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "        num_heads = trial.suggest_int('num_heads', 2, 16)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "\n",
    "        # 初始化 DeepGATModel\n",
    "        model = DeepGATModel(\n",
    "            node_feature_dim=1156,  # 整合后的节点特征维度\n",
    "            edge_feature_dim=4,     # 边特征维度\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_dim=2,             # 二分类\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            num_layers=num_layers\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        # 训练并返回最佳准确率和权重\n",
    "        best_acc, best_wts = train_deepgat(\n",
    "            model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
    "            device, num_epochs=50, patience=10, model_save_path='best_deepgat_optimized.pth'\n",
    "        )\n",
    "        return best_acc\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    print(f\"总试验次数: {len(study.trials)}\")\n",
    "    print(f\"DeepGATModel 最佳超参数: {study.best_params}\")\n",
    "    for trial in study.trials:\n",
    "        print(f\"Trial {trial.number}: State={trial.state}, Value={trial.value}\")\n",
    "    return study.best_params\n",
    "\n",
    "# ### 可解释性分析函数\n",
    "\n",
    "def explain_deepgat(model, train_loader, test_loader, device, output_folder):\n",
    "    # 1. SHAP分析（ESM-C特征）\n",
    "    print(\"正在进行 ESM-C 特征的 SHAP 分析...\")\n",
    "    esmc_features = np.hstack([train_embeddings, train_logits])  # [num_samples, 1153]\n",
    "    labels = train_labels\n",
    "    proxy_model = XGBClassifier()\n",
    "    proxy_model.fit(esmc_features, labels)\n",
    "    explainer = shap.Explainer(proxy_model)\n",
    "    shap_values = explainer(esmc_features)\n",
    "    # 绘制SHAP特征重要性\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, esmc_features, plot_type=\"bar\", show=False)\n",
    "    plt.title(\"ESM-C 特征重要性 (SHAP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, \"shap_esmc_features.png\"))\n",
    "    plt.close()\n",
    "    print(\"SHAP 分析完成，结果已保存至 shap_esmc_features.png\")\n",
    "\n",
    "    # 2. GNNExplainer分析（节点和边的重要性）\n",
    "    print(\"正在进行 GNNExplainer 分析...\")\n",
    "    model.eval()\n",
    "    reset(model)  # 重置模型参数以确保解释一致性\n",
    "    explainer = GNNExplainer(model, epochs=200, lr=0.01)\n",
    "    for sample_idx in range(min(5, len(test_struct_data))):  # 分析前5个测试样本\n",
    "        data = test_struct_data[sample_idx].to(device)\n",
    "        node_idx = 0  # 分析第一个节点\n",
    "        node_feat_mask, edge_mask = explainer.explain_node(node_idx, data.x, data.edge_index, data.edge_attr)\n",
    "        print(f\"样本 {sample_idx+1} | 节点 {node_idx} 特征重要性（前5个）: {node_feat_mask[:5]} | 边重要性（前5个）: {edge_mask[:5] if edge_mask is not None else '无'}\")\n",
    "    print(\"GNNExplainer 分析完成\")\n",
    "\n",
    "    # 3. t-SNE可视化（最后层特征分布）\n",
    "    print(\"正在进行 t-SNE 可视化...\")\n",
    "    def get_last_layer_features(model, loader, device):\n",
    "        model.eval()\n",
    "        features = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                feat = model.get_last_layer_features(data)\n",
    "                features.append(feat.cpu().numpy())\n",
    "                labels.append(data.y.cpu().numpy())\n",
    "        return np.vstack(features), np.hstack(labels)\n",
    "\n",
    "    features, labels = get_last_layer_features(model, test_loader, device)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels, cmap='coolwarm', alpha=0.6)\n",
    "    plt.title(\"DeepGATModel 最后层特征 t-SNE 可视化\")\n",
    "    plt.colorbar(label='Class (0=Neg, 1=Pos)')\n",
    "    plt.savefig(os.path.join(output_folder, \"tsne_deepgat_features.png\"))\n",
    "    plt.close()\n",
    "    print(\"t-SNE 可视化完成，结果已保存至 tsne_deepgat_features.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    deepgat_model = DeepGATModel(\n",
    "        node_feature_dim=1156,\n",
    "        edge_feature_dim=4,\n",
    "        hidden_dim=256,   \n",
    "        out_dim=2,        \n",
    "        num_heads=4,            \n",
    "        dropout=0.3,            \n",
    "        num_layers=3           \n",
    "    ).to(device)\n",
    "\n",
    "    print(\"开始优化 DeepGATModel 超参数...\")\n",
    "    best_params = optimize_deepgat(train_loader, test_loader, device, n_trials=10)\n",
    "\n",
    "    # 使用最佳超参数初始化并训练模型\n",
    "    deepgat_model = DeepGATModel(\n",
    "        node_feature_dim=1156,\n",
    "        edge_feature_dim=4,\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        out_dim=2,\n",
    "        num_heads=best_params['num_heads'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_layers=best_params['num_layers']\n",
    "    ).to(device)\n",
    "\n",
    "    # 训练模型\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(deepgat_model.parameters(), lr=best_params['lr'], weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    best_acc, best_wts = train_deepgat(\n",
    "        deepgat_model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
    "        device, num_epochs=50, patience=10, model_save_path=os.path.join(output_folder, 'best_deepgat_final.pth')\n",
    "    )\n",
    "\n",
    "    # 详细评估\n",
    "    metrics = detailed_test(deepgat_model, test_loader, device)\n",
    "    print(\"\\n### DeepGATModel 详细性能 ###\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # 保存模型和结果\n",
    "    if best_wts is not None:\n",
    "        torch.save(best_wts, os.path.join(output_folder, 'best_deepgat_final.pth'))\n",
    "    with open(os.path.join(output_folder, 'deepgat_metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    # 调试：打印部分预测结果\n",
    "    deepgat_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = deepgat_model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            true = data.y\n",
    "            print(f\"预测样本 - 真实标签: {true[:5].cpu().numpy()}, 预测标签: {pred[:5].cpu().numpy()}\")\n",
    "            break  # 仅打印第一个批次\n",
    "\n",
    "    # 可解释性分析\n",
    "    explain_deepgat(deepgat_model, train_loader, test_loader, device, output_folder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "a8BDbX0lIZSs",
    "6dUEQFzFIb3o",
    "JnqCMjKgIhng",
    "mBHmxnQzo6b6",
    "5Za8fI8fov8b",
    "fz6lWj8Lx6a_",
    "LADKYcKABvi8",
    "WOq4HHPYwv_h",
    "5RnxO1AX2u46",
    "JQqE1ofqsviH",
    "m0LT6AZYEBL3",
    "RQ-gFfgEuGH0",
    "sf4bWPY6cXlK",
    "4dxS0Eu0uKOq",
    "l2bQyS6_wILJ",
    "uYVaYr92Mecm",
    "4_RLor2ULZf0",
    "KrR3SQWpO5qt",
    "-pnBCTCqhdFH",
    "a4R_Fa-4ax-1",
    "a22DaAZnhjD9",
    "qX5pFqxTy1V0",
    "2CVhwiDzSEor",
    "8orO_EtEEoLh",
    "t_sew4v63Cfs",
    "y_AphIUVpAgK",
    "uzBacJwdpFnU",
    "bQ1ZVdoDDeoz",
    "NIlrqhJhTvaP",
    "XpoEzKQMj3J6",
    "gDwMJ97Z7HGb"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
